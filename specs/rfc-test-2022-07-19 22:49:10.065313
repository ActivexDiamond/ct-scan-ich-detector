=============== Features for image at index 1 ===============
len(patches_mean)=1000
len(patches_std)=1000
len(patches_var)=1000
len(lbp)=128
len(hog)=15876
len(glcm)=6
len(corner_kitchen_rosenfeld)=128
len(daisy)=25
len(draw_multiblock_lbp)=128

patches_mean=[101.0, 75.22222222222223, 112.88888888888889, 109.66666666666667, 166.0, 186.66666666666666, 110.22222222222223, 111.88888888888889, 152.44444444444446, 212.33333333333334, 126.77777777777777, 106.44444444444444, 20.11111111111111, 90.11111111111111, 248.44444444444446, 27.0, 113.44444444444444, 23.333333333333332, 17.88888888888889, 103.77777777777777, 105.22222222222223, 18.555555555555557, 200.11111111111111, 122.22222222222223, 221.77777777777777, 105.55555555555556, 188.44444444444446, 194.22222222222223, 248.33333333333334, 111.55555555555556, 120.0, 84.22222222222223, 18.333333333333332, 172.33333333333334, 236.88888888888889, 20.88888888888889, 145.55555555555554, 113.55555555555556, 21.555555555555557, 125.77777777777777, 140.0, 123.33333333333333, 20.0, 204.88888888888889, 150.0, 22.0, 57.111111111111114, 248.55555555555554, 104.66666666666667, 20.666666666666668, 122.22222222222223, 108.88888888888889, 113.88888888888889, 25.444444444444443, 249.0, 128.33333333333334, 66.22222222222223, 107.55555555555556, 134.33333333333334, 249.33333333333334, 17.88888888888889, 79.22222222222223, 18.88888888888889, 249.88888888888889, 116.22222222222223, 226.88888888888889, 102.0, 248.88888888888889, 243.0, 18.666666666666668, 119.22222222222223, 170.33333333333334, 85.77777777777777, 95.33333333333333, 19.11111111111111, 77.55555555555556, 19.11111111111111, 90.11111111111111, 250.44444444444446, 96.11111111111111, 168.11111111111111, 17.88888888888889, 18.333333333333332, 248.11111111111111, 103.11111111111111, 248.88888888888889, 24.0, 94.11111111111111, 20.555555555555557, 20.0, 138.88888888888889, 116.0, 109.22222222222223, 248.33333333333334, 37.111111111111114, 92.88888888888889, 118.77777777777777, 20.22222222222222, 134.0, 241.22222222222223, 18.0, 138.11111111111111, 104.77777777777777, 242.11111111111111, 249.0, 48.888888888888886, 25.0, 99.22222222222223, 18.666666666666668, 248.22222222222223, 22.333333333333332, 17.77777777777778, 18.77777777777778, 77.66666666666667, 237.11111111111111, 105.66666666666667, 30.77777777777778, 92.33333333333333, 66.88888888888889, 137.55555555555554, 21.0, 249.55555555555554, 250.0, 249.0, 96.22222222222223, 37.888888888888886, 102.0, 111.55555555555556, 117.0, 243.44444444444446, 214.33333333333334, 81.66666666666667, 116.0, 17.77777777777778, 209.22222222222223, 21.88888888888889, 104.66666666666667, 18.666666666666668, 108.55555555555556, 116.22222222222223, 117.11111111111111, 25.77777777777778, 19.88888888888889, 165.77777777777777, 18.77777777777778, 248.22222222222223, 191.77777777777777, 162.88888888888889, 249.77777777777777, 191.55555555555554, 79.55555555555556, 248.66666666666666, 249.44444444444446, 111.88888888888889, 77.66666666666667, 226.88888888888889, 18.0, 99.11111111111111, 249.11111111111111, 125.22222222222223, 248.77777777777777, 83.0, 131.77777777777777, 150.22222222222223, 226.55555555555554, 81.88888888888889, 75.44444444444444, 209.22222222222223, 121.88888888888889, 248.33333333333334, 235.88888888888889, 143.55555555555554, 26.0, 36.0, 18.333333333333332, 203.44444444444446, 119.88888888888889, 248.22222222222223, 52.55555555555556, 85.44444444444444, 238.88888888888889, 22.0, 165.22222222222223, 30.555555555555557, 116.44444444444444, 23.11111111111111, 120.33333333333333, 87.77777777777777, 76.88888888888889, 248.0, 184.88888888888889, 162.66666666666666, 102.77777777777777, 250.55555555555554, 126.77777777777777, 79.66666666666667, 112.66666666666667, 20.555555555555557, 190.0, 84.0, 20.22222222222222, 98.88888888888889, 243.77777777777777, 22.22222222222222, 17.88888888888889, 19.0, 35.666666666666664, 196.44444444444446, 97.66666666666667, 178.33333333333334, 249.88888888888889, 97.11111111111111, 94.44444444444444, 230.22222222222223, 34.333333333333336, 113.55555555555556, 161.66666666666666, 150.88888888888889, 23.11111111111111, 18.11111111111111, 250.0, 78.33333333333333, 251.66666666666666, 102.0, 250.66666666666666, 128.22222222222223, 107.66666666666667, 37.55555555555556, 52.55555555555556, 19.22222222222222, 134.44444444444446, 113.44444444444444, 81.0, 21.444444444444443, 95.77777777777777, 248.88888888888889, 20.11111111111111, 228.11111111111111, 19.555555555555557, 104.44444444444444, 36.0, 82.77777777777777, 83.0, 102.0, 79.88888888888889, 41.55555555555556, 112.66666666666667, 115.22222222222223, 124.11111111111111, 248.77777777777777, 74.66666666666667, 24.0, 142.33333333333334, 119.22222222222223, 80.33333333333333, 20.0, 136.55555555555554, 36.666666666666664, 21.11111111111111, 27.333333333333332, 36.55555555555556, 103.0, 19.0, 119.0, 106.22222222222223, 111.33333333333333, 139.0, 109.44444444444444, 118.55555555555556, 76.11111111111111, 18.0, 249.11111111111111, 250.44444444444446, 94.55555555555556, 87.0, 247.22222222222223, 123.33333333333333, 250.88888888888889, 140.55555555555554, 125.11111111111111, 84.11111111111111, 93.11111111111111, 66.55555555555556, 18.0, 123.77777777777777, 18.555555555555557, 76.33333333333333, 98.33333333333333, 24.555555555555557, 249.44444444444446, 112.88888888888889, 120.55555555555556, 148.11111111111111, 20.11111111111111, 23.333333333333332, 131.88888888888889, 120.55555555555556, 125.11111111111111, 22.11111111111111, 250.33333333333334, 49.111111111111114, 247.44444444444446, 56.0, 124.66666666666667, 198.55555555555554, 124.55555555555556, 24.666666666666668, 245.0, 100.33333333333333, 64.88888888888889, 215.11111111111111, 250.66666666666666, 76.33333333333333, 112.0, 88.66666666666667, 19.555555555555557, 121.22222222222223, 116.88888888888889, 20.11111111111111, 106.0, 38.77777777777778, 158.77777777777777, 18.77777777777778, 23.333333333333332, 119.11111111111111, 211.66666666666666, 250.77777777777777, 115.0, 106.0, 125.11111111111111, 86.11111111111111, 88.44444444444444, 138.44444444444446, 97.77777777777777, 249.22222222222223, 117.33333333333333, 184.33333333333334, 23.0, 247.11111111111111, 99.55555555555556, 118.0, 237.66666666666666, 248.33333333333334, 20.555555555555557, 119.88888888888889, 81.66666666666667, 91.88888888888889, 142.0, 99.11111111111111, 56.333333333333336, 132.33333333333334, 103.33333333333333, 116.44444444444444, 177.55555555555554, 90.33333333333333, 136.55555555555554, 119.33333333333333, 141.11111111111111, 183.0, 23.22222222222222, 232.44444444444446, 82.66666666666667, 73.77777777777777, 61.77777777777778, 102.55555555555556, 69.0, 249.0, 47.77777777777778, 100.11111111111111, 20.0, 43.77777777777778, 135.22222222222223, 127.0, 119.0, 35.888888888888886, 114.33333333333333, 116.11111111111111, 47.22222222222222, 127.77777777777777, 226.44444444444446, 105.55555555555556, 88.22222222222223, 118.88888888888889, 116.22222222222223, 182.22222222222223, 78.22222222222223, 101.22222222222223, 128.22222222222223, 19.77777777777778, 40.666666666666664, 233.33333333333334, 42.333333333333336, 174.66666666666666, 84.88888888888889, 126.33333333333333, 249.55555555555554, 150.77777777777777, 19.77777777777778, 122.0, 85.22222222222223, 136.33333333333334, 124.0, 105.66666666666667, 249.66666666666666, 84.55555555555556, 18.77777777777778, 78.22222222222223, 21.333333333333332, 98.55555555555556, 32.0, 102.11111111111111, 99.77777777777777, 120.22222222222223, 96.11111111111111, 249.88888888888889, 111.55555555555556, 140.77777777777777, 107.77777777777777, 87.0, 152.88888888888889, 86.77777777777777, 163.0, 65.0, 18.333333333333332, 137.77777777777777, 108.44444444444444, 248.33333333333334, 114.44444444444444, 248.33333333333334, 76.88888888888889, 104.11111111111111, 129.11111111111111, 183.66666666666666, 244.55555555555554, 130.44444444444446, 18.0, 23.88888888888889, 250.11111111111111, 18.0, 121.0, 128.0, 47.22222222222222, 95.44444444444444, 116.0, 84.0, 94.0, 247.33333333333334, 176.33333333333334, 225.33333333333334, 78.44444444444444, 62.44444444444444, 249.55555555555554, 240.66666666666666, 80.88888888888889, 22.555555555555557, 101.33333333333333, 106.11111111111111, 108.44444444444444, 129.55555555555554, 117.33333333333333, 149.11111111111111, 92.11111111111111, 248.66666666666666, 218.88888888888889, 249.22222222222223, 122.88888888888889, 128.66666666666666, 120.33333333333333, 212.11111111111111, 250.11111111111111, 113.33333333333333, 121.88888888888889, 196.33333333333334, 134.66666666666666, 221.33333333333334, 47.77777777777778, 133.33333333333334, 51.333333333333336, 23.22222222222222, 92.55555555555556, 111.11111111111111, 197.55555555555554, 219.55555555555554, 159.77777777777777, 19.666666666666668, 140.66666666666666, 20.77777777777778, 168.11111111111111, 108.88888888888889, 199.33333333333334, 113.0, 249.33333333333334, 104.88888888888889, 249.88888888888889, 23.0, 140.44444444444446, 20.666666666666668, 121.44444444444444, 19.88888888888889, 102.0, 116.0, 109.11111111111111, 158.44444444444446, 118.77777777777777, 247.55555555555554, 242.66666666666666, 95.33333333333333, 145.66666666666666, 237.55555555555554, 23.22222222222222, 98.44444444444444, 122.66666666666667, 20.0, 144.44444444444446, 152.88888888888889, 165.44444444444446, 60.22222222222222, 21.11111111111111, 111.22222222222223, 134.0, 110.11111111111111, 20.0, 132.33333333333334, 184.11111111111111, 19.77777777777778, 244.88888888888889, 110.55555555555556, 183.66666666666666, 115.0, 117.33333333333333, 123.33333333333333, 135.88888888888889, 114.11111111111111, 54.888888888888886, 244.55555555555554, 153.22222222222223, 224.88888888888889, 112.66666666666667, 117.33333333333333, 25.444444444444443, 230.44444444444446, 71.33333333333333, 248.88888888888889, 120.33333333333333, 75.22222222222223, 35.77777777777778, 81.55555555555556, 19.88888888888889, 249.55555555555554, 235.11111111111111, 213.44444444444446, 79.0, 125.11111111111111, 19.88888888888889, 162.55555555555554, 226.66666666666666, 250.0, 193.55555555555554, 248.66666666666666, 115.66666666666667, 20.11111111111111, 91.55555555555556, 174.88888888888889, 53.111111111111114, 76.22222222222223, 20.0, 240.88888888888889, 24.0, 21.0, 135.55555555555554, 142.44444444444446, 98.88888888888889, 125.0, 25.555555555555557, 98.11111111111111, 18.333333333333332, 119.55555555555556, 18.77777777777778, 249.0, 107.22222222222223, 82.55555555555556, 158.88888888888889, 142.77777777777777, 120.44444444444444, 26.11111111111111, 111.44444444444444, 136.33333333333334, 124.55555555555556, 104.66666666666667, 103.66666666666667, 108.33333333333333, 99.66666666666667, 139.55555555555554, 27.11111111111111, 131.33333333333334, 75.33333333333333, 20.333333333333332, 88.22222222222223, 122.33333333333333, 198.66666666666666, 19.88888888888889, 174.44444444444446, 25.666666666666668, 87.55555555555556, 59.55555555555556, 23.555555555555557, 219.66666666666666, 85.0, 243.0, 150.33333333333334, 251.44444444444446, 116.22222222222223, 250.77777777777777, 18.0, 18.444444444444443, 79.0, 131.77777777777777, 248.22222222222223, 86.11111111111111, 103.0, 90.11111111111111, 123.55555555555556, 33.22222222222222, 104.44444444444444, 249.11111111111111, 146.11111111111111, 80.33333333333333, 172.33333333333334, 131.44444444444446, 251.44444444444446, 107.33333333333333, 120.11111111111111, 152.55555555555554, 174.77777777777777, 18.0, 79.88888888888889, 73.0, 37.888888888888886, 243.77777777777777, 98.88888888888889, 195.0, 134.11111111111111, 249.55555555555554, 27.0, 37.111111111111114, 145.88888888888889, 82.11111111111111, 148.0, 90.88888888888889, 97.11111111111111, 118.66666666666667, 67.11111111111111, 79.33333333333333, 220.0, 145.11111111111111, 154.88888888888889, 120.55555555555556, 18.666666666666668, 139.88888888888889, 166.66666666666666, 20.0, 132.66666666666666, 141.0, 248.77777777777777, 87.22222222222223, 19.11111111111111, 165.0, 120.44444444444444, 19.666666666666668, 250.0, 251.11111111111111, 248.0, 118.22222222222223, 24.555555555555557, 250.11111111111111, 114.55555555555556, 98.66666666666667, 18.333333333333332, 103.11111111111111, 154.88888888888889, 97.66666666666667, 128.88888888888889, 131.55555555555554, 125.11111111111111, 74.77777777777777, 250.88888888888889, 149.66666666666666, 90.66666666666667, 96.11111111111111, 86.22222222222223, 120.88888888888889, 85.44444444444444, 34.77777777777778, 19.88888888888889, 38.55555555555556, 84.66666666666667, 249.33333333333334, 74.0, 18.22222222222222, 242.0, 155.33333333333334, 20.11111111111111, 139.22222222222223, 22.0, 18.0, 76.55555555555556, 126.66666666666667, 131.55555555555554, 163.77777777777777, 122.44444444444444, 110.88888888888889, 20.11111111111111, 35.888888888888886, 102.55555555555556, 77.33333333333333, 215.11111111111111, 18.11111111111111, 206.44444444444446, 115.77777777777777, 97.55555555555556, 173.55555555555554, 131.88888888888889, 87.33333333333333, 112.88888888888889, 158.0, 58.55555555555556, 20.0, 121.77777777777777, 18.88888888888889, 196.0, 170.88888888888889, 138.33333333333334, 68.44444444444444, 124.33333333333333, 249.11111111111111, 19.88888888888889, 65.22222222222223, 25.555555555555557, 98.55555555555556, 105.66666666666667, 19.555555555555557, 137.0, 119.88888888888889, 149.11111111111111, 234.77777777777777, 32.44444444444444, 108.33333333333333, 189.66666666666666, 26.0, 20.88888888888889, 20.0, 103.55555555555556, 72.33333333333333, 194.55555555555554, 65.0, 112.66666666666667, 115.0, 104.11111111111111, 55.888888888888886, 19.666666666666668, 54.77777777777778, 249.66666666666666, 106.66666666666667, 112.22222222222223, 20.0, 20.11111111111111, 25.0, 117.55555555555556, 127.0, 91.66666666666667, 249.33333333333334, 20.22222222222222, 181.55555555555554, 250.88888888888889, 123.77777777777777, 72.22222222222223, 144.0, 104.77777777777777, 249.11111111111111, 169.77777777777777, 23.11111111111111, 18.0, 46.44444444444444, 18.333333333333332, 136.77777777777777, 112.0, 169.22222222222223, 33.0, 20.88888888888889, 181.55555555555554, 108.44444444444444, 77.88888888888889, 204.88888888888889, 55.77777777777778, 240.22222222222223, 96.77777777777777, 110.11111111111111, 140.11111111111111, 161.44444444444446, 118.55555555555556, 82.88888888888889, 133.22222222222223, 19.22222222222222, 234.66666666666666, 35.888888888888886, 212.77777777777777, 61.44444444444444, 114.55555555555556, 105.0, 100.55555555555556, 212.44444444444446, 20.11111111111111, 17.77777777777778, 106.11111111111111, 126.0, 105.44444444444444, 209.11111111111111, 121.88888888888889, 83.11111111111111, 111.11111111111111, 94.33333333333333, 109.0, 177.33333333333334, 23.333333333333332, 114.66666666666667, 114.44444444444444, 82.22222222222223, 118.33333333333333, 154.55555555555554, 185.33333333333334, 141.77777777777777, 123.88888888888889, 110.0, 92.11111111111111, 249.77777777777777, 77.44444444444444, 124.33333333333333, 70.55555555555556, 250.33333333333334, 42.666666666666664, 250.33333333333334, 23.555555555555557, 21.11111111111111, 19.11111111111111, 83.33333333333333, 99.77777777777777, 189.88888888888889, 120.44444444444444, 247.66666666666666, 58.111111111111114, 250.66666666666666, 18.22222222222222, 117.44444444444444, 75.55555555555556, 110.44444444444444, 121.33333333333333, 98.33333333333333, 124.33333333333333, 19.555555555555557, 100.66666666666667, 18.0, 249.44444444444446, 18.555555555555557, 117.44444444444444, 100.11111111111111, 112.0, 127.66666666666667, 236.44444444444446, 63.44444444444444, 50.333333333333336, 90.11111111111111, 119.88888888888889, 249.44444444444446, 18.0, 247.33333333333334, 55.888888888888886, 249.33333333333334, 147.66666666666666, 104.33333333333333, 44.22222222222222, 132.11111111111111, 208.11111111111111, 237.66666666666666, 250.44444444444446, 250.11111111111111, 114.33333333333333, 208.22222222222223, 20.0, 104.55555555555556, 249.77777777777777, 137.44444444444446, 17.77777777777778, 102.66666666666667, 76.77777777777777, 89.88888888888889, 97.0, 104.0, 218.33333333333334, 172.88888888888889, 137.77777777777777, 165.11111111111111, 116.88888888888889, 96.44444444444444, 118.44444444444444, 248.55555555555554, 114.55555555555556, 176.88888888888889, 131.0, 84.22222222222223, 149.66666666666666, 119.77777777777777, 87.55555555555556, 115.22222222222223, 242.66666666666666, 74.88888888888889, 236.44444444444446, 18.0, 18.444444444444443, 70.44444444444444, 18.11111111111111, 142.55555555555554, 33.888888888888886, 17.88888888888889, 25.333333333333332, 130.66666666666666, 250.88888888888889, 19.666666666666668, 225.66666666666666, 37.44444444444444, 31.88888888888889, 41.111111111111114, 175.66666666666666, 18.11111111111111, 154.44444444444446, 122.33333333333333, 194.44444444444446, 21.22222222222222, 124.66666666666667, 189.77777777777777, 127.66666666666667, 23.555555555555557, 143.11111111111111, 89.0, 19.11111111111111, 42.77777777777778, 159.77777777777777, 250.55555555555554, 115.44444444444444, 146.55555555555554, 82.88888888888889, 116.22222222222223, 95.55555555555556, 18.444444444444443, 27.333333333333332, 18.333333333333332, 241.44444444444446, 249.22222222222223, 125.66666666666667, 58.111111111111114, 82.22222222222223, 49.111111111111114, 107.44444444444444, 21.333333333333332, 20.0, 104.77777777777777, 113.44444444444444, 139.55555555555554, 104.77777777777777, 132.66666666666666, 19.11111111111111, 72.66666666666667, 209.11111111111111, 18.333333333333332, 100.55555555555556, 90.22222222222223, 18.11111111111111, 108.77777777777777, 120.66666666666667, 18.555555555555557, 18.666666666666668, 18.444444444444443, 85.77777777777777, 79.22222222222223, 111.33333333333333, 124.44444444444444, 202.88888888888889, 138.33333333333334, 79.22222222222223, 248.66666666666666, 86.44444444444444, 108.77777777777777, 112.88888888888889, 227.66666666666666, 58.666666666666664, 26.444444444444443, 18.77777777777778, 239.0, 150.11111111111111, 246.66666666666666, 19.11111111111111]
patches_std=[11.803954139750516, 17.874735601946846, 100.51030289305828, 16.6266185511199, 12.073846850849888, 37.18123780140253, 16.150526498316996, 16.27844139716661, 110.47484491061616, 45.467937421146935, 11.49664470644252, 14.330318373691425, 0.3142696805273545, 20.38215147538238, 1.0657403385139377, 4.163331998932265, 18.46986200460458, 7.086763875156433, 0.3142696805273545, 8.953928719069236, 16.01927850905331, 0.6849348892187752, 32.9324036455972, 9.01576123337813, 19.251422907167598, 7.274172134814626, 93.72549885598086, 17.554852306589563, 9.140872800534726, 12.737618049707994, 15.846485765339617, 14.053688940612929, 0.4714045207910317, 107.03685761871417, 7.880276989554286, 1.0999438818457405, 11.226071946946744, 9.334655990936987, 1.4229164972072996, 13.121464838430688, 9.41629792788369, 18.2208671582886, 0.4714045207910317, 34.9648853834321, 14.734690736866897, 3.6209268304000717, 21.966866182424457, 1.4229164972072998, 11.575836902790225, 1.0540925533894598, 12.025692659910167, 10.51395310416321, 102.58919636041868, 3.269764215458258, 1.0540925533894598, 12.256517540566824, 13.878662902117672, 13.208676534856515, 8.640987597877148, 1.632993161855452, 0.5665577237325317, 13.389695688001424, 0.8748897637790901, 1.5234788000891208, 11.331154474650633, 38.8971419813974, 12.274635093014645, 0.9938079899999065, 2.494438257849294, 1.2472191289246473, 20.356695746814978, 12.987173159185437, 12.847635128499787, 8.666666666666666, 1.1967032904743342, 11.823809417264913, 1.0999438818457405, 11.278735591510685, 1.8324913891634047, 21.05782749396129, 98.23830939149333, 0.5665577237325317, 0.9428090415820634, 2.514157444218836, 15.198278005122411, 1.5234788000891208, 2.0548046676563256, 19.376070261160596, 1.5713484026367723, 0.0, 49.00440897271828, 7.586537784494028, 18.035630031060293, 1.5634719199411433, 45.58860589724499, 16.237435788561392, 14.029950326812976, 0.7856742013183862, 8.831760866327848, 13.82250803563021, 0.0, 13.859969892900155, 14.234369812205152, 2.9228769862146455, 1.5634719199411433, 42.24385410774617, 7.333333333333333, 12.760858363481647, 0.816496580927726, 1.314684396244359, 0.6666666666666666, 0.41573970964154905, 1.227262335243029, 20.477630071210225, 4.6772367066331535, 15.456030825826172, 23.184179365237544, 17.390929947660776, 16.83544174547945, 105.27119592442192, 1.4142135623730951, 1.8324913891634047, 1.4142135623730951, 0.9428090415820634, 17.27411536746224, 7.09372875533883, 12.840906856172149, 8.152860590152953, 20.46134567096374, 1.9499920860871385, 13.366625103842281, 22.271057451320086, 11.832159566199232, 0.628539361054709, 49.734356061373774, 6.419491922513017, 17.46106780494506, 0.9428090415820634, 102.34702526330169, 11.123326618495403, 14.464089205449815, 1.7497795275581802, 0.5665577237325317, 25.01012140793215, 0.628539361054709, 1.3966450099973928, 32.53981416930403, 71.4880634083519, 1.1331154474650633, 22.136501335080588, 11.324615383325252, 1.3333333333333333, 1.7069212773041353, 9.109755996786504, 20.477630071210225, 36.259694318882026, 0.0, 19.750918090866023, 1.0999438818457405, 14.527963669938803, 1.8121673811444545, 90.86497919685254, 12.190868875950446, 16.136761190784075, 30.45255366453457, 32.060513770210804, 13.77688169129003, 49.734356061373774, 16.016966313148792, 6.582805886043833, 7.69479541919146, 32.68404677484486, 3.197221015541813, 4.0, 1.0540925533894598, 44.72412004254804, 16.257951520850373, 1.0304020550550783, 73.03745022796504, 10.264405715449776, 8.238722183074056, 3.6209268304000717, 23.63508586721481, 10.985961861607356, 17.00399372115668, 3.7251232476089355, 18.05547008526779, 11.409331247105277, 15.814511229630169, 3.4318767136623336, 38.979893519164875, 12.970050972229146, 11.428791884998924, 1.7708197167232476, 27.295818881196105, 9.404490653110589, 19.510680835549195, 0.6849348892187752, 24.26245384677046, 7.788880963698615, 0.7856742013183861, 15.701694490612892, 17.636942149790638, 2.9731307022799225, 0.7370277311900889, 1.0540925533894598, 6.377042156569663, 77.46699815062628, 14.165686240583852, 94.3468777084506, 1.4487116456005886, 14.805737960102153, 21.177089880398324, 20.087463076244997, 5.456901847914967, 9.978990275252315, 11.61416759345623, 11.406084579385634, 7.2333247994487975, 0.3142696805273545, 2.160246899469287, 14.468356276140472, 1.1547005383792517, 16.431676725154983, 1.632993161855452, 11.544866851431589, 11.15546702045434, 4.833014037984788, 13.833221775543036, 0.9162456945817022, 16.700705980494167, 15.63550881175503, 16.713268182295565, 3.1661792997277796, 17.47873487525645, 1.4487116456005886, 0.3142696805273545, 13.763433393068464, 0.9558139185602919, 14.974876078961287, 4.0, 13.644516829341113, 17.993826101687706, 9.451631252505216, 18.008914116468016, 24.60402456289129, 21.007935008784973, 14.148245103410275, 12.801427389548268, 1.1331154474650633, 15.048071120394283, 5.597618541248888, 22.125902367034787, 9.210836344916759, 13.316656236958787, 0.0, 10.884352796674184, 18.29389697868299, 0.9938079899999066, 14.071247279470288, 44.4474998949725, 17.256238807393046, 0.816496580927726, 9.030811456096044, 15.689896096092314, 18.245242911205345, 32.37626016423488, 11.8894080883515, 12.93669295806923, 10.826351026232702, 0.4714045207910317, 1.1967032904743342, 1.257078722109418, 9.878271453730164, 16.44519517805868, 0.9162456945817024, 14.877275736280932, 0.9938079899999065, 6.499762578759851, 17.741629916632952, 22.873781422038853, 15.637877409563718, 30.688800386065964, 0.6666666666666666, 94.56423775259015, 0.9558139185602919, 16.36391694484477, 8.339997335464536, 5.833068777069639, 0.8314794192830981, 13.59284488614379, 9.844469525927416, 19.85192095068129, 0.3142696805273545, 5.773502691896258, 9.859506911768452, 5.964918013986466, 14.555979637588003, 4.6772367066331535, 1.5634719199411433, 67.95822972719581, 4.54877544232416, 61.949800466004554, 102.76402310363508, 91.033706957748, 17.391639824998364, 6.531972647421808, 13.880441875771343, 12.87547194388531, 10.49279588251381, 62.931081252862015, 1.1547005383792515, 13.5400640077266, 16.69331203406522, 19.94436706886879, 0.9558139185602919, 18.286472096290062, 11.82694141398682, 0.3142696805273545, 17.003267659809133, 34.82584537628397, 6.6629619335844215, 1.0304020550550783, 0.9428090415820634, 17.697736480782222, 74.70683443374583, 2.3934065809486684, 22.528993664954402, 13.199326582148887, 12.27865758537005, 12.377797725896551, 14.476886644357627, 19.866840665407555, 14.358719981466761, 0.6285393610547089, 13.165611772087667, 61.17007256639294, 8.137703743822469, 6.9032109186706965, 9.900741959975761, 9.189365834726814, 28.039654459750786, 1.49071198499986, 1.7069212773041351, 13.486161173954452, 8.205689083394114, 18.734664510499986, 8.679477710861024, 13.084719200013803, 8.628119403696523, 11.962905629950907, 14.023789311975086, 9.417608933518707, 57.96763677760416, 17.473789896108208, 11.738929641828612, 16.46545204697129, 12.260545977007753, 93.73840669058168, 7.3903507325848405, 14.863161852274688, 15.563490039905004, 13.488907193837113, 15.251796089333784, 20.667264029598496, 93.09254654494217, 1.4142135623730951, 31.104761256751452, 11.77987402728762, 0.4714045207910317, 58.495541668247924, 19.78651492404628, 19.44222209522358, 14.491376746189438, 6.402160129283527, 19.40217628114033, 12.653014084877118, 45.856728036056374, 14.381915942350641, 16.971290208997804, 15.188527184183835, 15.490339482527508, 8.723460633951344, 16.0539214854729, 96.30250692223008, 72.85873359953764, 11.409331247105277, 11.544866851431589, 1.0304020550550783, 6.4635731432217725, 7.055336829505575, 5.676462121975467, 57.1858568373529, 17.175204636946475, 37.79182745280019, 0.8314794192830981, 10.336319759606875, 0.6285393610547089, 23.22833518691246, 14.226561837928202, 9.030811456096044, 14.337208778404378, 14.483707321600289, 1.0540925533894598, 17.166576770710325, 1.0304020550550783, 12.335835582001442, 1.0540925533894598, 14.840718095168, 8.894442709417556, 8.812168915065934, 12.44345234140588, 13.838575535058013, 12.913769089571232, 1.286204100310025, 17.224731000067646, 10.840026422454244, 16.129874131752327, 13.097921802925667, 12.83609878813518, 16.92321073934834, 11.633285577743433, 19.78776277287444, 0.4714045207910317, 8.80375901757347, 15.326890756018116, 2.1081851067789197, 12.919503869998785, 1.1547005383792515, 8.710714276675395, 11.77987402728762, 21.408432629557936, 17.676098111417136, 10.468058411834559, 14.492228653938106, 0.0, 0.3142696805273545, 1.4487116456005886, 0.4714045207910317, 17.88854381999832, 22.40535650240808, 20.29656663783189, 19.1027113323453, 11.718930554164631, 7.788880963698615, 13.9522996909709, 2.6246692913372702, 52.13017893270227, 48.18021724041241, 11.738929641828614, 32.97848157380173, 1.5713484026367723, 3.366501646120693, 15.8706188666314, 3.4354721852756236, 12.202003478482084, 14.586481141754698, 13.881331277103481, 37.532537324629295, 13.333333333333334, 10.19198426263878, 19.13370757151104, 1.8856180831641267, 18.11349677539096, 1.617802197617893, 9.242948596926855, 12.274635093014645, 15.57776192739723, 37.42680098670958, 1.4487116456005886, 16.87865186822955, 19.151120909721328, 17.851237118661178, 13.097921802925667, 52.16214037871614, 70.84847332751431, 15.238839267549945, 59.288372478177465, 2.3465235646603193, 14.399931412731036, 15.76525324492262, 55.73371433504362, 12.401712346922196, 98.94081033940081, 0.6666666666666666, 8.94427190999916, 0.9162456945817024, 27.71459480909109, 15.198278005122411, 38.88730155490635, 103.10620199041806, 1.699673171197595, 19.547346761954646, 1.5234788000891208, 2.211083193570267, 8.833158628600795, 1.2472191289246473, 11.096657265552352, 0.3142696805273545, 12.274635093014645, 11.981467170407619, 10.115383712658703, 20.056094175592786, 14.029950326812976, 1.4989708403591158, 6.6332495807108005, 20.612833111653742, 26.29321839055336, 12.39274977208772, 7.3903507325848405, 31.556338018467894, 11.440668201153676, 0.0, 30.808047689997103, 8.184598604756843, 11.14660995909729, 12.725981977197305, 1.0999438818457405, 14.077387524188318, 103.82249809704595, 9.266959760885795, 0.6666666666666666, 13.114877048604002, 85.34562990106686, 0.6285393610547089, 2.469567863432541, 12.203015211287477, 55.0555275255002, 17.745108872274887, 26.679163738351654, 12.657891697365018, 16.380506330828755, 14.586481141754698, 10.826351026232702, 4.621474298463427, 26.91871623890369, 31.08927805331837, 6.879922480183431, 13.466006584482772, 3.890475866669244, 19.044457409563847, 23.098821518760552, 1.1967032904743342, 9.285592184789413, 17.874735601946846, 5.883645464388323, 14.150862644384862, 0.8748897637790901, 1.4989708403591158, 15.110294095560185, 48.696591509995415, 8.576453553512405, 19.541661731026778, 0.3142696805273545, 14.322562706064016, 14.321700705960557, 1.5634719199411433, 5.335647646019098, 1.5634719199411433, 16.0, 0.3142696805273545, 10.100727268767566, 12.059523156635676, 22.19832047923368, 15.746447767161667, 0.0, 3.1426968052735442, 8.537498983243799, 1.632993161855452, 21.281766379859395, 10.58417173698654, 15.701694490612892, 12.780193008453876, 5.079613089183316, 16.168862012072218, 0.4714045207910317, 20.597255001097764, 1.227262335243029, 1.632993161855452, 18.56387039252907, 15.195840886470007, 23.26391791042966, 10.086049527031305, 15.420845279549447, 2.1829869671542776, 12.553097104443198, 11.85092588975412, 17.391639824998364, 20.752509888364507, 10.749676997731399, 23.90722810272148, 10.0, 12.111620784382714, 10.68170236582628, 16.138291249213605, 10.03327796219494, 1.0540925533894598, 25.715441870932025, 13.131810402394805, 93.92313642311757, 0.7370277311900888, 12.419618093172065, 10.83205120618128, 19.90036912617514, 5.794846582402354, 2.6712922844825124, 38.47076812334269, 15.67730135507313, 10.893423092245461, 17.88233144133554, 1.257078722109418, 14.573779939617417, 1.227262335243029, 0.0, 0.9558139185602919, 17.21110752456745, 107.02624577154249, 2.199887763691481, 17.271971157176036, 13.308309851784752, 33.92293371892775, 13.817148050408724, 39.59735619058361, 12.535382023262626, 1.5234788000891208, 15.351036479262707, 13.316656236958787, 36.49048582241069, 13.817148050408722, 0.8314794192830981, 8.692269873603532, 14.356140338966418, 13.833221775543036, 28.440103835479214, 0.0, 17.37459476390389, 10.913803695829934, 5.526591162420394, 7.098947931094794, 15.701694490612892, 35.07452383457575, 18.561875171343633, 1.0657403385139377, 18.110770276274835, 45.58860589724499, 17.85400325133737, 11.019623349526611, 109.62055768270231, 21.047271897350203, 16.888157878916513, 9.067647005823629, 23.755441222051022, 19.113694915775266, 44.90483765079709, 27.569888745370353, 32.16777008278263, 18.84504716017277, 1.1547005383792515, 9.949253958010503, 29.60855732160327, 0.0, 18.879736344675063, 7.874007874011811, 0.7856742013183862, 9.565885888902615, 0.5665577237325317, 25.18376902336547, 10.965715370200291, 0.6666666666666666, 1.1547005383792515, 0.9938079899999067, 3.4318767136623336, 14.672557739442839, 8.139220698583305, 0.8748897637790901, 17.689363477720548, 10.583005244258363, 0.4714045207910317, 14.129908739537361, 14.850697285869886, 13.703203194062977, 107.82954083433356, 15.217760903417435, 17.741629916632952, 11.886292551392295, 0.8748897637790902, 67.14660577174894, 17.795130420052185, 15.793419476650143, 13.488907193837113, 13.008069670139758, 88.73069256164484, 4.516089207311461, 0.3142696805273545, 49.36960628454878, 7.542472332656507, 0.816496580927726, 13.784048752090222, 0.41573970964154905, 5.436502143433364, 23.87001838662421, 0.3142696805273545, 13.926615428163105, 1.247219128924647, 0.0, 11.75784476765393, 11.372481406154654, 28.488247522681704, 18.56387039252907, 8.473633762194497, 18.03357636585738, 0.3142696805273545, 6.402160129283527, 15.363095109971322, 7.195677714974301, 21.020859716240828, 0.3142696805273545, 30.481728994089817, 95.96385636475361, 17.372462955372864, 27.48377748220499, 9.242948596926853, 27.084230754362505, 17.93472388936206, 18.29996964174774, 31.742404554689617, 0.816496580927726, 10.633048875795772, 0.7370277311900888, 30.904871963998023, 106.43667334193964, 12.165525060596439, 20.597255001097764, 12.418624006798105, 1.0999438818457405, 0.8748897637790901, 17.138506208585635, 11.767290928776648, 27.17887705599132, 22.320892057044276, 1.0657403385139377, 9.591663046625438, 21.656122790587137, 10.19198426263878, 7.020252888413918, 18.886274328852988, 12.806248474865697, 42.554540167752826, 5.637178175095921, 1.0999438818457405, 0.4714045207910317, 15.70641136684925, 10.954451150103322, 44.76633435247993, 9.877021593352703, 12.622730819174325, 19.136933459209764, 10.979217179587224, 71.61229634395424, 0.4714045207910317, 20.7459649505246, 1.4142135623730951, 14.102797200870786, 17.20321531818836, 0.4714045207910317, 0.5665577237325317, 3.0912061651652345, 99.82218759275149, 19.966638842495918, 10.154364141151877, 1.699673171197595, 0.7856742013183862, 63.68692716215805, 2.23330569358242, 18.23779822101937, 27.867853953087547, 10.760008261045982, 14.234369812205152, 1.1967032904743342, 9.885767297571212, 2.5579698740491863, 0.6666666666666666, 72.35577395544435, 0.4714045207910317, 18.158424905859217, 21.270741511391755, 17.267681937833252, 9.079892314584157, 0.7370277311900888, 63.68692716215805, 10.792086721411625, 12.278657585370048, 23.992797273091238, 18.121673811444545, 15.127442155660008, 12.505801123014674, 12.71433525543724, 6.756798131338121, 14.51521126352253, 12.508762360939995, 13.690584310592156, 15.907914017716518, 1.0304020550550783, 5.374838498865699, 6.118177732416091, 67.65726991860893, 8.486736201745916, 11.917411269148374, 11.718930554164631, 17.8145453125068, 64.48619337348475, 0.7370277311900889, 0.41573970964154905, 18.935237254297693, 14.907119849998598, 11.917411269148376, 62.780825884703304, 16.09309337327624, 12.635439089626223, 14.255170168861888, 9.556847457887633, 11.737877907772672, 11.962905629950907, 7.3181661333667165, 13.936363306911247, 12.919503869998785, 12.699761833092932, 7.557189365836422, 7.440297352348092, 32.38655413730965, 19.57196281406292, 20.272830446708415, 13.416407864998739, 7.578396846657749, 1.987615979999813, 11.66296237488678, 11.244751862288666, 19.276416858783175, 1.2472191289246473, 11.8227652339788, 1.2472191289246473, 6.977919319158925, 1.1967032904743342, 0.8748897637790901, 17.657230184198703, 11.801862167356639, 40.50361971326196, 17.34045437768836, 1.4142135623730951, 17.444798297897314, 1.9436506316151, 0.7856742013183862, 15.727619803751553, 19.038622213870127, 16.007714189735115, 53.245761437987824, 11.135528725660043, 13.9522996909709, 0.6849348892187752, 20.270394394014364, 0.0, 1.257078722109418, 0.9558139185602919, 17.04967325272515, 105.53929699347322, 11.785113019775793, 14.996295838935989, 6.202349216423495, 60.481606223363684, 16.619934483090546, 22.922305703515782, 20.90956423609086, 1.2570787221094177, 0.0, 1.49071198499986, 43.033435177892514, 1.2472191289246473, 69.88085097745542, 10.077477638553981, 13.231089463648175, 14.601708000888452, 50.781546176724135, 28.039654459750786, 1.4989708403591158, 1.9116278371205837, 17.84501175243223, 39.625406490106485, 0.4714045207910317, 101.91184764961348, 1.227262335243029, 16.323882375295074, 0.41573970964154905, 17.29482902809713, 11.716823401301394, 22.382753501769788, 23.72059583287626, 16.048537489614297, 11.105554165971789, 12.431540929287213, 65.03294511720412, 97.90408508608417, 17.722831755453157, 14.720440143714839, 12.962433851635453, 0.8314794192830981, 11.615230529219538, 93.3481469725558, 16.69331203406522, 11.02298384465414, 12.727922061357855, 16.06775776224464, 19.276416858783175, 14.823238345979883, 17.320508075688775, 48.59456861384235, 33.069324863286525, 0.0, 0.6849348892187752, 27.96073084051636, 0.5665577237325317, 11.33442260560586, 6.08174763706845, 0.3142696805273545, 6.48074069840786, 21.134489978863144, 1.4487116456005886, 0.6666666666666666, 34.95711658589707, 25.75142330957914, 30.123613640555014, 40.73067215650863, 13.466006584482772, 0.5665577237325317, 15.283332323599467, 13.299958228840001, 31.58801147802442, 1.4740554623801778, 15.21694961401777, 38.322542764678566, 21.761331658599286, 4.4748956812449485, 13.403518977777974, 12.970050972229146, 0.7370277311900888, 12.52355804764922, 98.94081033940081, 1.1653431646335017, 17.075720977855674, 15.471199846532848, 13.690584310592156, 18.943059577925617, 22.618139775710564, 0.6849348892187752, 10.666666666666666, 0.816496580927726, 18.714884813437855, 1.314684396244359, 9.486832980505138, 19.980237149323578, 13.222689067388655, 10.450352938691108, 13.784944372685175, 1.3333333333333333, 0.4714045207910317, 14.234369812205152, 27.74130939435953, 12.111620784382714, 14.543251797294378, 11.69995251652283, 1.1967032904743342, 72.74613391789285, 46.323606759511684, 0.816496580927726, 18.774161387462, 11.103330609203885, 0.5665577237325317, 12.44345234140588, 18.88562063228706, 0.8314794192830981, 0.9428090415820634, 0.9558139185602919, 8.740426861909839, 12.461298111243625, 19.697715603592208, 11.076613111628092, 88.45267968360187, 20.564262420249577, 18.53391952177159, 1.3333333333333333, 17.845703567034537, 11.49664470644252, 17.93472388936206, 33.6286914537109, 20.242968600918637, 10.457438725202172, 0.9162456945817022, 5.962847939999439, 32.51134654733617, 7.378647873726218, 1.3698697784375504]
patches_var=[139.33333333333334, 319.5061728395061, 10102.32098765432, 276.44444444444446, 145.77777777777777, 1382.4444444444443, 260.8395061728395, 264.98765432098764, 12204.691358024691, 2067.3333333333335, 132.17283950617283, 205.35802469135803, 0.09876543209876544, 415.4320987654321, 1.1358024691358024, 17.333333333333332, 341.13580246913585, 50.22222222222222, 0.09876543209876544, 80.17283950617285, 256.61728395061726, 0.46913580246913583, 1084.5432098765434, 81.28395061728394, 370.6172839506173, 52.91358024691358, 8784.469135802468, 308.17283950617286, 83.55555555555556, 162.24691358024688, 251.11111111111111, 197.50617283950615, 0.2222222222222222, 11456.888888888889, 62.098765432098766, 1.2098765432098766, 126.02469135802468, 87.1358024691358, 2.024691358024691, 172.17283950617286, 88.66666666666667, 332.0, 0.2222222222222222, 1222.5432098765434, 217.11111111111111, 13.11111111111111, 482.5432098765432, 2.0246913580246915, 134.0, 1.1111111111111112, 144.61728395061726, 110.54320987654322, 10524.543209876541, 10.691358024691358, 1.1111111111111112, 150.22222222222223, 192.6172839506173, 174.46913580246914, 74.66666666666669, 2.666666666666667, 0.3209876543209877, 179.28395061728392, 0.7654320987654321, 2.3209876543209877, 128.39506172839506, 1512.9876543209875, 150.66666666666666, 0.9876543209876542, 6.222222222222222, 1.5555555555555558, 414.39506172839504, 168.66666666666666, 165.06172839506175, 75.11111111111111, 1.432098765432099, 139.80246913580245, 1.2098765432098766, 127.20987654320987, 3.358024691358025, 443.4320987654321, 9650.765432098766, 0.32098765432098764, 0.8888888888888888, 6.320987654320988, 230.98765432098767, 2.3209876543209877, 4.222222222222222, 375.4320987654321, 2.4691358024691357, 0.0, 2401.432098765432, 57.55555555555556, 325.2839506172839, 2.4444444444444446, 2078.3209876543206, 263.6543209876543, 196.83950617283952, 0.617283950617284, 78.0, 191.06172839506175, 0.0, 192.09876543209876, 202.61728395061732, 8.543209876543209, 2.4444444444444446, 1784.543209876543, 53.77777777777778, 162.83950617283952, 0.6666666666666666, 1.728395061728395, 0.4444444444444444, 0.17283950617283952, 1.506172839506173, 419.33333333333326, 21.876543209876544, 238.88888888888889, 537.5061728395062, 302.4444444444444, 283.4320987654321, 11082.024691358025, 2.0, 3.358024691358025, 2.0, 0.8888888888888888, 298.39506172839515, 50.32098765432099, 164.88888888888889, 66.46913580246914, 418.6666666666667, 3.80246913580247, 178.66666666666666, 496.0, 140.0, 0.39506172839506176, 2473.5061728395062, 41.20987654320987, 304.8888888888889, 0.8888888888888888, 10474.913580246915, 123.72839506172838, 209.20987654320987, 3.0617283950617282, 0.32098765432098764, 625.5061728395061, 0.39506172839506176, 1.9506172839506175, 1058.8395061728395, 5110.543209876543, 1.2839506172839505, 490.0246913580247, 128.24691358024694, 1.7777777777777777, 2.9135802469135808, 82.98765432098766, 419.33333333333326, 1314.7654320987654, 0.0, 390.09876543209873, 1.2098765432098764, 211.06172839506172, 3.2839506172839505, 8256.444444444445, 148.6172839506173, 260.3950617283951, 927.3580246913579, 1027.8765432098764, 189.80246913580243, 2473.5061728395062, 256.5432098765432, 43.33333333333334, 59.20987654320988, 1068.2469135802467, 10.222222222222221, 16.0, 1.1111111111111112, 2000.2469135802473, 264.32098765432096, 1.0617283950617287, 5334.469135802469, 105.35802469135803, 67.87654320987654, 13.11111111111111, 558.6172839506172, 120.69135802469135, 289.1358024691358, 13.876543209876543, 326.00000000000006, 130.17283950617286, 250.0987654320987, 11.777777777777779, 1519.432098765432, 168.22222222222223, 130.61728395061726, 3.1358024691358026, 745.0617283950618, 88.44444444444444, 380.6666666666667, 0.46913580246913583, 588.6666666666666, 60.666666666666664, 0.6172839506172838, 246.54320987654324, 311.06172839506166, 8.839506172839505, 0.54320987654321, 1.1111111111111112, 40.666666666666664, 6001.135802469135, 200.66666666666666, 8901.333333333334, 2.0987654320987654, 219.20987654320987, 448.46913580246905, 403.5061728395061, 29.77777777777778, 99.58024691358025, 134.88888888888889, 130.09876543209876, 52.32098765432099, 0.09876543209876544, 4.666666666666667, 209.33333333333337, 1.3333333333333335, 270.0, 2.666666666666667, 133.28395061728395, 124.44444444444444, 23.358024691358025, 191.35802469135803, 0.839506172839506, 278.9135802469136, 244.46913580246914, 279.3333333333333, 10.024691358024691, 305.5061728395061, 2.0987654320987654, 0.09876543209876544, 189.43209876543207, 0.9135802469135802, 224.24691358024697, 16.0, 186.17283950617283, 323.77777777777777, 89.33333333333333, 324.32098765432096, 605.358024691358, 441.33333333333326, 200.17283950617283, 163.87654320987656, 1.2839506172839505, 226.44444444444446, 31.333333333333332, 489.55555555555554, 84.8395061728395, 177.33333333333334, 0.0, 118.46913580246913, 334.6666666666667, 0.9876543209876544, 198.0, 1975.5802469135806, 297.77777777777777, 0.6666666666666666, 81.55555555555556, 246.1728395061728, 332.8888888888889, 1048.2222222222222, 141.35802469135805, 167.35802469135803, 117.20987654320987, 0.2222222222222222, 1.432098765432099, 1.580246913580247, 97.58024691358024, 270.44444444444446, 0.8395061728395062, 221.33333333333334, 0.9876543209876542, 42.24691358024691, 314.7654320987654, 523.2098765432097, 244.54320987654324, 941.8024691358025, 0.4444444444444444, 8942.395061728395, 0.9135802469135803, 267.77777777777777, 69.55555555555556, 34.02469135802469, 0.691358024691358, 184.76543209876542, 96.91358024691357, 394.09876543209873, 0.09876543209876544, 33.333333333333336, 97.20987654320987, 35.58024691358025, 211.87654320987656, 21.876543209876544, 2.4444444444444446, 4618.3209876543215, 20.69135802469136, 3837.777777777778, 10560.444444444445, 8287.135802469136, 302.46913580246917, 42.666666666666664, 192.66666666666666, 165.77777777777777, 110.09876543209876, 3960.3209876543206, 1.3333333333333333, 183.33333333333334, 278.6666666666667, 397.7777777777778, 0.9135802469135803, 334.3950617283951, 139.87654320987656, 0.09876543209876544, 289.1111111111111, 1212.8395061728397, 44.395061728395056, 1.0617283950617284, 0.8888888888888888, 313.20987654320993, 5581.111111111111, 5.728395061728396, 507.55555555555554, 174.22222222222223, 150.76543209876547, 153.20987654320984, 209.58024691358025, 394.69135802469134, 206.17283950617283, 0.3950617283950617, 173.33333333333334, 3741.777777777778, 66.22222222222223, 47.65432098765432, 98.02469135802468, 84.44444444444444, 786.2222222222222, 2.2222222222222228, 2.91358024691358, 181.87654320987653, 67.33333333333333, 350.9876543209877, 75.33333333333333, 171.20987654320984, 74.44444444444444, 143.11111111111111, 196.66666666666666, 88.69135802469135, 3360.246913580247, 305.3333333333333, 137.80246913580243, 271.1111111111111, 150.320987654321, 8786.888888888889, 54.617283950617285, 220.91358024691354, 242.22222222222223, 181.9506172839506, 232.6172839506173, 427.1358024691358, 8666.222222222223, 2.0, 967.5061728395062, 138.76543209876544, 0.2222222222222222, 3421.728395061729, 391.5061728395062, 378.0, 210.0, 40.987654320987666, 376.44444444444446, 160.09876543209873, 2102.839506172839, 206.83950617283952, 288.0246913580247, 230.69135802469134, 239.95061728395058, 76.09876543209877, 257.7283950617284, 9274.172839506173, 5308.395061728395, 130.17283950617286, 133.28395061728395, 1.0617283950617287, 41.77777777777778, 49.77777777777778, 32.22222222222222, 3270.2222222222217, 294.98765432098764, 1428.2222222222222, 0.691358024691358, 106.8395061728395, 0.3950617283950617, 539.5555555555555, 202.39506172839506, 81.55555555555556, 205.55555555555554, 209.7777777777778, 1.1111111111111112, 294.6913580246913, 1.0617283950617284, 152.17283950617286, 1.1111111111111112, 220.2469135802469, 79.11111111111111, 77.65432098765433, 154.8395061728395, 191.50617283950615, 166.76543209876542, 1.6543209876543212, 296.69135802469134, 117.50617283950616, 260.17283950617286, 171.55555555555554, 164.76543209876544, 286.39506172839504, 135.33333333333334, 391.55555555555554, 0.2222222222222222, 77.50617283950618, 234.91358024691357, 4.444444444444445, 166.91358024691357, 1.3333333333333333, 75.87654320987654, 138.76543209876544, 458.32098765432096, 312.44444444444446, 109.58024691358025, 210.02469135802468, 0.0, 0.09876543209876544, 2.0987654320987654, 0.2222222222222222, 320.0, 502.0, 411.9506172839506, 364.91358024691357, 137.33333333333334, 60.666666666666664, 194.66666666666666, 6.888888888888888, 2717.5555555555557, 2321.3333333333335, 137.80246913580245, 1087.5802469135804, 2.469135802469136, 11.333333333333334, 251.87654320987653, 11.80246913580247, 148.88888888888889, 212.76543209876544, 192.69135802469134, 1408.6913580246915, 177.77777777777777, 103.87654320987654, 366.0987654320988, 3.5555555555555554, 328.09876543209873, 2.617283950617284, 85.4320987654321, 150.66666666666666, 242.66666666666666, 1400.7654320987651, 2.0987654320987654, 284.8888888888889, 366.7654320987655, 318.6666666666667, 171.55555555555554, 2720.8888888888887, 5019.506172839508, 232.22222222222217, 3515.1111111111113, 5.5061728395061715, 207.35802469135803, 248.5432098765432, 3106.246913580247, 153.80246913580245, 9789.283950617284, 0.4444444444444444, 80.0, 0.8395061728395061, 768.0987654320988, 230.98765432098764, 1512.2222222222222, 10630.888888888889, 2.8888888888888893, 382.09876543209873, 2.3209876543209877, 4.888888888888889, 78.0246913580247, 1.5555555555555558, 123.1358024691358, 0.09876543209876544, 150.66666666666666, 143.55555555555554, 102.32098765432099, 402.24691358024694, 196.83950617283952, 2.246913580246914, 44.00000000000001, 424.8888888888889, 691.3333333333334, 153.58024691358023, 54.617283950617285, 995.8024691358023, 130.88888888888889, 0.0, 949.1358024691358, 66.98765432098766, 124.24691358024688, 161.95061728395063, 1.2098765432098766, 198.17283950617286, 10779.111111111111, 85.87654320987653, 0.4444444444444444, 172.00000000000003, 7283.876543209877, 0.3950617283950617, 6.098765432098765, 148.91358024691357, 3031.111111111111, 314.8888888888889, 711.7777777777778, 160.22222222222226, 268.32098765432096, 212.76543209876544, 117.20987654320987, 21.358024691358025, 724.6172839506172, 966.5432098765432, 47.333333333333336, 181.33333333333334, 15.135802469135804, 362.69135802469134, 533.5555555555555, 1.432098765432099, 86.22222222222223, 319.5061728395061, 34.617283950617285, 200.2469135802469, 0.7654320987654321, 2.246913580246914, 228.32098765432102, 2371.3580246913575, 73.55555555555556, 381.8765432098765, 0.09876543209876544, 205.1358024691358, 205.11111111111111, 2.4444444444444446, 28.469135802469136, 2.4444444444444446, 256.0, 0.09876543209876544, 102.02469135802468, 145.4320987654321, 492.7654320987654, 247.95061728395063, 0.0, 9.87654320987654, 72.88888888888889, 2.6666666666666665, 452.9135802469136, 112.02469135802468, 246.54320987654324, 163.33333333333334, 25.80246913580247, 261.4320987654321, 0.2222222222222222, 424.2469135802469, 1.506172839506173, 2.6666666666666665, 344.6172839506173, 230.91358024691357, 541.2098765432099, 101.72839506172839, 237.80246913580245, 4.765432098765432, 157.58024691358023, 140.44444444444446, 302.46913580246917, 430.6666666666667, 115.55555555555556, 571.5555555555557, 100.0, 146.69135802469134, 114.09876543209877, 260.44444444444446, 100.66666666666667, 1.1111111111111112, 661.283950617284, 172.44444444444443, 8821.555555555555, 0.5432098765432098, 154.2469135802469, 117.33333333333333, 396.02469135802465, 33.58024691358024, 7.135802469135801, 1480.0, 245.77777777777777, 118.66666666666667, 319.7777777777777, 1.580246913580247, 212.39506172839504, 1.5061728395061729, 0.0, 0.9135802469135803, 296.22222222222223, 11454.617283950618, 4.8395061728395055, 298.32098765432096, 177.11111111111111, 1150.7654320987654, 190.9135802469136, 1567.9506172839504, 157.1358024691358, 2.3209876543209877, 235.65432098765436, 177.33333333333334, 1331.5555555555557, 190.91358024691354, 0.691358024691358, 75.55555555555556, 206.0987654320988, 191.35802469135803, 808.8395061728395, 0.0, 301.8765432098765, 119.11111111111111, 30.543209876543205, 50.395061728395056, 246.54320987654324, 1230.2222222222222, 344.5432098765432, 1.1358024691358024, 328.0, 2078.3209876543206, 318.76543209876536, 121.4320987654321, 12016.666666666666, 442.9876543209877, 285.2098765432099, 82.22222222222223, 564.320987654321, 365.3333333333333, 2016.4444444444443, 760.0987654320988, 1034.7654320987654, 355.13580246913574, 1.3333333333333333, 98.98765432098766, 876.6666666666666, 0.0, 356.4444444444445, 62.0, 0.617283950617284, 91.50617283950619, 0.32098765432098764, 634.2222222222222, 120.24691358024691, 0.4444444444444444, 1.3333333333333333, 0.9876543209876546, 11.777777777777779, 215.28395061728395, 66.24691358024691, 0.7654320987654321, 312.91358024691357, 112.0, 0.2222222222222222, 199.6543209876543, 220.54320987654322, 187.77777777777777, 11627.20987654321, 231.58024691358025, 314.7654320987654, 141.28395061728395, 0.7654320987654323, 4508.666666666667, 316.6666666666667, 249.43209876543207, 181.95061728395063, 169.20987654320984, 7873.135802469135, 20.39506172839506, 0.09876543209876544, 2437.3580246913584, 56.888888888888886, 0.6666666666666666, 190.0, 0.1728395061728395, 29.555555555555557, 569.7777777777778, 0.09876543209876544, 193.95061728395063, 1.5555555555555556, 0.0, 138.2469135802469, 129.33333333333331, 811.5802469135803, 344.6172839506173, 71.80246913580247, 325.2098765432099, 0.09876543209876544, 40.987654320987666, 236.02469135802474, 51.77777777777778, 441.8765432098765, 0.09876543209876544, 929.1358024691358, 9209.061728395061, 301.8024691358024, 755.3580246913581, 85.43209876543209, 733.5555555555557, 321.6543209876543, 334.8888888888889, 1007.5802469135801, 0.6666666666666666, 113.06172839506172, 0.5432098765432098, 955.1111111111111, 11328.765432098764, 148.0, 424.2469135802469, 154.22222222222223, 1.2098765432098766, 0.7654320987654321, 293.7283950617284, 138.46913580246917, 738.6913580246915, 498.22222222222223, 1.1358024691358026, 92.0, 468.98765432098764, 103.87654320987654, 49.283950617283956, 356.69135802469134, 164.0, 1810.888888888889, 31.77777777777778, 1.2098765432098766, 0.2222222222222222, 246.69135802469134, 120.0, 2004.0246913580247, 97.55555555555556, 159.33333333333334, 366.22222222222223, 120.54320987654322, 5128.3209876543215, 0.2222222222222222, 430.3950617283951, 2.0000000000000004, 198.88888888888889, 295.95061728395063, 0.2222222222222222, 0.32098765432098764, 9.555555555555555, 9964.46913580247, 398.6666666666667, 103.11111111111111, 2.888888888888889, 0.6172839506172839, 4056.0246913580245, 4.987654320987654, 332.6172839506173, 776.6172839506172, 115.77777777777777, 202.61728395061732, 1.432098765432099, 97.7283950617284, 6.5432098765432105, 0.4444444444444444, 5235.358024691358, 0.2222222222222222, 329.72839506172835, 452.44444444444446, 298.1728395061728, 82.44444444444444, 0.5432098765432098, 4056.0246913580245, 116.46913580246913, 150.76543209876544, 575.6543209876543, 328.3950617283951, 228.83950617283952, 156.39506172839506, 161.65432098765433, 45.654320987654316, 210.69135802469134, 156.46913580246914, 187.43209876543207, 253.0617283950617, 1.0617283950617287, 28.88888888888889, 37.4320987654321, 4577.506172839505, 72.0246913580247, 142.02469135802468, 137.33333333333334, 317.358024691358, 4158.469135802468, 0.54320987654321, 0.1728395061728395, 358.5432098765432, 222.22222222222223, 142.0246913580247, 3941.4320987654323, 258.98765432098764, 159.65432098765436, 203.20987654320984, 91.33333333333331, 137.77777777777777, 143.11111111111111, 53.55555555555556, 194.2222222222222, 166.91358024691357, 161.28395061728395, 57.1111111111111, 55.358024691358025, 1048.888888888889, 383.0617283950617, 410.9876543209877, 180.0, 57.432098765432116, 3.9506172839506166, 136.02469135802468, 126.44444444444444, 371.58024691358025, 1.5555555555555558, 139.77777777777777, 1.5555555555555558, 48.69135802469136, 1.432098765432099, 0.7654320987654322, 311.77777777777777, 139.28395061728395, 1640.543209876543, 300.6913580246914, 2.0000000000000004, 304.320987654321, 3.7777777777777777, 0.6172839506172839, 247.35802469135803, 362.4691358024691, 256.24691358024694, 2835.1111111111113, 124.0, 194.66666666666669, 0.46913580246913583, 410.8888888888889, 0.0, 1.580246913580247, 0.9135802469135803, 290.69135802469134, 11138.543209876545, 138.88888888888889, 224.88888888888886, 38.46913580246914, 3658.0246913580245, 276.22222222222223, 525.4320987654321, 437.20987654320993, 1.5802469135802468, 0.0, 2.2222222222222228, 1851.8765432098767, 1.5555555555555558, 4883.333333333333, 101.55555555555554, 175.06172839506175, 213.20987654320984, 2578.765432098766, 786.2222222222222, 2.246913580246914, 3.654320987654321, 318.4444444444445, 1570.172839506173, 0.2222222222222222, 10386.024691358027, 1.506172839506173, 266.4691358024691, 0.17283950617283952, 299.1111111111111, 137.28395061728395, 500.98765432098764, 562.6666666666666, 257.55555555555554, 123.33333333333336, 154.5432098765432, 4229.283950617284, 9585.20987654321, 314.0987654320988, 216.69135802469134, 168.0246913580247, 0.691358024691358, 134.9135802469136, 8713.876543209877, 278.6666666666667, 121.50617283950618, 162.0, 258.17283950617286, 371.58024691358025, 219.7283950617284, 300.0, 2361.432098765432, 1093.5802469135804, 0.0, 0.46913580246913583, 781.8024691358025, 0.32098765432098764, 128.46913580246914, 36.987654320987666, 0.09876543209876544, 42.0, 446.6666666666667, 2.0987654320987654, 0.4444444444444444, 1222.0, 663.1358024691358, 907.4320987654322, 1658.9876543209875, 181.33333333333334, 0.3209876543209877, 233.58024691358025, 176.88888888888889, 997.8024691358025, 2.17283950617284, 231.55555555555554, 1468.6172839506173, 473.55555555555554, 20.024691358024693, 179.65432098765433, 168.22222222222223, 0.5432098765432098, 156.83950617283952, 9789.283950617284, 1.3580246913580247, 291.5802469135803, 239.35802469135803, 187.43209876543207, 358.8395061728395, 511.5802469135803, 0.46913580246913583, 113.77777777777777, 0.6666666666666666, 350.2469135802469, 1.7283950617283952, 90.00000000000001, 399.20987654320993, 174.8395061728395, 109.20987654320987, 190.02469135802468, 1.7777777777777777, 0.2222222222222222, 202.61728395061732, 769.5802469135803, 146.69135802469134, 211.50617283950615, 136.88888888888889, 1.432098765432099, 5292.0, 2145.8765432098767, 0.6666666666666666, 352.4691358024691, 123.28395061728394, 0.32098765432098764, 154.8395061728395, 356.6666666666667, 0.691358024691358, 0.8888888888888888, 0.9135802469135803, 76.39506172839506, 155.28395061728395, 388.0, 122.69135802469135, 7823.876543209875, 422.8888888888889, 343.5061728395061, 1.7777777777777777, 318.46913580246917, 132.17283950617283, 321.6543209876543, 1130.8888888888887, 409.77777777777777, 109.35802469135803, 0.839506172839506, 35.55555555555556, 1056.9876543209875, 54.44444444444444, 1.8765432098765433]
lbp=[[ 14  62  62 ...  62  62  56]
 [143 255 251 ... 247 239 248]
 [139 247   0 ... 255 223 248]
 ...
 [139 131 195 ... 247 239 248]
 [ 13 255 223 ... 255 223  24]
 [  2   0  34 ...  34  34  32]]
hog=[0.05084476 0.01674268 0.0158835  ... 0.00091106 0.00144051 0.        ]
glcm={'contrast': array([[5486.30341717]]), 'dissimilarity': array([[42.71208079]]), 'homogeneity': array([[0.15418713]]), 'energy': array([[0.04338519]]), 'correlation': array([[0.55263245]]), 'asm': array([[0.00188227]])}
corner_kitchen_rosenfeld=[[-160.         -202.           -1.         ...    8.
  -200.         -160.        ]
 [-204.         -326.         -322.         ...   11.
  -312.         -195.        ]
 [  -2.09996796 -164.            0.         ...    0.
   -34.8          -6.85134291]
 ...
 [ 592.49601376 -324.8912826  -174.92914083 ...   57.6
   184.4         158.65530161]
 [ -66.30465444  236.30769231   24.28880866 ...   29.
  -332.84027106 -175.50318471]
 [-258.1902667  -229.51460498  300.61725372 ...   17.
  -253.73023604 -271.34275093]]
daisy=[[[0.00037678 0.00041451 0.00026545 ... 0.0202656  0.0185761  0.00702031]
  [0.00025657 0.00022459 0.00021378 ... 0.01962686 0.01887593 0.00728052]
  [0.00045004 0.00030495 0.00039026 ... 0.01857486 0.01871702 0.00742092]
  ...
  [0.01485571 0.00666153 0.00205882 ... 0.00296519 0.00446992 0.00266081]
  [0.0098042  0.00395668 0.00085882 ... 0.00268569 0.00379976 0.00219755]
  [0.00220311 0.00095804 0.00026026 ... 0.00253077 0.00341155 0.00192832]]

 [[0.00023075 0.00029422 0.00027141 ... 0.01652126 0.01313074 0.00472049]
  [0.00036663 0.0003609  0.0004919  ... 0.015882   0.01361508 0.00513232]
  [0.00118163 0.00133047 0.00117331 ... 0.01501118 0.01380896 0.00547364]
  ...
  [0.00776516 0.00491296 0.00434588 ... 0.00277917 0.00434411 0.002902  ]
  [0.01767922 0.00788412 0.00196927 ... 0.00242111 0.00349491 0.00222112]
  [0.01239627 0.00545626 0.00104218 ... 0.00226005 0.00305048 0.00184096]]

 [[0.00028031 0.00027002 0.00044126 ... 0.01449298 0.00995481 0.00352721]
  [0.00098991 0.00106437 0.00103793 ... 0.01346513 0.01012278 0.00387764]
  [0.0038972  0.00574114 0.00298115 ... 0.01213516 0.00995149 0.00411955]
  ...
  [0.00223702 0.00372558 0.00635667 ... 0.00267098 0.00447564 0.00350335]
  [0.01000595 0.00536369 0.00299947 ... 0.00210467 0.0032368  0.00240157]
  [0.02214518 0.0097642  0.00189567 ... 0.00184843 0.00259191 0.00178248]]

 ...

 [[0.00326903 0.00507284 0.00923519 ... 0.00544859 0.00583491 0.00489362]
  [0.00566662 0.00291272 0.00216714 ... 0.00594252 0.00606808 0.00469823]
  [0.00748636 0.00437913 0.00172292 ... 0.00659977 0.00634567 0.00457395]
  ...
  [0.00424931 0.00444202 0.00396335 ... 0.0066363  0.00393783 0.00358756]
  [0.00308481 0.00463367 0.00348682 ... 0.00527567 0.00331265 0.00389869]
  [0.01191891 0.01719915 0.00748988 ... 0.00446457 0.00313067 0.00435838]]

 [[0.00202928 0.008728   0.01898046 ... 0.00559763 0.00621935 0.00535887]
  [0.00244674 0.00544176 0.01124616 ... 0.00582535 0.00618531 0.00492331]
  [0.00463638 0.00295381 0.003345   ... 0.00632128 0.00626592 0.00465832]
  ...
  [0.00304391 0.00460853 0.00307417 ... 0.00628243 0.00382381 0.00350386]
  [0.00789328 0.0166707  0.00982526 ... 0.00545065 0.00347354 0.00384519]
  [0.01687189 0.03024934 0.01459979 ... 0.00532621 0.00368276 0.00443506]]

 [[0.00171672 0.0060513  0.01153807 ... 0.00584741 0.00660444 0.00586163]
  [0.00192736 0.00901196 0.02051492 ... 0.00600136 0.00660377 0.00545984]
  [0.00207088 0.00753044 0.01643743 ... 0.00626619 0.00647977 0.00504232]
  ...
  [0.0091713  0.02111256 0.01395596 ... 0.0059633  0.00369343 0.00340101]
  [0.01336486 0.0310683  0.01867998 ... 0.00570873 0.00363653 0.00381848]
  [0.00833661 0.01802324 0.00984744 ... 0.00608161 0.00409396 0.0043825 ]]]
draw_multiblock_lbp=[[[0.03921569 0.38421569 0.51921569]
  [0.03921569 0.38421569 0.51921569]
  [0.03921569 0.38421569 0.51921569]
  ...
  [0.07843137 0.07843137 0.07843137]
  [0.07843137 0.07843137 0.07843137]
  [0.07843137 0.07843137 0.07843137]]

 [[0.03921569 0.38421569 0.51921569]
  [0.03921569 0.38421569 0.51921569]
  [0.03921569 0.38421569 0.51921569]
  ...
  [0.07843137 0.07843137 0.07843137]
  [0.07843137 0.07843137 0.07843137]
  [0.07843137 0.07843137 0.07843137]]

 [[0.03921569 0.38421569 0.51921569]
  [0.03921569 0.38421569 0.51921569]
  [0.04117647 0.38617647 0.52117647]
  ...
  [0.0745098  0.0745098  0.0745098 ]
  [0.07843137 0.07843137 0.07843137]
  [0.07843137 0.07843137 0.07843137]]

 ...

 [[0.09803922 0.09803922 0.09803922]
  [0.11372549 0.11372549 0.11372549]
  [0.12156863 0.12156863 0.12156863]
  ...
  [0.07058824 0.07058824 0.07058824]
  [0.07058824 0.07058824 0.07058824]
  [0.07058824 0.07058824 0.07058824]]

 [[0.10588235 0.10588235 0.10588235]
  [0.09411765 0.09411765 0.09411765]
  [0.10196078 0.10196078 0.10196078]
  ...
  [0.06666667 0.06666667 0.06666667]
  [0.07058824 0.07058824 0.07058824]
  [0.07843137 0.07843137 0.07843137]]

 [[0.12941176 0.12941176 0.12941176]
  [0.13333333 0.13333333 0.13333333]
  [0.12941176 0.12941176 0.12941176]
  ...
  [0.13333333 0.13333333 0.13333333]
  [0.13333333 0.13333333 0.13333333]
  [0.13333333 0.13333333 0.13333333]]]
==============================
patch_size=(x, 3)
Feature template={feature_template}

=====> Testing: ['patches_mean']
===> Feature subset testing at step: 1/512
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=1>.		Took <total_dur=0.7938511s> in total.<training_dur=0.7233838s>; <prediction_dur=0.0704672s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=80.0000%> with <n_comps=2>.		Took <total_dur=0.7525059s> in total.<training_dur=0.6760255s>; <prediction_dur=0.0764804s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=80.0000%> with <n_comps=3>.		Took <total_dur=0.7210465s> in total.<training_dur=0.6581181s>; <prediction_dur=0.0629284s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=77.5000%> with <n_comps=4>.		Took <total_dur=0.7663107s> in total.<training_dur=0.7029622s>; <prediction_dur=0.0633485s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=75.0000%> with <n_comps=5>.		Took <total_dur=0.7706856s> in total.<training_dur=0.6896926s>; <prediction_dur=0.0809930s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=75.0000%> with <n_comps=6>.		Took <total_dur=0.7637840s> in total.<training_dur=0.7006022s>; <prediction_dur=0.0631818s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=7>.		Took <total_dur=0.7385325s> in total.<training_dur=0.6718611s>; <prediction_dur=0.0666714s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=8>.		Took <total_dur=0.7188316s> in total.<training_dur=0.6581463s>; <prediction_dur=0.0606853s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=9>.		Took <total_dur=0.7367508s> in total.<training_dur=0.6764371s>; <prediction_dur=0.0603137s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=10>.		Took <total_dur=0.7472240s> in total.<training_dur=0.6874741s>; <prediction_dur=0.0597499s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=11>.		Took <total_dur=0.7371902s> in total.<training_dur=0.6766118s>; <prediction_dur=0.0605783s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=12>.		Took <total_dur=0.7737471s> in total.<training_dur=0.7137868s>; <prediction_dur=0.0599603s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=13>.		Took <total_dur=0.7618684s> in total.<training_dur=0.7030062s>; <prediction_dur=0.0588622s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=14>.		Took <total_dur=0.7650777s> in total.<training_dur=0.6921512s>; <prediction_dur=0.0729265s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=15>.		Took <total_dur=0.7650622s> in total.<training_dur=0.7026212s>; <prediction_dur=0.0624410s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=16>.		Took <total_dur=0.8015621s> in total.<training_dur=0.7405996s>; <prediction_dur=0.0609625s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=17>.		Took <total_dur=0.7939537s> in total.<training_dur=0.7321478s>; <prediction_dur=0.0618059s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=18>.		Took <total_dur=0.7774326s> in total.<training_dur=0.7124142s>; <prediction_dur=0.0650184s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=72.5000%> with <n_comps=19>.		Took <total_dur=0.8296131s> in total.<training_dur=0.7599060s>; <prediction_dur=0.0697071s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=75.0000%> with <n_comps=20>.		Took <total_dur=0.7936949s> in total.<training_dur=0.7330204s>; <prediction_dur=0.0606745s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=21>.		Took <total_dur=1.1045032s> in total.<training_dur=1.0948450s>; <prediction_dur=0.0096582s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=22>.		Took <total_dur=1.1037465s> in total.<training_dur=1.0937863s>; <prediction_dur=0.0099602s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=23>.		Took <total_dur=1.0940346s> in total.<training_dur=1.0405305s>; <prediction_dur=0.0535041s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=24>.		Took <total_dur=1.0937325s> in total.<training_dur=1.0570567s>; <prediction_dur=0.0366758s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=25>.		Took <total_dur=0.8446941s> in total.<training_dur=0.7611496s>; <prediction_dur=0.0835445s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=77.5000%> with <n_comps=26>.		Took <total_dur=1.1080571s> in total.<training_dur=1.0976814s>; <prediction_dur=0.0103757s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=72.5000%> with <n_comps=27>.		Took <total_dur=0.9360736s> in total.<training_dur=0.8575074s>; <prediction_dur=0.0785663s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=75.0000%> with <n_comps=28>.		Took <total_dur=0.8073868s> in total.<training_dur=0.7469289s>; <prediction_dur=0.0604579s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=29>.		Took <total_dur=0.7939341s> in total.<training_dur=0.7301695s>; <prediction_dur=0.0637646s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=30>.		Took <total_dur=0.8294107s> in total.<training_dur=0.7650099s>; <prediction_dur=0.0644008s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=31>.		Took <total_dur=0.8452744s> in total.<training_dur=0.7609674s>; <prediction_dur=0.0843070s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=72.5000%> with <n_comps=32>.		Took <total_dur=0.9324907s> in total.<training_dur=0.8701984s>; <prediction_dur=0.0622922s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=72.5000%> with <n_comps=33>.		Took <total_dur=0.8246322s> in total.<training_dur=0.7444521s>; <prediction_dur=0.0801801s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=34>.		Took <total_dur=0.8249476s> in total.<training_dur=0.7588630s>; <prediction_dur=0.0660846s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=35>.		Took <total_dur=0.8270820s> in total.<training_dur=0.7447349s>; <prediction_dur=0.0823471s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=36>.		Took <total_dur=0.9333976s> in total.<training_dur=0.8253259s>; <prediction_dur=0.1080718s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=75.0000%> with <n_comps=37>.		Took <total_dur=1.0914939s> in total.<training_dur=1.0181826s>; <prediction_dur=0.0733112s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=38>.		Took <total_dur=1.0700895s> in total.<training_dur=1.0029804s>; <prediction_dur=0.0671090s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=72.5000%> with <n_comps=39>.		Took <total_dur=0.8429845s> in total.<training_dur=0.7773113s>; <prediction_dur=0.0656732s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=40>.		Took <total_dur=0.9329391s> in total.<training_dur=0.8448847s>; <prediction_dur=0.0880544s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=41>.		Took <total_dur=0.9587829s> in total.<training_dur=0.8959398s>; <prediction_dur=0.0628431s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=42>.		Took <total_dur=0.8552898s> in total.<training_dur=0.7939002s>; <prediction_dur=0.0613896s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=77.5000%> with <n_comps=43>.		Took <total_dur=0.8228907s> in total.<training_dur=0.7605642s>; <prediction_dur=0.0623265s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=44>.		Took <total_dur=1.1123374s> in total.<training_dur=1.1023079s>; <prediction_dur=0.0100295s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=75.0000%> with <n_comps=45>.		Took <total_dur=0.9481406s> in total.<training_dur=0.8846054s>; <prediction_dur=0.0635351s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=46>.		Took <total_dur=0.8577403s> in total.<training_dur=0.7963355s>; <prediction_dur=0.0614048s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=47>.		Took <total_dur=0.8334324s> in total.<training_dur=0.7644359s>; <prediction_dur=0.0689965s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=48>.		Took <total_dur=0.8421997s> in total.<training_dur=0.7823076s>; <prediction_dur=0.0598921s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=49>.		Took <total_dur=0.8621806s> in total.<training_dur=0.7980258s>; <prediction_dur=0.0641548s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=50>.		Took <total_dur=0.8989907s> in total.<training_dur=0.8091263s>; <prediction_dur=0.0898644s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=51>.		Took <total_dur=1.1145471s> in total.<training_dur=1.1045237s>; <prediction_dur=0.0100234s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=52>.		Took <total_dur=1.1112433s> in total.<training_dur=1.1012158s>; <prediction_dur=0.0100275s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=53>.		Took <total_dur=0.8527105s> in total.<training_dur=0.7931642s>; <prediction_dur=0.0595463s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=54>.		Took <total_dur=0.8736585s> in total.<training_dur=0.8127290s>; <prediction_dur=0.0609296s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=55>.		Took <total_dur=0.8851641s> in total.<training_dur=0.8244421s>; <prediction_dur=0.0607220s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=56>.		Took <total_dur=0.8439109s> in total.<training_dur=0.7803022s>; <prediction_dur=0.0636087s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=57>.		Took <total_dur=0.8427190s> in total.<training_dur=0.7811985s>; <prediction_dur=0.0615205s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=58>.		Took <total_dur=1.0872354s> in total.<training_dur=1.0210634s>; <prediction_dur=0.0661720s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=59>.		Took <total_dur=0.8507728s> in total.<training_dur=0.7906989s>; <prediction_dur=0.0600740s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=57.5000%> with <n_comps=60>.		Took <total_dur=0.9083239s> in total.<training_dur=0.8340040s>; <prediction_dur=0.0743199s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=61>.		Took <total_dur=0.8620136s> in total.<training_dur=0.7924461s>; <prediction_dur=0.0695675s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=62>.		Took <total_dur=0.8345154s> in total.<training_dur=0.7741391s>; <prediction_dur=0.0603763s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=63>.		Took <total_dur=0.8634357s> in total.<training_dur=0.8005495s>; <prediction_dur=0.0628862s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=64>.		Took <total_dur=0.9597628s> in total.<training_dur=0.8972019s>; <prediction_dur=0.0625610s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=65>.		Took <total_dur=0.8973549s> in total.<training_dur=0.8284377s>; <prediction_dur=0.0689172s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=66>.		Took <total_dur=1.0940908s> in total.<training_dur=1.0303969s>; <prediction_dur=0.0636939s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=67>.		Took <total_dur=0.9401382s> in total.<training_dur=0.8735262s>; <prediction_dur=0.0666121s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=68>.		Took <total_dur=0.8887877s> in total.<training_dur=0.8275404s>; <prediction_dur=0.0612473s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=72.5000%> with <n_comps=69>.		Took <total_dur=0.8810006s> in total.<training_dur=0.8108733s>; <prediction_dur=0.0701273s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=70>.		Took <total_dur=0.9228896s> in total.<training_dur=0.8562181s>; <prediction_dur=0.0666715s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=71>.		Took <total_dur=0.8865110s> in total.<training_dur=0.8241861s>; <prediction_dur=0.0623249s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=72>.		Took <total_dur=0.9154580s> in total.<training_dur=0.8469293s>; <prediction_dur=0.0685286s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=73>.		Took <total_dur=0.8907427s> in total.<training_dur=0.8301511s>; <prediction_dur=0.0605916s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=74>.		Took <total_dur=0.8708029s> in total.<training_dur=0.8096088s>; <prediction_dur=0.0611940s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=75>.		Took <total_dur=0.8926465s> in total.<training_dur=0.8315233s>; <prediction_dur=0.0611232s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=76>.		Took <total_dur=1.1120109s> in total.<training_dur=1.1020361s>; <prediction_dur=0.0099748s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=77>.		Took <total_dur=1.1170686s> in total.<training_dur=1.1069450s>; <prediction_dur=0.0101235s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=72.5000%> with <n_comps=78>.		Took <total_dur=1.1161949s> in total.<training_dur=1.1062293s>; <prediction_dur=0.0099656s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=79>.		Took <total_dur=1.0961684s> in total.<training_dur=1.0684198s>; <prediction_dur=0.0277486s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=80>.		Took <total_dur=1.1068172s> in total.<training_dur=1.0968823s>; <prediction_dur=0.0099350s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=81>.		Took <total_dur=1.0800875s> in total.<training_dur=1.0152114s>; <prediction_dur=0.0648761s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=82>.		Took <total_dur=0.9033752s> in total.<training_dur=0.8406848s>; <prediction_dur=0.0626904s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=57.5000%> with <n_comps=83>.		Took <total_dur=0.9105259s> in total.<training_dur=0.8470468s>; <prediction_dur=0.0634791s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=84>.		Took <total_dur=0.9166727s> in total.<training_dur=0.8565133s>; <prediction_dur=0.0601594s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=57.5000%> with <n_comps=85>.		Took <total_dur=0.9448019s> in total.<training_dur=0.8752987s>; <prediction_dur=0.0695032s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=86>.		Took <total_dur=0.9367911s> in total.<training_dur=0.8766889s>; <prediction_dur=0.0601022s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=72.5000%> with <n_comps=87>.		Took <total_dur=1.1182715s> in total.<training_dur=1.1083099s>; <prediction_dur=0.0099616s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=88>.		Took <total_dur=1.1190163s> in total.<training_dur=1.1090092s>; <prediction_dur=0.0100071s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=89>.		Took <total_dur=1.1192209s> in total.<training_dur=1.1092634s>; <prediction_dur=0.0099575s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=90>.		Took <total_dur=1.1179874s> in total.<training_dur=1.1081434s>; <prediction_dur=0.0098440s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=91>.		Took <total_dur=1.0177048s> in total.<training_dur=0.9521680s>; <prediction_dur=0.0655368s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=92>.		Took <total_dur=1.1209459s> in total.<training_dur=1.1107651s>; <prediction_dur=0.0101808s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=93>.		Took <total_dur=0.9664487s> in total.<training_dur=0.9033019s>; <prediction_dur=0.0631468s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=94>.		Took <total_dur=0.8950235s> in total.<training_dur=0.8327071s>; <prediction_dur=0.0623164s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=95>.		Took <total_dur=0.9137014s> in total.<training_dur=0.8533001s>; <prediction_dur=0.0604014s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=96>.		Took <total_dur=0.9271529s> in total.<training_dur=0.8368682s>; <prediction_dur=0.0902848s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=97>.		Took <total_dur=1.0204397s> in total.<training_dur=0.9573832s>; <prediction_dur=0.0630565s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=98>.		Took <total_dur=1.1111690s> in total.<training_dur=1.1012965s>; <prediction_dur=0.0098724s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=99>.		Took <total_dur=1.1203020s> in total.<training_dur=1.1101997s>; <prediction_dur=0.0101022s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=57.5000%> with <n_comps=100>.		Took <total_dur=1.1193644s> in total.<training_dur=1.1091098s>; <prediction_dur=0.0102546s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=101>.		Took <total_dur=0.9180337s> in total.<training_dur=0.8560633s>; <prediction_dur=0.0619704s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=102>.		Took <total_dur=0.9430716s> in total.<training_dur=0.8737248s>; <prediction_dur=0.0693468s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=103>.		Took <total_dur=0.9253857s> in total.<training_dur=0.8647317s>; <prediction_dur=0.0606541s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=104>.		Took <total_dur=1.0288386s> in total.<training_dur=0.9406666s>; <prediction_dur=0.0881720s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=57.5000%> with <n_comps=105>.		Took <total_dur=1.0046985s> in total.<training_dur=0.9373947s>; <prediction_dur=0.0673038s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=75.0000%> with <n_comps=106>.		Took <total_dur=0.9728622s> in total.<training_dur=0.9065016s>; <prediction_dur=0.0663606s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=107>.		Took <total_dur=0.9223420s> in total.<training_dur=0.8465209s>; <prediction_dur=0.0758211s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=108>.		Took <total_dur=0.9466035s> in total.<training_dur=0.8785893s>; <prediction_dur=0.0680142s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=109>.		Took <total_dur=0.9102298s> in total.<training_dur=0.8465157s>; <prediction_dur=0.0637141s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=57.5000%> with <n_comps=110>.		Took <total_dur=0.9477977s> in total.<training_dur=0.8872001s>; <prediction_dur=0.0605976s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=111>.		Took <total_dur=0.9696194s> in total.<training_dur=0.9039555s>; <prediction_dur=0.0656640s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=77.5000%> with <n_comps=112>.		Took <total_dur=0.9388234s> in total.<training_dur=0.8772516s>; <prediction_dur=0.0615718s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=113>.		Took <total_dur=0.9421420s> in total.<training_dur=0.8811689s>; <prediction_dur=0.0609731s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=55.0000%> with <n_comps=114>.		Took <total_dur=0.9314150s> in total.<training_dur=0.8711202s>; <prediction_dur=0.0602947s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=115>.		Took <total_dur=0.9493030s> in total.<training_dur=0.8888649s>; <prediction_dur=0.0604381s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=116>.		Took <total_dur=1.1142018s> in total.<training_dur=1.1036790s>; <prediction_dur=0.0105229s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=72.5000%> with <n_comps=117>.		Took <total_dur=1.1228479s> in total.<training_dur=1.1128474s>; <prediction_dur=0.0100005s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=118>.		Took <total_dur=1.0435258s> in total.<training_dur=0.9705541s>; <prediction_dur=0.0729716s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=119>.		Took <total_dur=1.0976041s> in total.<training_dur=1.0568166s>; <prediction_dur=0.0407875s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=120>.		Took <total_dur=1.1224203s> in total.<training_dur=1.1125792s>; <prediction_dur=0.0098411s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=121>.		Took <total_dur=1.0419091s> in total.<training_dur=0.9821710s>; <prediction_dur=0.0597381s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=122>.		Took <total_dur=1.0095776s> in total.<training_dur=0.9483558s>; <prediction_dur=0.0612218s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=123>.		Took <total_dur=1.1256661s> in total.<training_dur=1.1156231s>; <prediction_dur=0.0100430s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=124>.		Took <total_dur=1.1258376s> in total.<training_dur=1.1160367s>; <prediction_dur=0.0098009s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=125>.		Took <total_dur=1.1255913s> in total.<training_dur=1.1156189s>; <prediction_dur=0.0099724s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=126>.		Took <total_dur=1.0453764s> in total.<training_dur=0.9783010s>; <prediction_dur=0.0670754s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=127>.		Took <total_dur=1.0993250s> in total.<training_dur=1.0698329s>; <prediction_dur=0.0294920s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=128>.		Took <total_dur=1.1035143s> in total.<training_dur=1.0934064s>; <prediction_dur=0.0101079s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=72.5000%> with <n_comps=129>.		Took <total_dur=0.9312193s> in total.<training_dur=0.8695483s>; <prediction_dur=0.0616710s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=130>.		Took <total_dur=0.9825938s> in total.<training_dur=0.8937325s>; <prediction_dur=0.0888613s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=131>.		Took <total_dur=0.9439411s> in total.<training_dur=0.8830755s>; <prediction_dur=0.0608656s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=132>.		Took <total_dur=0.9568388s> in total.<training_dur=0.8909380s>; <prediction_dur=0.0659007s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=72.5000%> with <n_comps=133>.		Took <total_dur=1.0102384s> in total.<training_dur=0.9431574s>; <prediction_dur=0.0670810s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=134>.		Took <total_dur=0.9542864s> in total.<training_dur=0.8920795s>; <prediction_dur=0.0622069s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=72.5000%> with <n_comps=135>.		Took <total_dur=0.9835682s> in total.<training_dur=0.9239130s>; <prediction_dur=0.0596551s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=57.5000%> with <n_comps=136>.		Took <total_dur=0.9700967s> in total.<training_dur=0.8975837s>; <prediction_dur=0.0725130s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=137>.		Took <total_dur=0.9632744s> in total.<training_dur=0.8934147s>; <prediction_dur=0.0698597s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=138>.		Took <total_dur=0.9784637s> in total.<training_dur=0.9050754s>; <prediction_dur=0.0733883s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=139>.		Took <total_dur=0.9944290s> in total.<training_dur=0.9207338s>; <prediction_dur=0.0736952s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=140>.		Took <total_dur=1.0204914s> in total.<training_dur=0.9474864s>; <prediction_dur=0.0730050s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=141>.		Took <total_dur=1.0126261s> in total.<training_dur=0.9393382s>; <prediction_dur=0.0732879s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=142>.		Took <total_dur=0.9637637s> in total.<training_dur=0.9037374s>; <prediction_dur=0.0600263s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=143>.		Took <total_dur=0.9419048s> in total.<training_dur=0.8809983s>; <prediction_dur=0.0609065s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=144>.		Took <total_dur=1.0059625s> in total.<training_dur=0.9308894s>; <prediction_dur=0.0750731s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=145>.		Took <total_dur=0.9779685s> in total.<training_dur=0.9169485s>; <prediction_dur=0.0610200s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=146>.		Took <total_dur=1.0232348s> in total.<training_dur=0.9520226s>; <prediction_dur=0.0712122s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=147>.		Took <total_dur=1.0059680s> in total.<training_dur=0.9404226s>; <prediction_dur=0.0655454s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=148>.		Took <total_dur=1.1309787s> in total.<training_dur=1.1208658s>; <prediction_dur=0.0101129s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=149>.		Took <total_dur=1.0726520s> in total.<training_dur=1.0110108s>; <prediction_dur=0.0616412s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=150>.		Took <total_dur=0.9758737s> in total.<training_dur=0.9139466s>; <prediction_dur=0.0619271s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=151>.		Took <total_dur=0.9866690s> in total.<training_dur=0.9227890s>; <prediction_dur=0.0638801s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=152>.		Took <total_dur=1.0038019s> in total.<training_dur=0.9346269s>; <prediction_dur=0.0691750s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=153>.		Took <total_dur=1.0058273s> in total.<training_dur=0.9348484s>; <prediction_dur=0.0709789s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=55.0000%> with <n_comps=154>.		Took <total_dur=0.9852233s> in total.<training_dur=0.9106820s>; <prediction_dur=0.0745413s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=47.5000%> with <n_comps=155>.		Took <total_dur=1.0116723s> in total.<training_dur=0.9494205s>; <prediction_dur=0.0622518s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=156>.		Took <total_dur=1.0176642s> in total.<training_dur=0.9436355s>; <prediction_dur=0.0740287s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=55.0000%> with <n_comps=157>.		Took <total_dur=1.0248730s> in total.<training_dur=0.9609801s>; <prediction_dur=0.0638929s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=158>.		Took <total_dur=1.0413695s> in total.<training_dur=0.9500232s>; <prediction_dur=0.0913463s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=159>.		Took <total_dur=0.9662238s> in total.<training_dur=0.9039960s>; <prediction_dur=0.0622278s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=160>.		Took <total_dur=0.9921221s> in total.<training_dur=0.9313115s>; <prediction_dur=0.0608107s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=57.5000%> with <n_comps=161>.		Took <total_dur=1.0356597s> in total.<training_dur=0.9597536s>; <prediction_dur=0.0759061s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=162>.		Took <total_dur=1.0972493s> in total.<training_dur=1.0860919s>; <prediction_dur=0.0111573s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=163>.		Took <total_dur=0.9864586s> in total.<training_dur=0.9241571s>; <prediction_dur=0.0623015s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=164>.		Took <total_dur=1.1045786s> in total.<training_dur=1.0944398s>; <prediction_dur=0.0101388s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=165>.		Took <total_dur=1.0880570s> in total.<training_dur=1.0365431s>; <prediction_dur=0.0515139s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=50.0000%> with <n_comps=166>.		Took <total_dur=1.0096816s> in total.<training_dur=0.9392617s>; <prediction_dur=0.0704199s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=167>.		Took <total_dur=1.0988353s> in total.<training_dur=1.0778387s>; <prediction_dur=0.0209966s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=168>.		Took <total_dur=0.9856460s> in total.<training_dur=0.9193809s>; <prediction_dur=0.0662651s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=169>.		Took <total_dur=1.1247218s> in total.<training_dur=1.1148258s>; <prediction_dur=0.0098961s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=75.0000%> with <n_comps=170>.		Took <total_dur=1.0441704s> in total.<training_dur=0.9664861s>; <prediction_dur=0.0776844s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=171>.		Took <total_dur=1.0159657s> in total.<training_dur=0.9500924s>; <prediction_dur=0.0658734s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=172>.		Took <total_dur=1.0387613s> in total.<training_dur=0.9754916s>; <prediction_dur=0.0632698s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=57.5000%> with <n_comps=173>.		Took <total_dur=1.0257810s> in total.<training_dur=0.9594453s>; <prediction_dur=0.0663357s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=57.5000%> with <n_comps=174>.		Took <total_dur=1.0314471s> in total.<training_dur=0.9671966s>; <prediction_dur=0.0642505s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=175>.		Took <total_dur=1.0295210s> in total.<training_dur=0.9616298s>; <prediction_dur=0.0678913s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=72.5000%> with <n_comps=176>.		Took <total_dur=0.9935712s> in total.<training_dur=0.9328300s>; <prediction_dur=0.0607412s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=177>.		Took <total_dur=0.9956962s> in total.<training_dur=0.9339530s>; <prediction_dur=0.0617432s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=178>.		Took <total_dur=1.1306062s> in total.<training_dur=1.1204558s>; <prediction_dur=0.0101504s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=72.5000%> with <n_comps=179>.		Took <total_dur=1.1113080s> in total.<training_dur=1.1014999s>; <prediction_dur=0.0098081s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=180>.		Took <total_dur=1.0040827s> in total.<training_dur=0.9432576s>; <prediction_dur=0.0608250s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=55.0000%> with <n_comps=181>.		Took <total_dur=1.1349914s> in total.<training_dur=1.1252046s>; <prediction_dur=0.0097868s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=75.0000%> with <n_comps=182>.		Took <total_dur=1.1279343s> in total.<training_dur=1.1178707s>; <prediction_dur=0.0100637s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=183>.		Took <total_dur=1.1304734s> in total.<training_dur=1.1205344s>; <prediction_dur=0.0099390s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=184>.		Took <total_dur=1.0941006s> in total.<training_dur=1.0544266s>; <prediction_dur=0.0396740s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=185>.		Took <total_dur=1.0994751s> in total.<training_dur=1.0718713s>; <prediction_dur=0.0276037s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=186>.		Took <total_dur=1.0123748s> in total.<training_dur=0.9507693s>; <prediction_dur=0.0616055s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=187>.		Took <total_dur=1.0028361s> in total.<training_dur=0.9418775s>; <prediction_dur=0.0609586s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=188>.		Took <total_dur=1.1309201s> in total.<training_dur=1.1208673s>; <prediction_dur=0.0100528s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=189>.		Took <total_dur=1.1297126s> in total.<training_dur=1.1196158s>; <prediction_dur=0.0100968s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=190>.		Took <total_dur=1.1207111s> in total.<training_dur=1.1094271s>; <prediction_dur=0.0112840s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=191>.		Took <total_dur=1.1312932s> in total.<training_dur=1.1211419s>; <prediction_dur=0.0101513s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=192>.		Took <total_dur=1.1315724s> in total.<training_dur=1.1213049s>; <prediction_dur=0.0102675s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=193>.		Took <total_dur=1.1314254s> in total.<training_dur=1.1212317s>; <prediction_dur=0.0101937s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=194>.		Took <total_dur=1.1341809s> in total.<training_dur=1.1229683s>; <prediction_dur=0.0112126s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=195>.		Took <total_dur=1.0768576s> in total.<training_dur=1.0074938s>; <prediction_dur=0.0693637s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=196>.		Took <total_dur=1.0680572s> in total.<training_dur=0.9947811s>; <prediction_dur=0.0732761s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=197>.		Took <total_dur=1.0112488s> in total.<training_dur=0.9498949s>; <prediction_dur=0.0613540s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=198>.		Took <total_dur=1.0962992s> in total.<training_dur=1.0564967s>; <prediction_dur=0.0398025s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=57.5000%> with <n_comps=199>.		Took <total_dur=1.0463712s> in total.<training_dur=0.9695060s>; <prediction_dur=0.0768652s>).

=====> Testing: ['patches_std']
===> Feature subset testing at step: 2/512
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=45.0000%> with <n_comps=1>.		Took <total_dur=1.0957819s> in total.<training_dur=1.0424395s>; <prediction_dur=0.0533424s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=42.5000%> with <n_comps=2>.		Took <total_dur=1.0944227s> in total.<training_dur=1.0397704s>; <prediction_dur=0.0546524s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=47.5000%> with <n_comps=3>.		Took <total_dur=1.0946018s> in total.<training_dur=1.0334954s>; <prediction_dur=0.0611064s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=4>.		Took <total_dur=1.0985990s> in total.<training_dur=1.0722496s>; <prediction_dur=0.0263495s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=50.0000%> with <n_comps=5>.		Took <total_dur=1.0965275s> in total.<training_dur=1.0673292s>; <prediction_dur=0.0291984s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=45.0000%> with <n_comps=6>.		Took <total_dur=1.0981831s> in total.<training_dur=1.0691103s>; <prediction_dur=0.0290728s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=7>.		Took <total_dur=1.0964938s> in total.<training_dur=1.0672180s>; <prediction_dur=0.0292758s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=47.5000%> with <n_comps=8>.		Took <total_dur=1.0967196s> in total.<training_dur=1.0668694s>; <prediction_dur=0.0298502s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=9>.		Took <total_dur=1.1019878s> in total.<training_dur=1.0913103s>; <prediction_dur=0.0106775s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=10>.		Took <total_dur=1.1023912s> in total.<training_dur=1.0919481s>; <prediction_dur=0.0104431s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=57.5000%> with <n_comps=11>.		Took <total_dur=1.1015859s> in total.<training_dur=1.0909790s>; <prediction_dur=0.0106069s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=57.5000%> with <n_comps=12>.		Took <total_dur=1.1023690s> in total.<training_dur=1.0917252s>; <prediction_dur=0.0106439s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=13>.		Took <total_dur=1.0990736s> in total.<training_dur=1.0762332s>; <prediction_dur=0.0228404s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=47.5000%> with <n_comps=14>.		Took <total_dur=1.0961118s> in total.<training_dur=1.0485462s>; <prediction_dur=0.0475656s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=15>.		Took <total_dur=0.7847733s> in total.<training_dur=0.7225526s>; <prediction_dur=0.0622207s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=45.0000%> with <n_comps=16>.		Took <total_dur=0.7837764s> in total.<training_dur=0.7216513s>; <prediction_dur=0.0621250s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=47.5000%> with <n_comps=17>.		Took <total_dur=0.9755321s> in total.<training_dur=0.9109181s>; <prediction_dur=0.0646141s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=18>.		Took <total_dur=0.7898206s> in total.<training_dur=0.7294301s>; <prediction_dur=0.0603905s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=19>.		Took <total_dur=0.7831007s> in total.<training_dur=0.7212346s>; <prediction_dur=0.0618662s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=55.0000%> with <n_comps=20>.		Took <total_dur=0.9214843s> in total.<training_dur=0.8441539s>; <prediction_dur=0.0773304s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=21>.		Took <total_dur=1.0970013s> in total.<training_dur=1.0498728s>; <prediction_dur=0.0471285s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=42.5000%> with <n_comps=22>.		Took <total_dur=1.0957137s> in total.<training_dur=1.0521472s>; <prediction_dur=0.0435664s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=23>.		Took <total_dur=1.0005761s> in total.<training_dur=0.9332764s>; <prediction_dur=0.0672997s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=24>.		Took <total_dur=1.1034479s> in total.<training_dur=1.0936178s>; <prediction_dur=0.0098301s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=57.5000%> with <n_comps=25>.		Took <total_dur=1.0560496s> in total.<training_dur=0.9941176s>; <prediction_dur=0.0619320s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=26>.		Took <total_dur=0.8326256s> in total.<training_dur=0.7711395s>; <prediction_dur=0.0614860s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=45.0000%> with <n_comps=27>.		Took <total_dur=0.8263905s> in total.<training_dur=0.7582740s>; <prediction_dur=0.0681166s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=42.5000%> with <n_comps=28>.		Took <total_dur=0.8639189s> in total.<training_dur=0.7951589s>; <prediction_dur=0.0687601s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=29>.		Took <total_dur=0.8675810s> in total.<training_dur=0.8042080s>; <prediction_dur=0.0633730s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=47.5000%> with <n_comps=30>.		Took <total_dur=0.8001460s> in total.<training_dur=0.7370424s>; <prediction_dur=0.0631037s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=31>.		Took <total_dur=0.8296946s> in total.<training_dur=0.7657216s>; <prediction_dur=0.0639730s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=55.0000%> with <n_comps=32>.		Took <total_dur=0.8273473s> in total.<training_dur=0.7620536s>; <prediction_dur=0.0652938s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=57.5000%> with <n_comps=33>.		Took <total_dur=0.8131746s> in total.<training_dur=0.7405138s>; <prediction_dur=0.0726608s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=47.5000%> with <n_comps=34>.		Took <total_dur=0.8424688s> in total.<training_dur=0.7782279s>; <prediction_dur=0.0642409s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=35>.		Took <total_dur=0.8320762s> in total.<training_dur=0.7697226s>; <prediction_dur=0.0623537s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=55.0000%> with <n_comps=36>.		Took <total_dur=0.8395617s> in total.<training_dur=0.7783893s>; <prediction_dur=0.0611725s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=37>.		Took <total_dur=1.0116536s> in total.<training_dur=0.9236602s>; <prediction_dur=0.0879933s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=50.0000%> with <n_comps=38>.		Took <total_dur=1.1088687s> in total.<training_dur=1.0983485s>; <prediction_dur=0.0105202s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=39>.		Took <total_dur=1.1106152s> in total.<training_dur=1.1007515s>; <prediction_dur=0.0098637s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=40>.		Took <total_dur=1.0118138s> in total.<training_dur=0.9524527s>; <prediction_dur=0.0593611s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=42.5000%> with <n_comps=41>.		Took <total_dur=0.9763805s> in total.<training_dur=0.9108769s>; <prediction_dur=0.0655036s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=45.0000%> with <n_comps=42>.		Took <total_dur=0.8638193s> in total.<training_dur=0.7759457s>; <prediction_dur=0.0878736s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=50.0000%> with <n_comps=43>.		Took <total_dur=1.1039304s> in total.<training_dur=1.0924114s>; <prediction_dur=0.0115189s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=44>.		Took <total_dur=0.8526399s> in total.<training_dur=0.7910743s>; <prediction_dur=0.0615656s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=45>.		Took <total_dur=0.8345053s> in total.<training_dur=0.7718555s>; <prediction_dur=0.0626498s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=47.5000%> with <n_comps=46>.		Took <total_dur=0.8743962s> in total.<training_dur=0.8118605s>; <prediction_dur=0.0625356s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=42.5000%> with <n_comps=47>.		Took <total_dur=0.8721964s> in total.<training_dur=0.8116403s>; <prediction_dur=0.0605561s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=48>.		Took <total_dur=0.8285805s> in total.<training_dur=0.7646308s>; <prediction_dur=0.0639497s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=49>.		Took <total_dur=0.8496512s> in total.<training_dur=0.7890409s>; <prediction_dur=0.0606103s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=55.0000%> with <n_comps=50>.		Took <total_dur=0.8991211s> in total.<training_dur=0.8390056s>; <prediction_dur=0.0601155s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=51>.		Took <total_dur=0.8370119s> in total.<training_dur=0.7713553s>; <prediction_dur=0.0656566s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=50.0000%> with <n_comps=52>.		Took <total_dur=0.8754796s> in total.<training_dur=0.8123151s>; <prediction_dur=0.0631645s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=55.0000%> with <n_comps=53>.		Took <total_dur=1.0990408s> in total.<training_dur=1.0795112s>; <prediction_dur=0.0195295s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=54>.		Took <total_dur=1.1140938s> in total.<training_dur=1.1040751s>; <prediction_dur=0.0100187s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=55.0000%> with <n_comps=55>.		Took <total_dur=1.0973276s> in total.<training_dur=1.0682273s>; <prediction_dur=0.0291003s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=57.5000%> with <n_comps=56>.		Took <total_dur=0.9863124s> in total.<training_dur=0.8963205s>; <prediction_dur=0.0899918s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=50.0000%> with <n_comps=57>.		Took <total_dur=1.1001799s> in total.<training_dur=1.0901335s>; <prediction_dur=0.0100464s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=58>.		Took <total_dur=0.9794710s> in total.<training_dur=0.9182349s>; <prediction_dur=0.0612361s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=59>.		Took <total_dur=0.8722813s> in total.<training_dur=0.8065370s>; <prediction_dur=0.0657443s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=57.5000%> with <n_comps=60>.		Took <total_dur=0.8863644s> in total.<training_dur=0.8147524s>; <prediction_dur=0.0716120s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=61>.		Took <total_dur=0.8873448s> in total.<training_dur=0.8218246s>; <prediction_dur=0.0655202s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=62>.		Took <total_dur=0.8761639s> in total.<training_dur=0.8143691s>; <prediction_dur=0.0617948s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=63>.		Took <total_dur=0.9009393s> in total.<training_dur=0.8364042s>; <prediction_dur=0.0645352s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=64>.		Took <total_dur=1.1123260s> in total.<training_dur=1.1022196s>; <prediction_dur=0.0101063s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=40.0000%> with <n_comps=65>.		Took <total_dur=1.1170266s> in total.<training_dur=1.1064773s>; <prediction_dur=0.0105493s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=42.5000%> with <n_comps=66>.		Took <total_dur=1.1169817s> in total.<training_dur=1.1066793s>; <prediction_dur=0.0103024s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=57.5000%> with <n_comps=67>.		Took <total_dur=1.1171986s> in total.<training_dur=1.1069830s>; <prediction_dur=0.0102157s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=57.5000%> with <n_comps=68>.		Took <total_dur=1.0098733s> in total.<training_dur=0.9445788s>; <prediction_dur=0.0652945s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=55.0000%> with <n_comps=69>.		Took <total_dur=0.9100836s> in total.<training_dur=0.8441756s>; <prediction_dur=0.0659080s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=45.0000%> with <n_comps=70>.		Took <total_dur=0.9277925s> in total.<training_dur=0.8613090s>; <prediction_dur=0.0664835s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=71>.		Took <total_dur=0.9168872s> in total.<training_dur=0.8522456s>; <prediction_dur=0.0646416s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=72>.		Took <total_dur=0.9138949s> in total.<training_dur=0.8506369s>; <prediction_dur=0.0632580s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=55.0000%> with <n_comps=73>.		Took <total_dur=0.9007640s> in total.<training_dur=0.8218436s>; <prediction_dur=0.0789204s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=74>.		Took <total_dur=0.9527077s> in total.<training_dur=0.8797737s>; <prediction_dur=0.0729340s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=75>.		Took <total_dur=1.0249163s> in total.<training_dur=0.9568173s>; <prediction_dur=0.0680990s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=76>.		Took <total_dur=0.9035576s> in total.<training_dur=0.8374041s>; <prediction_dur=0.0661535s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=50.0000%> with <n_comps=77>.		Took <total_dur=0.9334986s> in total.<training_dur=0.8433783s>; <prediction_dur=0.0901203s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=78>.		Took <total_dur=1.1149547s> in total.<training_dur=1.1044108s>; <prediction_dur=0.0105439s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=79>.		Took <total_dur=1.1209270s> in total.<training_dur=1.1092005s>; <prediction_dur=0.0117265s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=50.0000%> with <n_comps=80>.		Took <total_dur=1.1176958s> in total.<training_dur=1.1074126s>; <prediction_dur=0.0102832s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=81>.		Took <total_dur=0.9605461s> in total.<training_dur=0.8910798s>; <prediction_dur=0.0694663s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=47.5000%> with <n_comps=82>.		Took <total_dur=1.0067373s> in total.<training_dur=0.9464105s>; <prediction_dur=0.0603267s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=83>.		Took <total_dur=1.0919913s> in total.<training_dur=1.0403613s>; <prediction_dur=0.0516300s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=55.0000%> with <n_comps=84>.		Took <total_dur=0.9973288s> in total.<training_dur=0.8898919s>; <prediction_dur=0.1074368s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=50.0000%> with <n_comps=85>.		Took <total_dur=0.9477832s> in total.<training_dur=0.8876865s>; <prediction_dur=0.0600966s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=86>.		Took <total_dur=1.1072876s> in total.<training_dur=1.0973431s>; <prediction_dur=0.0099444s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=55.0000%> with <n_comps=87>.		Took <total_dur=1.0826260s> in total.<training_dur=1.0223880s>; <prediction_dur=0.0602380s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=50.0000%> with <n_comps=88>.		Took <total_dur=0.9171297s> in total.<training_dur=0.8484239s>; <prediction_dur=0.0687058s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=89>.		Took <total_dur=1.1186985s> in total.<training_dur=1.1084109s>; <prediction_dur=0.0102876s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=90>.		Took <total_dur=1.1175908s> in total.<training_dur=1.1077565s>; <prediction_dur=0.0098343s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=50.0000%> with <n_comps=91>.		Took <total_dur=1.1097364s> in total.<training_dur=1.0997737s>; <prediction_dur=0.0099627s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=92>.		Took <total_dur=1.0170728s> in total.<training_dur=0.9471844s>; <prediction_dur=0.0698884s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=93>.		Took <total_dur=0.9143529s> in total.<training_dur=0.8536369s>; <prediction_dur=0.0607160s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=47.5000%> with <n_comps=94>.		Took <total_dur=0.9123234s> in total.<training_dur=0.8505953s>; <prediction_dur=0.0617281s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=95>.		Took <total_dur=0.9065709s> in total.<training_dur=0.8376247s>; <prediction_dur=0.0689462s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=96>.		Took <total_dur=0.9007154s> in total.<training_dur=0.8339713s>; <prediction_dur=0.0667441s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=97>.		Took <total_dur=0.9014625s> in total.<training_dur=0.8419608s>; <prediction_dur=0.0595017s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=57.5000%> with <n_comps=98>.		Took <total_dur=0.9762286s> in total.<training_dur=0.9133048s>; <prediction_dur=0.0629238s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=57.5000%> with <n_comps=99>.		Took <total_dur=0.9368031s> in total.<training_dur=0.8752231s>; <prediction_dur=0.0615800s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=100>.		Took <total_dur=0.9370153s> in total.<training_dur=0.8764883s>; <prediction_dur=0.0605270s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=101>.		Took <total_dur=0.9558083s> in total.<training_dur=0.8871975s>; <prediction_dur=0.0686108s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=102>.		Took <total_dur=0.9619811s> in total.<training_dur=0.9014997s>; <prediction_dur=0.0604814s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=57.5000%> with <n_comps=103>.		Took <total_dur=0.9368728s> in total.<training_dur=0.8735941s>; <prediction_dur=0.0632787s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=47.5000%> with <n_comps=104>.		Took <total_dur=0.9367159s> in total.<training_dur=0.8716910s>; <prediction_dur=0.0650249s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=42.5000%> with <n_comps=105>.		Took <total_dur=0.9413885s> in total.<training_dur=0.8793002s>; <prediction_dur=0.0620883s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=50.0000%> with <n_comps=106>.		Took <total_dur=0.9534124s> in total.<training_dur=0.8901433s>; <prediction_dur=0.0632691s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=72.5000%> with <n_comps=107>.		Took <total_dur=0.9662932s> in total.<training_dur=0.9044587s>; <prediction_dur=0.0618345s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=40.0000%> with <n_comps=108>.		Took <total_dur=1.1235122s> in total.<training_dur=1.1121446s>; <prediction_dur=0.0113676s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=109>.		Took <total_dur=0.9275711s> in total.<training_dur=0.8624294s>; <prediction_dur=0.0651417s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=47.5000%> with <n_comps=110>.		Took <total_dur=1.0470226s> in total.<training_dur=0.9859587s>; <prediction_dur=0.0610639s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=42.5000%> with <n_comps=111>.		Took <total_dur=1.0039148s> in total.<training_dur=0.9143418s>; <prediction_dur=0.0895730s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=55.0000%> with <n_comps=112>.		Took <total_dur=0.9381425s> in total.<training_dur=0.8707946s>; <prediction_dur=0.0673480s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=50.0000%> with <n_comps=113>.		Took <total_dur=1.0856186s> in total.<training_dur=1.0172026s>; <prediction_dur=0.0684161s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=50.0000%> with <n_comps=114>.		Took <total_dur=1.1124997s> in total.<training_dur=1.1020165s>; <prediction_dur=0.0104833s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=115>.		Took <total_dur=1.1227203s> in total.<training_dur=1.1126117s>; <prediction_dur=0.0101086s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=57.5000%> with <n_comps=116>.		Took <total_dur=1.1215007s> in total.<training_dur=1.1116279s>; <prediction_dur=0.0098728s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=55.0000%> with <n_comps=117>.		Took <total_dur=1.1219965s> in total.<training_dur=1.1117059s>; <prediction_dur=0.0102906s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=42.5000%> with <n_comps=118>.		Took <total_dur=0.9411718s> in total.<training_dur=0.8741717s>; <prediction_dur=0.0670001s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=45.0000%> with <n_comps=119>.		Took <total_dur=1.0998554s> in total.<training_dur=1.0890418s>; <prediction_dur=0.0108136s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=120>.		Took <total_dur=0.9604382s> in total.<training_dur=0.8934289s>; <prediction_dur=0.0670094s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=55.0000%> with <n_comps=121>.		Took <total_dur=0.9882293s> in total.<training_dur=0.9241000s>; <prediction_dur=0.0641293s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=30.0000%> with <n_comps=122>.		Took <total_dur=0.9803960s> in total.<training_dur=0.9135900s>; <prediction_dur=0.0668060s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=45.0000%> with <n_comps=123>.		Took <total_dur=1.0043181s> in total.<training_dur=0.9388510s>; <prediction_dur=0.0654670s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=45.0000%> with <n_comps=124>.		Took <total_dur=1.0191543s> in total.<training_dur=0.9540969s>; <prediction_dur=0.0650575s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=42.5000%> with <n_comps=125>.		Took <total_dur=1.1157052s> in total.<training_dur=1.1037007s>; <prediction_dur=0.0120046s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=50.0000%> with <n_comps=126>.		Took <total_dur=0.9669811s> in total.<training_dur=0.8965680s>; <prediction_dur=0.0704131s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=127>.		Took <total_dur=1.1254290s> in total.<training_dur=1.1154063s>; <prediction_dur=0.0100227s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=128>.		Took <total_dur=1.1251697s> in total.<training_dur=1.1152952s>; <prediction_dur=0.0098745s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=45.0000%> with <n_comps=129>.		Took <total_dur=1.1252972s> in total.<training_dur=1.1153490s>; <prediction_dur=0.0099482s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=57.5000%> with <n_comps=130>.		Took <total_dur=1.1158416s> in total.<training_dur=1.1058172s>; <prediction_dur=0.0100244s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=57.5000%> with <n_comps=131>.		Took <total_dur=0.9587244s> in total.<training_dur=0.8986258s>; <prediction_dur=0.0600986s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=132>.		Took <total_dur=0.9848112s> in total.<training_dur=0.9138950s>; <prediction_dur=0.0709162s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=50.0000%> with <n_comps=133>.		Took <total_dur=0.9977291s> in total.<training_dur=0.9359367s>; <prediction_dur=0.0617924s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=47.5000%> with <n_comps=134>.		Took <total_dur=1.1147447s> in total.<training_dur=1.1041723s>; <prediction_dur=0.0105724s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=37.5000%> with <n_comps=135>.		Took <total_dur=1.1261807s> in total.<training_dur=1.1160677s>; <prediction_dur=0.0101130s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=40.0000%> with <n_comps=136>.		Took <total_dur=1.1241820s> in total.<training_dur=1.1141649s>; <prediction_dur=0.0100170s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=50.0000%> with <n_comps=137>.		Took <total_dur=1.0083429s> in total.<training_dur=0.9424824s>; <prediction_dur=0.0658604s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=138>.		Took <total_dur=0.9873337s> in total.<training_dur=0.9201986s>; <prediction_dur=0.0671351s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=47.5000%> with <n_comps=139>.		Took <total_dur=0.9575946s> in total.<training_dur=0.8974291s>; <prediction_dur=0.0601655s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=55.0000%> with <n_comps=140>.		Took <total_dur=1.0891025s> in total.<training_dur=0.9960348s>; <prediction_dur=0.0930676s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=40.0000%> with <n_comps=141>.		Took <total_dur=1.1244464s> in total.<training_dur=1.1142427s>; <prediction_dur=0.0102037s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=37.5000%> with <n_comps=142>.		Took <total_dur=1.1245278s> in total.<training_dur=1.1137827s>; <prediction_dur=0.0107451s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=35.0000%> with <n_comps=143>.		Took <total_dur=1.1231664s> in total.<training_dur=1.1131477s>; <prediction_dur=0.0100187s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=50.0000%> with <n_comps=144>.		Took <total_dur=1.1233237s> in total.<training_dur=1.1131181s>; <prediction_dur=0.0102056s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=145>.		Took <total_dur=1.0148486s> in total.<training_dur=0.9415583s>; <prediction_dur=0.0732903s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=50.0000%> with <n_comps=146>.		Took <total_dur=1.0250924s> in total.<training_dur=0.9511883s>; <prediction_dur=0.0739041s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=147>.		Took <total_dur=1.1046806s> in total.<training_dur=1.0944925s>; <prediction_dur=0.0101881s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=55.0000%> with <n_comps=148>.		Took <total_dur=1.1278054s> in total.<training_dur=1.1176384s>; <prediction_dur=0.0101669s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=149>.		Took <total_dur=1.0483502s> in total.<training_dur=0.9866705s>; <prediction_dur=0.0616797s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=55.0000%> with <n_comps=150>.		Took <total_dur=0.9578355s> in total.<training_dur=0.8930849s>; <prediction_dur=0.0647505s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=55.0000%> with <n_comps=151>.		Took <total_dur=1.0338659s> in total.<training_dur=1.0226079s>; <prediction_dur=0.0112580s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=152>.		Took <total_dur=1.0226058s> in total.<training_dur=0.9622660s>; <prediction_dur=0.0603398s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=50.0000%> with <n_comps=153>.		Took <total_dur=1.0525876s> in total.<training_dur=0.9919649s>; <prediction_dur=0.0606226s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=55.0000%> with <n_comps=154>.		Took <total_dur=0.9998838s> in total.<training_dur=0.9337382s>; <prediction_dur=0.0661455s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=55.0000%> with <n_comps=155>.		Took <total_dur=1.0041331s> in total.<training_dur=0.9370286s>; <prediction_dur=0.0671045s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=40.0000%> with <n_comps=156>.		Took <total_dur=0.9987208s> in total.<training_dur=0.9299501s>; <prediction_dur=0.0687708s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=40.0000%> with <n_comps=157>.		Took <total_dur=1.1445753s> in total.<training_dur=1.1329830s>; <prediction_dur=0.0115922s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=42.5000%> with <n_comps=158>.		Took <total_dur=1.1334561s> in total.<training_dur=1.1183176s>; <prediction_dur=0.0151385s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=37.5000%> with <n_comps=159>.		Took <total_dur=1.1279394s> in total.<training_dur=1.1144391s>; <prediction_dur=0.0135004s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=42.5000%> with <n_comps=160>.		Took <total_dur=1.0433115s> in total.<training_dur=0.9719814s>; <prediction_dur=0.0713301s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=35.0000%> with <n_comps=161>.		Took <total_dur=0.9707374s> in total.<training_dur=0.9597580s>; <prediction_dur=0.0109794s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=37.5000%> with <n_comps=162>.		Took <total_dur=1.0239656s> in total.<training_dur=0.9582477s>; <prediction_dur=0.0657179s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=163>.		Took <total_dur=1.0574099s> in total.<training_dur=1.0431355s>; <prediction_dur=0.0142744s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=164>.		Took <total_dur=1.0264037s> in total.<training_dur=1.0156993s>; <prediction_dur=0.0107044s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=47.5000%> with <n_comps=165>.		Took <total_dur=1.0440425s> in total.<training_dur=1.0336943s>; <prediction_dur=0.0103482s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=166>.		Took <total_dur=1.1393145s> in total.<training_dur=1.1214446s>; <prediction_dur=0.0178699s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=50.0000%> with <n_comps=167>.		Took <total_dur=1.1010081s> in total.<training_dur=1.0763869s>; <prediction_dur=0.0246212s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=50.0000%> with <n_comps=168>.		Took <total_dur=1.0784304s> in total.<training_dur=1.0676928s>; <prediction_dur=0.0107376s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=169>.		Took <total_dur=1.0787081s> in total.<training_dur=1.0686426s>; <prediction_dur=0.0100655s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=37.5000%> with <n_comps=170>.		Took <total_dur=1.0723097s> in total.<training_dur=1.0622212s>; <prediction_dur=0.0100885s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=47.5000%> with <n_comps=171>.		Took <total_dur=1.0521688s> in total.<training_dur=1.0419900s>; <prediction_dur=0.0101787s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=42.5000%> with <n_comps=172>.		Took <total_dur=1.0163395s> in total.<training_dur=1.0061581s>; <prediction_dur=0.0101814s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=42.5000%> with <n_comps=173>.		Took <total_dur=1.1363684s> in total.<training_dur=1.1187749s>; <prediction_dur=0.0175934s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=55.0000%> with <n_comps=174>.		Took <total_dur=1.1229209s> in total.<training_dur=1.1061150s>; <prediction_dur=0.0168059s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=47.5000%> with <n_comps=175>.		Took <total_dur=1.0003672s> in total.<training_dur=0.9324151s>; <prediction_dur=0.0679521s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=57.5000%> with <n_comps=176>.		Took <total_dur=1.0394172s> in total.<training_dur=0.9469608s>; <prediction_dur=0.0924564s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=177>.		Took <total_dur=1.1293222s> in total.<training_dur=1.1192588s>; <prediction_dur=0.0100634s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=178>.		Took <total_dur=1.1256956s> in total.<training_dur=1.1152292s>; <prediction_dur=0.0104664s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=42.5000%> with <n_comps=179>.		Took <total_dur=1.0046410s> in total.<training_dur=0.9402126s>; <prediction_dur=0.0644284s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=180>.		Took <total_dur=1.1100589s> in total.<training_dur=1.1000614s>; <prediction_dur=0.0099974s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=55.0000%> with <n_comps=181>.		Took <total_dur=1.0990664s> in total.<training_dur=1.0879576s>; <prediction_dur=0.0111088s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=47.5000%> with <n_comps=182>.		Took <total_dur=1.0192385s> in total.<training_dur=0.9559577s>; <prediction_dur=0.0632808s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=183>.		Took <total_dur=1.0059807s> in total.<training_dur=0.9422981s>; <prediction_dur=0.0636827s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=55.0000%> with <n_comps=184>.		Took <total_dur=1.0150515s> in total.<training_dur=0.9388072s>; <prediction_dur=0.0762443s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=185>.		Took <total_dur=1.0832356s> in total.<training_dur=1.0169490s>; <prediction_dur=0.0662866s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=47.5000%> with <n_comps=186>.		Took <total_dur=1.0580336s> in total.<training_dur=0.9908785s>; <prediction_dur=0.0671551s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=187>.		Took <total_dur=1.0567929s> in total.<training_dur=0.9842347s>; <prediction_dur=0.0725583s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=188>.		Took <total_dur=1.0007364s> in total.<training_dur=0.9362893s>; <prediction_dur=0.0644471s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=50.0000%> with <n_comps=189>.		Took <total_dur=1.0268554s> in total.<training_dur=0.9338900s>; <prediction_dur=0.0929654s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=55.0000%> with <n_comps=190>.		Took <total_dur=1.1083271s> in total.<training_dur=1.0970875s>; <prediction_dur=0.0112396s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=57.5000%> with <n_comps=191>.		Took <total_dur=1.1134865s> in total.<training_dur=1.1034385s>; <prediction_dur=0.0100479s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=192>.		Took <total_dur=1.0706898s> in total.<training_dur=1.0605490s>; <prediction_dur=0.0101408s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=45.0000%> with <n_comps=193>.		Took <total_dur=1.1411500s> in total.<training_dur=1.1282870s>; <prediction_dur=0.0128630s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=45.0000%> with <n_comps=194>.		Took <total_dur=1.0794398s> in total.<training_dur=1.0177634s>; <prediction_dur=0.0616764s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=57.5000%> with <n_comps=195>.		Took <total_dur=1.1348747s> in total.<training_dur=1.1243234s>; <prediction_dur=0.0105514s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=50.0000%> with <n_comps=196>.		Took <total_dur=1.0334789s> in total.<training_dur=0.9665990s>; <prediction_dur=0.0668799s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=57.5000%> with <n_comps=197>.		Took <total_dur=1.0709090s> in total.<training_dur=0.9993900s>; <prediction_dur=0.0715190s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=198>.		Took <total_dur=1.0923901s> in total.<training_dur=1.0380190s>; <prediction_dur=0.0543711s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=199>.		Took <total_dur=1.0404374s> in total.<training_dur=0.9800230s>; <prediction_dur=0.0604144s>).

=====> Testing: ['patches_mean', 'patches_std']
===> Feature subset testing at step: 3/512
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=1>.		Took <total_dur=0.8983115s> in total.<training_dur=0.8273912s>; <prediction_dur=0.0709203s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=75.0000%> with <n_comps=2>.		Took <total_dur=1.0903114s> in total.<training_dur=1.0473763s>; <prediction_dur=0.0429351s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=85.0000%> with <n_comps=3>.		Took <total_dur=0.8184711s> in total.<training_dur=0.7543758s>; <prediction_dur=0.0640953s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=72.5000%> with <n_comps=4>.		Took <total_dur=0.7849672s> in total.<training_dur=0.7092589s>; <prediction_dur=0.0757083s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=5>.		Took <total_dur=0.9005085s> in total.<training_dur=0.8380034s>; <prediction_dur=0.0625051s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=72.5000%> with <n_comps=6>.		Took <total_dur=0.7464951s> in total.<training_dur=0.6762387s>; <prediction_dur=0.0702565s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=7>.		Took <total_dur=0.9086971s> in total.<training_dur=0.8448310s>; <prediction_dur=0.0638661s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=72.5000%> with <n_comps=8>.		Took <total_dur=0.7625657s> in total.<training_dur=0.6985746s>; <prediction_dur=0.0639911s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=9>.		Took <total_dur=0.7659155s> in total.<training_dur=0.7024604s>; <prediction_dur=0.0634551s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=10>.		Took <total_dur=0.9347944s> in total.<training_dur=0.8727134s>; <prediction_dur=0.0620809s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=11>.		Took <total_dur=0.8799104s> in total.<training_dur=0.8161972s>; <prediction_dur=0.0637132s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=12>.		Took <total_dur=0.8398709s> in total.<training_dur=0.7716307s>; <prediction_dur=0.0682401s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=13>.		Took <total_dur=0.8872023s> in total.<training_dur=0.7901371s>; <prediction_dur=0.0970653s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=14>.		Took <total_dur=0.9854279s> in total.<training_dur=0.9153844s>; <prediction_dur=0.0700436s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=15>.		Took <total_dur=0.8479830s> in total.<training_dur=0.7838222s>; <prediction_dur=0.0641609s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=16>.		Took <total_dur=0.7960530s> in total.<training_dur=0.7341241s>; <prediction_dur=0.0619288s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=72.5000%> with <n_comps=17>.		Took <total_dur=0.8673427s> in total.<training_dur=0.7865803s>; <prediction_dur=0.0807623s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=18>.		Took <total_dur=0.8906248s> in total.<training_dur=0.8255154s>; <prediction_dur=0.0651095s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=75.0000%> with <n_comps=19>.		Took <total_dur=1.1012360s> in total.<training_dur=1.0899172s>; <prediction_dur=0.0113188s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=20>.		Took <total_dur=1.0030939s> in total.<training_dur=0.9143682s>; <prediction_dur=0.0887257s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=55.0000%> with <n_comps=21>.		Took <total_dur=0.8807208s> in total.<training_dur=0.8201828s>; <prediction_dur=0.0605380s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=22>.		Took <total_dur=0.7962239s> in total.<training_dur=0.7351414s>; <prediction_dur=0.0610825s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=23>.		Took <total_dur=0.7856851s> in total.<training_dur=0.7247001s>; <prediction_dur=0.0609849s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=75.0000%> with <n_comps=24>.		Took <total_dur=0.8122903s> in total.<training_dur=0.7507913s>; <prediction_dur=0.0614990s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=25>.		Took <total_dur=0.8870142s> in total.<training_dur=0.8253552s>; <prediction_dur=0.0616591s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=26>.		Took <total_dur=1.0964658s> in total.<training_dur=1.0600704s>; <prediction_dur=0.0363954s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=27>.		Took <total_dur=1.1001723s> in total.<training_dur=1.0892612s>; <prediction_dur=0.0109111s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=28>.		Took <total_dur=0.8261279s> in total.<training_dur=0.7514187s>; <prediction_dur=0.0747093s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=75.0000%> with <n_comps=29>.		Took <total_dur=0.9098867s> in total.<training_dur=0.8197414s>; <prediction_dur=0.0901453s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=30>.		Took <total_dur=1.1127524s> in total.<training_dur=1.0939040s>; <prediction_dur=0.0188485s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=31>.		Took <total_dur=1.1054985s> in total.<training_dur=1.0905346s>; <prediction_dur=0.0149639s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=72.5000%> with <n_comps=32>.		Took <total_dur=1.0308347s> in total.<training_dur=0.9588933s>; <prediction_dur=0.0719414s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=72.5000%> with <n_comps=33>.		Took <total_dur=0.8280866s> in total.<training_dur=0.7624880s>; <prediction_dur=0.0655986s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=34>.		Took <total_dur=0.8701075s> in total.<training_dur=0.7991018s>; <prediction_dur=0.0710057s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=35>.		Took <total_dur=1.1037662s> in total.<training_dur=1.0935767s>; <prediction_dur=0.0101896s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=75.0000%> with <n_comps=36>.		Took <total_dur=1.1085773s> in total.<training_dur=1.0980535s>; <prediction_dur=0.0105238s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=37>.		Took <total_dur=0.8260628s> in total.<training_dur=0.7668136s>; <prediction_dur=0.0592492s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=38>.		Took <total_dur=0.8347647s> in total.<training_dur=0.7671572s>; <prediction_dur=0.0676075s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=72.5000%> with <n_comps=39>.		Took <total_dur=0.8402841s> in total.<training_dur=0.7714304s>; <prediction_dur=0.0688537s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=72.5000%> with <n_comps=40>.		Took <total_dur=0.8593122s> in total.<training_dur=0.7882122s>; <prediction_dur=0.0711000s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=41>.		Took <total_dur=0.8274361s> in total.<training_dur=0.7662127s>; <prediction_dur=0.0612235s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=42>.		Took <total_dur=0.8336319s> in total.<training_dur=0.7708350s>; <prediction_dur=0.0627969s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=43>.		Took <total_dur=0.8276084s> in total.<training_dur=0.7622908s>; <prediction_dur=0.0653176s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=72.5000%> with <n_comps=44>.		Took <total_dur=0.8303239s> in total.<training_dur=0.7655079s>; <prediction_dur=0.0648160s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=80.0000%> with <n_comps=45>.		Took <total_dur=0.8274163s> in total.<training_dur=0.7664413s>; <prediction_dur=0.0609749s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=75.0000%> with <n_comps=46>.		Took <total_dur=0.8355435s> in total.<training_dur=0.7679616s>; <prediction_dur=0.0675819s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=47>.		Took <total_dur=0.8282491s> in total.<training_dur=0.7619312s>; <prediction_dur=0.0663179s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=48>.		Took <total_dur=0.8298795s> in total.<training_dur=0.7681826s>; <prediction_dur=0.0616969s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=49>.		Took <total_dur=0.8583190s> in total.<training_dur=0.7936026s>; <prediction_dur=0.0647165s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=50>.		Took <total_dur=0.8404382s> in total.<training_dur=0.7800825s>; <prediction_dur=0.0603558s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=72.5000%> with <n_comps=51>.		Took <total_dur=0.9908315s> in total.<training_dur=0.9288049s>; <prediction_dur=0.0620266s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=52>.		Took <total_dur=0.8693253s> in total.<training_dur=0.7997672s>; <prediction_dur=0.0695580s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=53>.		Took <total_dur=0.8542518s> in total.<training_dur=0.7933067s>; <prediction_dur=0.0609451s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=57.5000%> with <n_comps=54>.		Took <total_dur=0.8908717s> in total.<training_dur=0.8267928s>; <prediction_dur=0.0640789s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=55>.		Took <total_dur=0.8769071s> in total.<training_dur=0.8102299s>; <prediction_dur=0.0666772s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=56>.		Took <total_dur=0.8345455s> in total.<training_dur=0.7732195s>; <prediction_dur=0.0613260s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=57.5000%> with <n_comps=57>.		Took <total_dur=0.8647267s> in total.<training_dur=0.8010450s>; <prediction_dur=0.0636817s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=72.5000%> with <n_comps=58>.		Took <total_dur=0.8630875s> in total.<training_dur=0.7998428s>; <prediction_dur=0.0632447s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=59>.		Took <total_dur=0.8518358s> in total.<training_dur=0.7902501s>; <prediction_dur=0.0615856s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=72.5000%> with <n_comps=60>.		Took <total_dur=0.8884811s> in total.<training_dur=0.8277368s>; <prediction_dur=0.0607443s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=61>.		Took <total_dur=0.8767564s> in total.<training_dur=0.8026161s>; <prediction_dur=0.0741403s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=62>.		Took <total_dur=0.8850932s> in total.<training_dur=0.7936508s>; <prediction_dur=0.0914423s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=63>.		Took <total_dur=1.0945832s> in total.<training_dur=1.0707059s>; <prediction_dur=0.0238774s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=64>.		Took <total_dur=0.8688161s> in total.<training_dur=0.8080520s>; <prediction_dur=0.0607640s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=65>.		Took <total_dur=0.8701554s> in total.<training_dur=0.8084986s>; <prediction_dur=0.0616568s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=66>.		Took <total_dur=0.9099400s> in total.<training_dur=0.8255150s>; <prediction_dur=0.0844250s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=67>.		Took <total_dur=0.8928180s> in total.<training_dur=0.8318089s>; <prediction_dur=0.0610091s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=68>.		Took <total_dur=0.9043979s> in total.<training_dur=0.8438589s>; <prediction_dur=0.0605390s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=69>.		Took <total_dur=0.8936561s> in total.<training_dur=0.8184475s>; <prediction_dur=0.0752086s>).
Testing model.
================= About to train.
================= About to predict.
