=============== Features for image at index 1 ===============
len(patches_mean)=1000
len(patches_std)=1000
len(patches_var)=1000
len(lbp)=128
len(hog)=15876
len(glcm)=6
len(corner_kitchen_rosenfeld)=128
len(daisy)=25
len(draw_multiblock_lbp)=128

patches_mean=[101.0, 75.22222222222223, 112.88888888888889, 109.66666666666667, 166.0, 186.66666666666666, 110.22222222222223, 111.88888888888889, 152.44444444444446, 212.33333333333334, 126.77777777777777, 106.44444444444444, 20.11111111111111, 90.11111111111111, 248.44444444444446, 27.0, 113.44444444444444, 23.333333333333332, 17.88888888888889, 103.77777777777777, 105.22222222222223, 18.555555555555557, 200.11111111111111, 122.22222222222223, 221.77777777777777, 105.55555555555556, 188.44444444444446, 194.22222222222223, 248.33333333333334, 111.55555555555556, 120.0, 84.22222222222223, 18.333333333333332, 172.33333333333334, 236.88888888888889, 20.88888888888889, 145.55555555555554, 113.55555555555556, 21.555555555555557, 125.77777777777777, 140.0, 123.33333333333333, 20.0, 204.88888888888889, 150.0, 22.0, 57.111111111111114, 248.55555555555554, 104.66666666666667, 20.666666666666668, 122.22222222222223, 108.88888888888889, 113.88888888888889, 25.444444444444443, 249.0, 128.33333333333334, 66.22222222222223, 107.55555555555556, 134.33333333333334, 249.33333333333334, 17.88888888888889, 79.22222222222223, 18.88888888888889, 249.88888888888889, 116.22222222222223, 226.88888888888889, 102.0, 248.88888888888889, 243.0, 18.666666666666668, 119.22222222222223, 170.33333333333334, 85.77777777777777, 95.33333333333333, 19.11111111111111, 77.55555555555556, 19.11111111111111, 90.11111111111111, 250.44444444444446, 96.11111111111111, 168.11111111111111, 17.88888888888889, 18.333333333333332, 248.11111111111111, 103.11111111111111, 248.88888888888889, 24.0, 94.11111111111111, 20.555555555555557, 20.0, 138.88888888888889, 116.0, 109.22222222222223, 248.33333333333334, 37.111111111111114, 92.88888888888889, 118.77777777777777, 20.22222222222222, 134.0, 241.22222222222223, 18.0, 138.11111111111111, 104.77777777777777, 242.11111111111111, 249.0, 48.888888888888886, 25.0, 99.22222222222223, 18.666666666666668, 248.22222222222223, 22.333333333333332, 17.77777777777778, 18.77777777777778, 77.66666666666667, 237.11111111111111, 105.66666666666667, 30.77777777777778, 92.33333333333333, 66.88888888888889, 137.55555555555554, 21.0, 249.55555555555554, 250.0, 249.0, 96.22222222222223, 37.888888888888886, 102.0, 111.55555555555556, 117.0, 243.44444444444446, 214.33333333333334, 81.66666666666667, 116.0, 17.77777777777778, 209.22222222222223, 21.88888888888889, 104.66666666666667, 18.666666666666668, 108.55555555555556, 116.22222222222223, 117.11111111111111, 25.77777777777778, 19.88888888888889, 165.77777777777777, 18.77777777777778, 248.22222222222223, 191.77777777777777, 162.88888888888889, 249.77777777777777, 191.55555555555554, 79.55555555555556, 248.66666666666666, 249.44444444444446, 111.88888888888889, 77.66666666666667, 226.88888888888889, 18.0, 99.11111111111111, 249.11111111111111, 125.22222222222223, 248.77777777777777, 83.0, 131.77777777777777, 150.22222222222223, 226.55555555555554, 81.88888888888889, 75.44444444444444, 209.22222222222223, 121.88888888888889, 248.33333333333334, 235.88888888888889, 143.55555555555554, 26.0, 36.0, 18.333333333333332, 203.44444444444446, 119.88888888888889, 248.22222222222223, 52.55555555555556, 85.44444444444444, 238.88888888888889, 22.0, 165.22222222222223, 30.555555555555557, 116.44444444444444, 23.11111111111111, 120.33333333333333, 87.77777777777777, 76.88888888888889, 248.0, 184.88888888888889, 162.66666666666666, 102.77777777777777, 250.55555555555554, 126.77777777777777, 79.66666666666667, 112.66666666666667, 20.555555555555557, 190.0, 84.0, 20.22222222222222, 98.88888888888889, 243.77777777777777, 22.22222222222222, 17.88888888888889, 19.0, 35.666666666666664, 196.44444444444446, 97.66666666666667, 178.33333333333334, 249.88888888888889, 97.11111111111111, 94.44444444444444, 230.22222222222223, 34.333333333333336, 113.55555555555556, 161.66666666666666, 150.88888888888889, 23.11111111111111, 18.11111111111111, 250.0, 78.33333333333333, 251.66666666666666, 102.0, 250.66666666666666, 128.22222222222223, 107.66666666666667, 37.55555555555556, 52.55555555555556, 19.22222222222222, 134.44444444444446, 113.44444444444444, 81.0, 21.444444444444443, 95.77777777777777, 248.88888888888889, 20.11111111111111, 228.11111111111111, 19.555555555555557, 104.44444444444444, 36.0, 82.77777777777777, 83.0, 102.0, 79.88888888888889, 41.55555555555556, 112.66666666666667, 115.22222222222223, 124.11111111111111, 248.77777777777777, 74.66666666666667, 24.0, 142.33333333333334, 119.22222222222223, 80.33333333333333, 20.0, 136.55555555555554, 36.666666666666664, 21.11111111111111, 27.333333333333332, 36.55555555555556, 103.0, 19.0, 119.0, 106.22222222222223, 111.33333333333333, 139.0, 109.44444444444444, 118.55555555555556, 76.11111111111111, 18.0, 249.11111111111111, 250.44444444444446, 94.55555555555556, 87.0, 247.22222222222223, 123.33333333333333, 250.88888888888889, 140.55555555555554, 125.11111111111111, 84.11111111111111, 93.11111111111111, 66.55555555555556, 18.0, 123.77777777777777, 18.555555555555557, 76.33333333333333, 98.33333333333333, 24.555555555555557, 249.44444444444446, 112.88888888888889, 120.55555555555556, 148.11111111111111, 20.11111111111111, 23.333333333333332, 131.88888888888889, 120.55555555555556, 125.11111111111111, 22.11111111111111, 250.33333333333334, 49.111111111111114, 247.44444444444446, 56.0, 124.66666666666667, 198.55555555555554, 124.55555555555556, 24.666666666666668, 245.0, 100.33333333333333, 64.88888888888889, 215.11111111111111, 250.66666666666666, 76.33333333333333, 112.0, 88.66666666666667, 19.555555555555557, 121.22222222222223, 116.88888888888889, 20.11111111111111, 106.0, 38.77777777777778, 158.77777777777777, 18.77777777777778, 23.333333333333332, 119.11111111111111, 211.66666666666666, 250.77777777777777, 115.0, 106.0, 125.11111111111111, 86.11111111111111, 88.44444444444444, 138.44444444444446, 97.77777777777777, 249.22222222222223, 117.33333333333333, 184.33333333333334, 23.0, 247.11111111111111, 99.55555555555556, 118.0, 237.66666666666666, 248.33333333333334, 20.555555555555557, 119.88888888888889, 81.66666666666667, 91.88888888888889, 142.0, 99.11111111111111, 56.333333333333336, 132.33333333333334, 103.33333333333333, 116.44444444444444, 177.55555555555554, 90.33333333333333, 136.55555555555554, 119.33333333333333, 141.11111111111111, 183.0, 23.22222222222222, 232.44444444444446, 82.66666666666667, 73.77777777777777, 61.77777777777778, 102.55555555555556, 69.0, 249.0, 47.77777777777778, 100.11111111111111, 20.0, 43.77777777777778, 135.22222222222223, 127.0, 119.0, 35.888888888888886, 114.33333333333333, 116.11111111111111, 47.22222222222222, 127.77777777777777, 226.44444444444446, 105.55555555555556, 88.22222222222223, 118.88888888888889, 116.22222222222223, 182.22222222222223, 78.22222222222223, 101.22222222222223, 128.22222222222223, 19.77777777777778, 40.666666666666664, 233.33333333333334, 42.333333333333336, 174.66666666666666, 84.88888888888889, 126.33333333333333, 249.55555555555554, 150.77777777777777, 19.77777777777778, 122.0, 85.22222222222223, 136.33333333333334, 124.0, 105.66666666666667, 249.66666666666666, 84.55555555555556, 18.77777777777778, 78.22222222222223, 21.333333333333332, 98.55555555555556, 32.0, 102.11111111111111, 99.77777777777777, 120.22222222222223, 96.11111111111111, 249.88888888888889, 111.55555555555556, 140.77777777777777, 107.77777777777777, 87.0, 152.88888888888889, 86.77777777777777, 163.0, 65.0, 18.333333333333332, 137.77777777777777, 108.44444444444444, 248.33333333333334, 114.44444444444444, 248.33333333333334, 76.88888888888889, 104.11111111111111, 129.11111111111111, 183.66666666666666, 244.55555555555554, 130.44444444444446, 18.0, 23.88888888888889, 250.11111111111111, 18.0, 121.0, 128.0, 47.22222222222222, 95.44444444444444, 116.0, 84.0, 94.0, 247.33333333333334, 176.33333333333334, 225.33333333333334, 78.44444444444444, 62.44444444444444, 249.55555555555554, 240.66666666666666, 80.88888888888889, 22.555555555555557, 101.33333333333333, 106.11111111111111, 108.44444444444444, 129.55555555555554, 117.33333333333333, 149.11111111111111, 92.11111111111111, 248.66666666666666, 218.88888888888889, 249.22222222222223, 122.88888888888889, 128.66666666666666, 120.33333333333333, 212.11111111111111, 250.11111111111111, 113.33333333333333, 121.88888888888889, 196.33333333333334, 134.66666666666666, 221.33333333333334, 47.77777777777778, 133.33333333333334, 51.333333333333336, 23.22222222222222, 92.55555555555556, 111.11111111111111, 197.55555555555554, 219.55555555555554, 159.77777777777777, 19.666666666666668, 140.66666666666666, 20.77777777777778, 168.11111111111111, 108.88888888888889, 199.33333333333334, 113.0, 249.33333333333334, 104.88888888888889, 249.88888888888889, 23.0, 140.44444444444446, 20.666666666666668, 121.44444444444444, 19.88888888888889, 102.0, 116.0, 109.11111111111111, 158.44444444444446, 118.77777777777777, 247.55555555555554, 242.66666666666666, 95.33333333333333, 145.66666666666666, 237.55555555555554, 23.22222222222222, 98.44444444444444, 122.66666666666667, 20.0, 144.44444444444446, 152.88888888888889, 165.44444444444446, 60.22222222222222, 21.11111111111111, 111.22222222222223, 134.0, 110.11111111111111, 20.0, 132.33333333333334, 184.11111111111111, 19.77777777777778, 244.88888888888889, 110.55555555555556, 183.66666666666666, 115.0, 117.33333333333333, 123.33333333333333, 135.88888888888889, 114.11111111111111, 54.888888888888886, 244.55555555555554, 153.22222222222223, 224.88888888888889, 112.66666666666667, 117.33333333333333, 25.444444444444443, 230.44444444444446, 71.33333333333333, 248.88888888888889, 120.33333333333333, 75.22222222222223, 35.77777777777778, 81.55555555555556, 19.88888888888889, 249.55555555555554, 235.11111111111111, 213.44444444444446, 79.0, 125.11111111111111, 19.88888888888889, 162.55555555555554, 226.66666666666666, 250.0, 193.55555555555554, 248.66666666666666, 115.66666666666667, 20.11111111111111, 91.55555555555556, 174.88888888888889, 53.111111111111114, 76.22222222222223, 20.0, 240.88888888888889, 24.0, 21.0, 135.55555555555554, 142.44444444444446, 98.88888888888889, 125.0, 25.555555555555557, 98.11111111111111, 18.333333333333332, 119.55555555555556, 18.77777777777778, 249.0, 107.22222222222223, 82.55555555555556, 158.88888888888889, 142.77777777777777, 120.44444444444444, 26.11111111111111, 111.44444444444444, 136.33333333333334, 124.55555555555556, 104.66666666666667, 103.66666666666667, 108.33333333333333, 99.66666666666667, 139.55555555555554, 27.11111111111111, 131.33333333333334, 75.33333333333333, 20.333333333333332, 88.22222222222223, 122.33333333333333, 198.66666666666666, 19.88888888888889, 174.44444444444446, 25.666666666666668, 87.55555555555556, 59.55555555555556, 23.555555555555557, 219.66666666666666, 85.0, 243.0, 150.33333333333334, 251.44444444444446, 116.22222222222223, 250.77777777777777, 18.0, 18.444444444444443, 79.0, 131.77777777777777, 248.22222222222223, 86.11111111111111, 103.0, 90.11111111111111, 123.55555555555556, 33.22222222222222, 104.44444444444444, 249.11111111111111, 146.11111111111111, 80.33333333333333, 172.33333333333334, 131.44444444444446, 251.44444444444446, 107.33333333333333, 120.11111111111111, 152.55555555555554, 174.77777777777777, 18.0, 79.88888888888889, 73.0, 37.888888888888886, 243.77777777777777, 98.88888888888889, 195.0, 134.11111111111111, 249.55555555555554, 27.0, 37.111111111111114, 145.88888888888889, 82.11111111111111, 148.0, 90.88888888888889, 97.11111111111111, 118.66666666666667, 67.11111111111111, 79.33333333333333, 220.0, 145.11111111111111, 154.88888888888889, 120.55555555555556, 18.666666666666668, 139.88888888888889, 166.66666666666666, 20.0, 132.66666666666666, 141.0, 248.77777777777777, 87.22222222222223, 19.11111111111111, 165.0, 120.44444444444444, 19.666666666666668, 250.0, 251.11111111111111, 248.0, 118.22222222222223, 24.555555555555557, 250.11111111111111, 114.55555555555556, 98.66666666666667, 18.333333333333332, 103.11111111111111, 154.88888888888889, 97.66666666666667, 128.88888888888889, 131.55555555555554, 125.11111111111111, 74.77777777777777, 250.88888888888889, 149.66666666666666, 90.66666666666667, 96.11111111111111, 86.22222222222223, 120.88888888888889, 85.44444444444444, 34.77777777777778, 19.88888888888889, 38.55555555555556, 84.66666666666667, 249.33333333333334, 74.0, 18.22222222222222, 242.0, 155.33333333333334, 20.11111111111111, 139.22222222222223, 22.0, 18.0, 76.55555555555556, 126.66666666666667, 131.55555555555554, 163.77777777777777, 122.44444444444444, 110.88888888888889, 20.11111111111111, 35.888888888888886, 102.55555555555556, 77.33333333333333, 215.11111111111111, 18.11111111111111, 206.44444444444446, 115.77777777777777, 97.55555555555556, 173.55555555555554, 131.88888888888889, 87.33333333333333, 112.88888888888889, 158.0, 58.55555555555556, 20.0, 121.77777777777777, 18.88888888888889, 196.0, 170.88888888888889, 138.33333333333334, 68.44444444444444, 124.33333333333333, 249.11111111111111, 19.88888888888889, 65.22222222222223, 25.555555555555557, 98.55555555555556, 105.66666666666667, 19.555555555555557, 137.0, 119.88888888888889, 149.11111111111111, 234.77777777777777, 32.44444444444444, 108.33333333333333, 189.66666666666666, 26.0, 20.88888888888889, 20.0, 103.55555555555556, 72.33333333333333, 194.55555555555554, 65.0, 112.66666666666667, 115.0, 104.11111111111111, 55.888888888888886, 19.666666666666668, 54.77777777777778, 249.66666666666666, 106.66666666666667, 112.22222222222223, 20.0, 20.11111111111111, 25.0, 117.55555555555556, 127.0, 91.66666666666667, 249.33333333333334, 20.22222222222222, 181.55555555555554, 250.88888888888889, 123.77777777777777, 72.22222222222223, 144.0, 104.77777777777777, 249.11111111111111, 169.77777777777777, 23.11111111111111, 18.0, 46.44444444444444, 18.333333333333332, 136.77777777777777, 112.0, 169.22222222222223, 33.0, 20.88888888888889, 181.55555555555554, 108.44444444444444, 77.88888888888889, 204.88888888888889, 55.77777777777778, 240.22222222222223, 96.77777777777777, 110.11111111111111, 140.11111111111111, 161.44444444444446, 118.55555555555556, 82.88888888888889, 133.22222222222223, 19.22222222222222, 234.66666666666666, 35.888888888888886, 212.77777777777777, 61.44444444444444, 114.55555555555556, 105.0, 100.55555555555556, 212.44444444444446, 20.11111111111111, 17.77777777777778, 106.11111111111111, 126.0, 105.44444444444444, 209.11111111111111, 121.88888888888889, 83.11111111111111, 111.11111111111111, 94.33333333333333, 109.0, 177.33333333333334, 23.333333333333332, 114.66666666666667, 114.44444444444444, 82.22222222222223, 118.33333333333333, 154.55555555555554, 185.33333333333334, 141.77777777777777, 123.88888888888889, 110.0, 92.11111111111111, 249.77777777777777, 77.44444444444444, 124.33333333333333, 70.55555555555556, 250.33333333333334, 42.666666666666664, 250.33333333333334, 23.555555555555557, 21.11111111111111, 19.11111111111111, 83.33333333333333, 99.77777777777777, 189.88888888888889, 120.44444444444444, 247.66666666666666, 58.111111111111114, 250.66666666666666, 18.22222222222222, 117.44444444444444, 75.55555555555556, 110.44444444444444, 121.33333333333333, 98.33333333333333, 124.33333333333333, 19.555555555555557, 100.66666666666667, 18.0, 249.44444444444446, 18.555555555555557, 117.44444444444444, 100.11111111111111, 112.0, 127.66666666666667, 236.44444444444446, 63.44444444444444, 50.333333333333336, 90.11111111111111, 119.88888888888889, 249.44444444444446, 18.0, 247.33333333333334, 55.888888888888886, 249.33333333333334, 147.66666666666666, 104.33333333333333, 44.22222222222222, 132.11111111111111, 208.11111111111111, 237.66666666666666, 250.44444444444446, 250.11111111111111, 114.33333333333333, 208.22222222222223, 20.0, 104.55555555555556, 249.77777777777777, 137.44444444444446, 17.77777777777778, 102.66666666666667, 76.77777777777777, 89.88888888888889, 97.0, 104.0, 218.33333333333334, 172.88888888888889, 137.77777777777777, 165.11111111111111, 116.88888888888889, 96.44444444444444, 118.44444444444444, 248.55555555555554, 114.55555555555556, 176.88888888888889, 131.0, 84.22222222222223, 149.66666666666666, 119.77777777777777, 87.55555555555556, 115.22222222222223, 242.66666666666666, 74.88888888888889, 236.44444444444446, 18.0, 18.444444444444443, 70.44444444444444, 18.11111111111111, 142.55555555555554, 33.888888888888886, 17.88888888888889, 25.333333333333332, 130.66666666666666, 250.88888888888889, 19.666666666666668, 225.66666666666666, 37.44444444444444, 31.88888888888889, 41.111111111111114, 175.66666666666666, 18.11111111111111, 154.44444444444446, 122.33333333333333, 194.44444444444446, 21.22222222222222, 124.66666666666667, 189.77777777777777, 127.66666666666667, 23.555555555555557, 143.11111111111111, 89.0, 19.11111111111111, 42.77777777777778, 159.77777777777777, 250.55555555555554, 115.44444444444444, 146.55555555555554, 82.88888888888889, 116.22222222222223, 95.55555555555556, 18.444444444444443, 27.333333333333332, 18.333333333333332, 241.44444444444446, 249.22222222222223, 125.66666666666667, 58.111111111111114, 82.22222222222223, 49.111111111111114, 107.44444444444444, 21.333333333333332, 20.0, 104.77777777777777, 113.44444444444444, 139.55555555555554, 104.77777777777777, 132.66666666666666, 19.11111111111111, 72.66666666666667, 209.11111111111111, 18.333333333333332, 100.55555555555556, 90.22222222222223, 18.11111111111111, 108.77777777777777, 120.66666666666667, 18.555555555555557, 18.666666666666668, 18.444444444444443, 85.77777777777777, 79.22222222222223, 111.33333333333333, 124.44444444444444, 202.88888888888889, 138.33333333333334, 79.22222222222223, 248.66666666666666, 86.44444444444444, 108.77777777777777, 112.88888888888889, 227.66666666666666, 58.666666666666664, 26.444444444444443, 18.77777777777778, 239.0, 150.11111111111111, 246.66666666666666, 19.11111111111111]
patches_std=[11.803954139750516, 17.874735601946846, 100.51030289305828, 16.6266185511199, 12.073846850849888, 37.18123780140253, 16.150526498316996, 16.27844139716661, 110.47484491061616, 45.467937421146935, 11.49664470644252, 14.330318373691425, 0.3142696805273545, 20.38215147538238, 1.0657403385139377, 4.163331998932265, 18.46986200460458, 7.086763875156433, 0.3142696805273545, 8.953928719069236, 16.01927850905331, 0.6849348892187752, 32.9324036455972, 9.01576123337813, 19.251422907167598, 7.274172134814626, 93.72549885598086, 17.554852306589563, 9.140872800534726, 12.737618049707994, 15.846485765339617, 14.053688940612929, 0.4714045207910317, 107.03685761871417, 7.880276989554286, 1.0999438818457405, 11.226071946946744, 9.334655990936987, 1.4229164972072996, 13.121464838430688, 9.41629792788369, 18.2208671582886, 0.4714045207910317, 34.9648853834321, 14.734690736866897, 3.6209268304000717, 21.966866182424457, 1.4229164972072998, 11.575836902790225, 1.0540925533894598, 12.025692659910167, 10.51395310416321, 102.58919636041868, 3.269764215458258, 1.0540925533894598, 12.256517540566824, 13.878662902117672, 13.208676534856515, 8.640987597877148, 1.632993161855452, 0.5665577237325317, 13.389695688001424, 0.8748897637790901, 1.5234788000891208, 11.331154474650633, 38.8971419813974, 12.274635093014645, 0.9938079899999065, 2.494438257849294, 1.2472191289246473, 20.356695746814978, 12.987173159185437, 12.847635128499787, 8.666666666666666, 1.1967032904743342, 11.823809417264913, 1.0999438818457405, 11.278735591510685, 1.8324913891634047, 21.05782749396129, 98.23830939149333, 0.5665577237325317, 0.9428090415820634, 2.514157444218836, 15.198278005122411, 1.5234788000891208, 2.0548046676563256, 19.376070261160596, 1.5713484026367723, 0.0, 49.00440897271828, 7.586537784494028, 18.035630031060293, 1.5634719199411433, 45.58860589724499, 16.237435788561392, 14.029950326812976, 0.7856742013183862, 8.831760866327848, 13.82250803563021, 0.0, 13.859969892900155, 14.234369812205152, 2.9228769862146455, 1.5634719199411433, 42.24385410774617, 7.333333333333333, 12.760858363481647, 0.816496580927726, 1.314684396244359, 0.6666666666666666, 0.41573970964154905, 1.227262335243029, 20.477630071210225, 4.6772367066331535, 15.456030825826172, 23.184179365237544, 17.390929947660776, 16.83544174547945, 105.27119592442192, 1.4142135623730951, 1.8324913891634047, 1.4142135623730951, 0.9428090415820634, 17.27411536746224, 7.09372875533883, 12.840906856172149, 8.152860590152953, 20.46134567096374, 1.9499920860871385, 13.366625103842281, 22.271057451320086, 11.832159566199232, 0.628539361054709, 49.734356061373774, 6.419491922513017, 17.46106780494506, 0.9428090415820634, 102.34702526330169, 11.123326618495403, 14.464089205449815, 1.7497795275581802, 0.5665577237325317, 25.01012140793215, 0.628539361054709, 1.3966450099973928, 32.53981416930403, 71.4880634083519, 1.1331154474650633, 22.136501335080588, 11.324615383325252, 1.3333333333333333, 1.7069212773041353, 9.109755996786504, 20.477630071210225, 36.259694318882026, 0.0, 19.750918090866023, 1.0999438818457405, 14.527963669938803, 1.8121673811444545, 90.86497919685254, 12.190868875950446, 16.136761190784075, 30.45255366453457, 32.060513770210804, 13.77688169129003, 49.734356061373774, 16.016966313148792, 6.582805886043833, 7.69479541919146, 32.68404677484486, 3.197221015541813, 4.0, 1.0540925533894598, 44.72412004254804, 16.257951520850373, 1.0304020550550783, 73.03745022796504, 10.264405715449776, 8.238722183074056, 3.6209268304000717, 23.63508586721481, 10.985961861607356, 17.00399372115668, 3.7251232476089355, 18.05547008526779, 11.409331247105277, 15.814511229630169, 3.4318767136623336, 38.979893519164875, 12.970050972229146, 11.428791884998924, 1.7708197167232476, 27.295818881196105, 9.404490653110589, 19.510680835549195, 0.6849348892187752, 24.26245384677046, 7.788880963698615, 0.7856742013183861, 15.701694490612892, 17.636942149790638, 2.9731307022799225, 0.7370277311900889, 1.0540925533894598, 6.377042156569663, 77.46699815062628, 14.165686240583852, 94.3468777084506, 1.4487116456005886, 14.805737960102153, 21.177089880398324, 20.087463076244997, 5.456901847914967, 9.978990275252315, 11.61416759345623, 11.406084579385634, 7.2333247994487975, 0.3142696805273545, 2.160246899469287, 14.468356276140472, 1.1547005383792517, 16.431676725154983, 1.632993161855452, 11.544866851431589, 11.15546702045434, 4.833014037984788, 13.833221775543036, 0.9162456945817022, 16.700705980494167, 15.63550881175503, 16.713268182295565, 3.1661792997277796, 17.47873487525645, 1.4487116456005886, 0.3142696805273545, 13.763433393068464, 0.9558139185602919, 14.974876078961287, 4.0, 13.644516829341113, 17.993826101687706, 9.451631252505216, 18.008914116468016, 24.60402456289129, 21.007935008784973, 14.148245103410275, 12.801427389548268, 1.1331154474650633, 15.048071120394283, 5.597618541248888, 22.125902367034787, 9.210836344916759, 13.316656236958787, 0.0, 10.884352796674184, 18.29389697868299, 0.9938079899999066, 14.071247279470288, 44.4474998949725, 17.256238807393046, 0.816496580927726, 9.030811456096044, 15.689896096092314, 18.245242911205345, 32.37626016423488, 11.8894080883515, 12.93669295806923, 10.826351026232702, 0.4714045207910317, 1.1967032904743342, 1.257078722109418, 9.878271453730164, 16.44519517805868, 0.9162456945817024, 14.877275736280932, 0.9938079899999065, 6.499762578759851, 17.741629916632952, 22.873781422038853, 15.637877409563718, 30.688800386065964, 0.6666666666666666, 94.56423775259015, 0.9558139185602919, 16.36391694484477, 8.339997335464536, 5.833068777069639, 0.8314794192830981, 13.59284488614379, 9.844469525927416, 19.85192095068129, 0.3142696805273545, 5.773502691896258, 9.859506911768452, 5.964918013986466, 14.555979637588003, 4.6772367066331535, 1.5634719199411433, 67.95822972719581, 4.54877544232416, 61.949800466004554, 102.76402310363508, 91.033706957748, 17.391639824998364, 6.531972647421808, 13.880441875771343, 12.87547194388531, 10.49279588251381, 62.931081252862015, 1.1547005383792515, 13.5400640077266, 16.69331203406522, 19.94436706886879, 0.9558139185602919, 18.286472096290062, 11.82694141398682, 0.3142696805273545, 17.003267659809133, 34.82584537628397, 6.6629619335844215, 1.0304020550550783, 0.9428090415820634, 17.697736480782222, 74.70683443374583, 2.3934065809486684, 22.528993664954402, 13.199326582148887, 12.27865758537005, 12.377797725896551, 14.476886644357627, 19.866840665407555, 14.358719981466761, 0.6285393610547089, 13.165611772087667, 61.17007256639294, 8.137703743822469, 6.9032109186706965, 9.900741959975761, 9.189365834726814, 28.039654459750786, 1.49071198499986, 1.7069212773041351, 13.486161173954452, 8.205689083394114, 18.734664510499986, 8.679477710861024, 13.084719200013803, 8.628119403696523, 11.962905629950907, 14.023789311975086, 9.417608933518707, 57.96763677760416, 17.473789896108208, 11.738929641828612, 16.46545204697129, 12.260545977007753, 93.73840669058168, 7.3903507325848405, 14.863161852274688, 15.563490039905004, 13.488907193837113, 15.251796089333784, 20.667264029598496, 93.09254654494217, 1.4142135623730951, 31.104761256751452, 11.77987402728762, 0.4714045207910317, 58.495541668247924, 19.78651492404628, 19.44222209522358, 14.491376746189438, 6.402160129283527, 19.40217628114033, 12.653014084877118, 45.856728036056374, 14.381915942350641, 16.971290208997804, 15.188527184183835, 15.490339482527508, 8.723460633951344, 16.0539214854729, 96.30250692223008, 72.85873359953764, 11.409331247105277, 11.544866851431589, 1.0304020550550783, 6.4635731432217725, 7.055336829505575, 5.676462121975467, 57.1858568373529, 17.175204636946475, 37.79182745280019, 0.8314794192830981, 10.336319759606875, 0.6285393610547089, 23.22833518691246, 14.226561837928202, 9.030811456096044, 14.337208778404378, 14.483707321600289, 1.0540925533894598, 17.166576770710325, 1.0304020550550783, 12.335835582001442, 1.0540925533894598, 14.840718095168, 8.894442709417556, 8.812168915065934, 12.44345234140588, 13.838575535058013, 12.913769089571232, 1.286204100310025, 17.224731000067646, 10.840026422454244, 16.129874131752327, 13.097921802925667, 12.83609878813518, 16.92321073934834, 11.633285577743433, 19.78776277287444, 0.4714045207910317, 8.80375901757347, 15.326890756018116, 2.1081851067789197, 12.919503869998785, 1.1547005383792515, 8.710714276675395, 11.77987402728762, 21.408432629557936, 17.676098111417136, 10.468058411834559, 14.492228653938106, 0.0, 0.3142696805273545, 1.4487116456005886, 0.4714045207910317, 17.88854381999832, 22.40535650240808, 20.29656663783189, 19.1027113323453, 11.718930554164631, 7.788880963698615, 13.9522996909709, 2.6246692913372702, 52.13017893270227, 48.18021724041241, 11.738929641828614, 32.97848157380173, 1.5713484026367723, 3.366501646120693, 15.8706188666314, 3.4354721852756236, 12.202003478482084, 14.586481141754698, 13.881331277103481, 37.532537324629295, 13.333333333333334, 10.19198426263878, 19.13370757151104, 1.8856180831641267, 18.11349677539096, 1.617802197617893, 9.242948596926855, 12.274635093014645, 15.57776192739723, 37.42680098670958, 1.4487116456005886, 16.87865186822955, 19.151120909721328, 17.851237118661178, 13.097921802925667, 52.16214037871614, 70.84847332751431, 15.238839267549945, 59.288372478177465, 2.3465235646603193, 14.399931412731036, 15.76525324492262, 55.73371433504362, 12.401712346922196, 98.94081033940081, 0.6666666666666666, 8.94427190999916, 0.9162456945817024, 27.71459480909109, 15.198278005122411, 38.88730155490635, 103.10620199041806, 1.699673171197595, 19.547346761954646, 1.5234788000891208, 2.211083193570267, 8.833158628600795, 1.2472191289246473, 11.096657265552352, 0.3142696805273545, 12.274635093014645, 11.981467170407619, 10.115383712658703, 20.056094175592786, 14.029950326812976, 1.4989708403591158, 6.6332495807108005, 20.612833111653742, 26.29321839055336, 12.39274977208772, 7.3903507325848405, 31.556338018467894, 11.440668201153676, 0.0, 30.808047689997103, 8.184598604756843, 11.14660995909729, 12.725981977197305, 1.0999438818457405, 14.077387524188318, 103.82249809704595, 9.266959760885795, 0.6666666666666666, 13.114877048604002, 85.34562990106686, 0.6285393610547089, 2.469567863432541, 12.203015211287477, 55.0555275255002, 17.745108872274887, 26.679163738351654, 12.657891697365018, 16.380506330828755, 14.586481141754698, 10.826351026232702, 4.621474298463427, 26.91871623890369, 31.08927805331837, 6.879922480183431, 13.466006584482772, 3.890475866669244, 19.044457409563847, 23.098821518760552, 1.1967032904743342, 9.285592184789413, 17.874735601946846, 5.883645464388323, 14.150862644384862, 0.8748897637790901, 1.4989708403591158, 15.110294095560185, 48.696591509995415, 8.576453553512405, 19.541661731026778, 0.3142696805273545, 14.322562706064016, 14.321700705960557, 1.5634719199411433, 5.335647646019098, 1.5634719199411433, 16.0, 0.3142696805273545, 10.100727268767566, 12.059523156635676, 22.19832047923368, 15.746447767161667, 0.0, 3.1426968052735442, 8.537498983243799, 1.632993161855452, 21.281766379859395, 10.58417173698654, 15.701694490612892, 12.780193008453876, 5.079613089183316, 16.168862012072218, 0.4714045207910317, 20.597255001097764, 1.227262335243029, 1.632993161855452, 18.56387039252907, 15.195840886470007, 23.26391791042966, 10.086049527031305, 15.420845279549447, 2.1829869671542776, 12.553097104443198, 11.85092588975412, 17.391639824998364, 20.752509888364507, 10.749676997731399, 23.90722810272148, 10.0, 12.111620784382714, 10.68170236582628, 16.138291249213605, 10.03327796219494, 1.0540925533894598, 25.715441870932025, 13.131810402394805, 93.92313642311757, 0.7370277311900888, 12.419618093172065, 10.83205120618128, 19.90036912617514, 5.794846582402354, 2.6712922844825124, 38.47076812334269, 15.67730135507313, 10.893423092245461, 17.88233144133554, 1.257078722109418, 14.573779939617417, 1.227262335243029, 0.0, 0.9558139185602919, 17.21110752456745, 107.02624577154249, 2.199887763691481, 17.271971157176036, 13.308309851784752, 33.92293371892775, 13.817148050408724, 39.59735619058361, 12.535382023262626, 1.5234788000891208, 15.351036479262707, 13.316656236958787, 36.49048582241069, 13.817148050408722, 0.8314794192830981, 8.692269873603532, 14.356140338966418, 13.833221775543036, 28.440103835479214, 0.0, 17.37459476390389, 10.913803695829934, 5.526591162420394, 7.098947931094794, 15.701694490612892, 35.07452383457575, 18.561875171343633, 1.0657403385139377, 18.110770276274835, 45.58860589724499, 17.85400325133737, 11.019623349526611, 109.62055768270231, 21.047271897350203, 16.888157878916513, 9.067647005823629, 23.755441222051022, 19.113694915775266, 44.90483765079709, 27.569888745370353, 32.16777008278263, 18.84504716017277, 1.1547005383792515, 9.949253958010503, 29.60855732160327, 0.0, 18.879736344675063, 7.874007874011811, 0.7856742013183862, 9.565885888902615, 0.5665577237325317, 25.18376902336547, 10.965715370200291, 0.6666666666666666, 1.1547005383792515, 0.9938079899999067, 3.4318767136623336, 14.672557739442839, 8.139220698583305, 0.8748897637790901, 17.689363477720548, 10.583005244258363, 0.4714045207910317, 14.129908739537361, 14.850697285869886, 13.703203194062977, 107.82954083433356, 15.217760903417435, 17.741629916632952, 11.886292551392295, 0.8748897637790902, 67.14660577174894, 17.795130420052185, 15.793419476650143, 13.488907193837113, 13.008069670139758, 88.73069256164484, 4.516089207311461, 0.3142696805273545, 49.36960628454878, 7.542472332656507, 0.816496580927726, 13.784048752090222, 0.41573970964154905, 5.436502143433364, 23.87001838662421, 0.3142696805273545, 13.926615428163105, 1.247219128924647, 0.0, 11.75784476765393, 11.372481406154654, 28.488247522681704, 18.56387039252907, 8.473633762194497, 18.03357636585738, 0.3142696805273545, 6.402160129283527, 15.363095109971322, 7.195677714974301, 21.020859716240828, 0.3142696805273545, 30.481728994089817, 95.96385636475361, 17.372462955372864, 27.48377748220499, 9.242948596926853, 27.084230754362505, 17.93472388936206, 18.29996964174774, 31.742404554689617, 0.816496580927726, 10.633048875795772, 0.7370277311900888, 30.904871963998023, 106.43667334193964, 12.165525060596439, 20.597255001097764, 12.418624006798105, 1.0999438818457405, 0.8748897637790901, 17.138506208585635, 11.767290928776648, 27.17887705599132, 22.320892057044276, 1.0657403385139377, 9.591663046625438, 21.656122790587137, 10.19198426263878, 7.020252888413918, 18.886274328852988, 12.806248474865697, 42.554540167752826, 5.637178175095921, 1.0999438818457405, 0.4714045207910317, 15.70641136684925, 10.954451150103322, 44.76633435247993, 9.877021593352703, 12.622730819174325, 19.136933459209764, 10.979217179587224, 71.61229634395424, 0.4714045207910317, 20.7459649505246, 1.4142135623730951, 14.102797200870786, 17.20321531818836, 0.4714045207910317, 0.5665577237325317, 3.0912061651652345, 99.82218759275149, 19.966638842495918, 10.154364141151877, 1.699673171197595, 0.7856742013183862, 63.68692716215805, 2.23330569358242, 18.23779822101937, 27.867853953087547, 10.760008261045982, 14.234369812205152, 1.1967032904743342, 9.885767297571212, 2.5579698740491863, 0.6666666666666666, 72.35577395544435, 0.4714045207910317, 18.158424905859217, 21.270741511391755, 17.267681937833252, 9.079892314584157, 0.7370277311900888, 63.68692716215805, 10.792086721411625, 12.278657585370048, 23.992797273091238, 18.121673811444545, 15.127442155660008, 12.505801123014674, 12.71433525543724, 6.756798131338121, 14.51521126352253, 12.508762360939995, 13.690584310592156, 15.907914017716518, 1.0304020550550783, 5.374838498865699, 6.118177732416091, 67.65726991860893, 8.486736201745916, 11.917411269148374, 11.718930554164631, 17.8145453125068, 64.48619337348475, 0.7370277311900889, 0.41573970964154905, 18.935237254297693, 14.907119849998598, 11.917411269148376, 62.780825884703304, 16.09309337327624, 12.635439089626223, 14.255170168861888, 9.556847457887633, 11.737877907772672, 11.962905629950907, 7.3181661333667165, 13.936363306911247, 12.919503869998785, 12.699761833092932, 7.557189365836422, 7.440297352348092, 32.38655413730965, 19.57196281406292, 20.272830446708415, 13.416407864998739, 7.578396846657749, 1.987615979999813, 11.66296237488678, 11.244751862288666, 19.276416858783175, 1.2472191289246473, 11.8227652339788, 1.2472191289246473, 6.977919319158925, 1.1967032904743342, 0.8748897637790901, 17.657230184198703, 11.801862167356639, 40.50361971326196, 17.34045437768836, 1.4142135623730951, 17.444798297897314, 1.9436506316151, 0.7856742013183862, 15.727619803751553, 19.038622213870127, 16.007714189735115, 53.245761437987824, 11.135528725660043, 13.9522996909709, 0.6849348892187752, 20.270394394014364, 0.0, 1.257078722109418, 0.9558139185602919, 17.04967325272515, 105.53929699347322, 11.785113019775793, 14.996295838935989, 6.202349216423495, 60.481606223363684, 16.619934483090546, 22.922305703515782, 20.90956423609086, 1.2570787221094177, 0.0, 1.49071198499986, 43.033435177892514, 1.2472191289246473, 69.88085097745542, 10.077477638553981, 13.231089463648175, 14.601708000888452, 50.781546176724135, 28.039654459750786, 1.4989708403591158, 1.9116278371205837, 17.84501175243223, 39.625406490106485, 0.4714045207910317, 101.91184764961348, 1.227262335243029, 16.323882375295074, 0.41573970964154905, 17.29482902809713, 11.716823401301394, 22.382753501769788, 23.72059583287626, 16.048537489614297, 11.105554165971789, 12.431540929287213, 65.03294511720412, 97.90408508608417, 17.722831755453157, 14.720440143714839, 12.962433851635453, 0.8314794192830981, 11.615230529219538, 93.3481469725558, 16.69331203406522, 11.02298384465414, 12.727922061357855, 16.06775776224464, 19.276416858783175, 14.823238345979883, 17.320508075688775, 48.59456861384235, 33.069324863286525, 0.0, 0.6849348892187752, 27.96073084051636, 0.5665577237325317, 11.33442260560586, 6.08174763706845, 0.3142696805273545, 6.48074069840786, 21.134489978863144, 1.4487116456005886, 0.6666666666666666, 34.95711658589707, 25.75142330957914, 30.123613640555014, 40.73067215650863, 13.466006584482772, 0.5665577237325317, 15.283332323599467, 13.299958228840001, 31.58801147802442, 1.4740554623801778, 15.21694961401777, 38.322542764678566, 21.761331658599286, 4.4748956812449485, 13.403518977777974, 12.970050972229146, 0.7370277311900888, 12.52355804764922, 98.94081033940081, 1.1653431646335017, 17.075720977855674, 15.471199846532848, 13.690584310592156, 18.943059577925617, 22.618139775710564, 0.6849348892187752, 10.666666666666666, 0.816496580927726, 18.714884813437855, 1.314684396244359, 9.486832980505138, 19.980237149323578, 13.222689067388655, 10.450352938691108, 13.784944372685175, 1.3333333333333333, 0.4714045207910317, 14.234369812205152, 27.74130939435953, 12.111620784382714, 14.543251797294378, 11.69995251652283, 1.1967032904743342, 72.74613391789285, 46.323606759511684, 0.816496580927726, 18.774161387462, 11.103330609203885, 0.5665577237325317, 12.44345234140588, 18.88562063228706, 0.8314794192830981, 0.9428090415820634, 0.9558139185602919, 8.740426861909839, 12.461298111243625, 19.697715603592208, 11.076613111628092, 88.45267968360187, 20.564262420249577, 18.53391952177159, 1.3333333333333333, 17.845703567034537, 11.49664470644252, 17.93472388936206, 33.6286914537109, 20.242968600918637, 10.457438725202172, 0.9162456945817022, 5.962847939999439, 32.51134654733617, 7.378647873726218, 1.3698697784375504]
patches_var=[139.33333333333334, 319.5061728395061, 10102.32098765432, 276.44444444444446, 145.77777777777777, 1382.4444444444443, 260.8395061728395, 264.98765432098764, 12204.691358024691, 2067.3333333333335, 132.17283950617283, 205.35802469135803, 0.09876543209876544, 415.4320987654321, 1.1358024691358024, 17.333333333333332, 341.13580246913585, 50.22222222222222, 0.09876543209876544, 80.17283950617285, 256.61728395061726, 0.46913580246913583, 1084.5432098765434, 81.28395061728394, 370.6172839506173, 52.91358024691358, 8784.469135802468, 308.17283950617286, 83.55555555555556, 162.24691358024688, 251.11111111111111, 197.50617283950615, 0.2222222222222222, 11456.888888888889, 62.098765432098766, 1.2098765432098766, 126.02469135802468, 87.1358024691358, 2.024691358024691, 172.17283950617286, 88.66666666666667, 332.0, 0.2222222222222222, 1222.5432098765434, 217.11111111111111, 13.11111111111111, 482.5432098765432, 2.0246913580246915, 134.0, 1.1111111111111112, 144.61728395061726, 110.54320987654322, 10524.543209876541, 10.691358024691358, 1.1111111111111112, 150.22222222222223, 192.6172839506173, 174.46913580246914, 74.66666666666669, 2.666666666666667, 0.3209876543209877, 179.28395061728392, 0.7654320987654321, 2.3209876543209877, 128.39506172839506, 1512.9876543209875, 150.66666666666666, 0.9876543209876542, 6.222222222222222, 1.5555555555555558, 414.39506172839504, 168.66666666666666, 165.06172839506175, 75.11111111111111, 1.432098765432099, 139.80246913580245, 1.2098765432098766, 127.20987654320987, 3.358024691358025, 443.4320987654321, 9650.765432098766, 0.32098765432098764, 0.8888888888888888, 6.320987654320988, 230.98765432098767, 2.3209876543209877, 4.222222222222222, 375.4320987654321, 2.4691358024691357, 0.0, 2401.432098765432, 57.55555555555556, 325.2839506172839, 2.4444444444444446, 2078.3209876543206, 263.6543209876543, 196.83950617283952, 0.617283950617284, 78.0, 191.06172839506175, 0.0, 192.09876543209876, 202.61728395061732, 8.543209876543209, 2.4444444444444446, 1784.543209876543, 53.77777777777778, 162.83950617283952, 0.6666666666666666, 1.728395061728395, 0.4444444444444444, 0.17283950617283952, 1.506172839506173, 419.33333333333326, 21.876543209876544, 238.88888888888889, 537.5061728395062, 302.4444444444444, 283.4320987654321, 11082.024691358025, 2.0, 3.358024691358025, 2.0, 0.8888888888888888, 298.39506172839515, 50.32098765432099, 164.88888888888889, 66.46913580246914, 418.6666666666667, 3.80246913580247, 178.66666666666666, 496.0, 140.0, 0.39506172839506176, 2473.5061728395062, 41.20987654320987, 304.8888888888889, 0.8888888888888888, 10474.913580246915, 123.72839506172838, 209.20987654320987, 3.0617283950617282, 0.32098765432098764, 625.5061728395061, 0.39506172839506176, 1.9506172839506175, 1058.8395061728395, 5110.543209876543, 1.2839506172839505, 490.0246913580247, 128.24691358024694, 1.7777777777777777, 2.9135802469135808, 82.98765432098766, 419.33333333333326, 1314.7654320987654, 0.0, 390.09876543209873, 1.2098765432098764, 211.06172839506172, 3.2839506172839505, 8256.444444444445, 148.6172839506173, 260.3950617283951, 927.3580246913579, 1027.8765432098764, 189.80246913580243, 2473.5061728395062, 256.5432098765432, 43.33333333333334, 59.20987654320988, 1068.2469135802467, 10.222222222222221, 16.0, 1.1111111111111112, 2000.2469135802473, 264.32098765432096, 1.0617283950617287, 5334.469135802469, 105.35802469135803, 67.87654320987654, 13.11111111111111, 558.6172839506172, 120.69135802469135, 289.1358024691358, 13.876543209876543, 326.00000000000006, 130.17283950617286, 250.0987654320987, 11.777777777777779, 1519.432098765432, 168.22222222222223, 130.61728395061726, 3.1358024691358026, 745.0617283950618, 88.44444444444444, 380.6666666666667, 0.46913580246913583, 588.6666666666666, 60.666666666666664, 0.6172839506172838, 246.54320987654324, 311.06172839506166, 8.839506172839505, 0.54320987654321, 1.1111111111111112, 40.666666666666664, 6001.135802469135, 200.66666666666666, 8901.333333333334, 2.0987654320987654, 219.20987654320987, 448.46913580246905, 403.5061728395061, 29.77777777777778, 99.58024691358025, 134.88888888888889, 130.09876543209876, 52.32098765432099, 0.09876543209876544, 4.666666666666667, 209.33333333333337, 1.3333333333333335, 270.0, 2.666666666666667, 133.28395061728395, 124.44444444444444, 23.358024691358025, 191.35802469135803, 0.839506172839506, 278.9135802469136, 244.46913580246914, 279.3333333333333, 10.024691358024691, 305.5061728395061, 2.0987654320987654, 0.09876543209876544, 189.43209876543207, 0.9135802469135802, 224.24691358024697, 16.0, 186.17283950617283, 323.77777777777777, 89.33333333333333, 324.32098765432096, 605.358024691358, 441.33333333333326, 200.17283950617283, 163.87654320987656, 1.2839506172839505, 226.44444444444446, 31.333333333333332, 489.55555555555554, 84.8395061728395, 177.33333333333334, 0.0, 118.46913580246913, 334.6666666666667, 0.9876543209876544, 198.0, 1975.5802469135806, 297.77777777777777, 0.6666666666666666, 81.55555555555556, 246.1728395061728, 332.8888888888889, 1048.2222222222222, 141.35802469135805, 167.35802469135803, 117.20987654320987, 0.2222222222222222, 1.432098765432099, 1.580246913580247, 97.58024691358024, 270.44444444444446, 0.8395061728395062, 221.33333333333334, 0.9876543209876542, 42.24691358024691, 314.7654320987654, 523.2098765432097, 244.54320987654324, 941.8024691358025, 0.4444444444444444, 8942.395061728395, 0.9135802469135803, 267.77777777777777, 69.55555555555556, 34.02469135802469, 0.691358024691358, 184.76543209876542, 96.91358024691357, 394.09876543209873, 0.09876543209876544, 33.333333333333336, 97.20987654320987, 35.58024691358025, 211.87654320987656, 21.876543209876544, 2.4444444444444446, 4618.3209876543215, 20.69135802469136, 3837.777777777778, 10560.444444444445, 8287.135802469136, 302.46913580246917, 42.666666666666664, 192.66666666666666, 165.77777777777777, 110.09876543209876, 3960.3209876543206, 1.3333333333333333, 183.33333333333334, 278.6666666666667, 397.7777777777778, 0.9135802469135803, 334.3950617283951, 139.87654320987656, 0.09876543209876544, 289.1111111111111, 1212.8395061728397, 44.395061728395056, 1.0617283950617284, 0.8888888888888888, 313.20987654320993, 5581.111111111111, 5.728395061728396, 507.55555555555554, 174.22222222222223, 150.76543209876547, 153.20987654320984, 209.58024691358025, 394.69135802469134, 206.17283950617283, 0.3950617283950617, 173.33333333333334, 3741.777777777778, 66.22222222222223, 47.65432098765432, 98.02469135802468, 84.44444444444444, 786.2222222222222, 2.2222222222222228, 2.91358024691358, 181.87654320987653, 67.33333333333333, 350.9876543209877, 75.33333333333333, 171.20987654320984, 74.44444444444444, 143.11111111111111, 196.66666666666666, 88.69135802469135, 3360.246913580247, 305.3333333333333, 137.80246913580243, 271.1111111111111, 150.320987654321, 8786.888888888889, 54.617283950617285, 220.91358024691354, 242.22222222222223, 181.9506172839506, 232.6172839506173, 427.1358024691358, 8666.222222222223, 2.0, 967.5061728395062, 138.76543209876544, 0.2222222222222222, 3421.728395061729, 391.5061728395062, 378.0, 210.0, 40.987654320987666, 376.44444444444446, 160.09876543209873, 2102.839506172839, 206.83950617283952, 288.0246913580247, 230.69135802469134, 239.95061728395058, 76.09876543209877, 257.7283950617284, 9274.172839506173, 5308.395061728395, 130.17283950617286, 133.28395061728395, 1.0617283950617287, 41.77777777777778, 49.77777777777778, 32.22222222222222, 3270.2222222222217, 294.98765432098764, 1428.2222222222222, 0.691358024691358, 106.8395061728395, 0.3950617283950617, 539.5555555555555, 202.39506172839506, 81.55555555555556, 205.55555555555554, 209.7777777777778, 1.1111111111111112, 294.6913580246913, 1.0617283950617284, 152.17283950617286, 1.1111111111111112, 220.2469135802469, 79.11111111111111, 77.65432098765433, 154.8395061728395, 191.50617283950615, 166.76543209876542, 1.6543209876543212, 296.69135802469134, 117.50617283950616, 260.17283950617286, 171.55555555555554, 164.76543209876544, 286.39506172839504, 135.33333333333334, 391.55555555555554, 0.2222222222222222, 77.50617283950618, 234.91358024691357, 4.444444444444445, 166.91358024691357, 1.3333333333333333, 75.87654320987654, 138.76543209876544, 458.32098765432096, 312.44444444444446, 109.58024691358025, 210.02469135802468, 0.0, 0.09876543209876544, 2.0987654320987654, 0.2222222222222222, 320.0, 502.0, 411.9506172839506, 364.91358024691357, 137.33333333333334, 60.666666666666664, 194.66666666666666, 6.888888888888888, 2717.5555555555557, 2321.3333333333335, 137.80246913580245, 1087.5802469135804, 2.469135802469136, 11.333333333333334, 251.87654320987653, 11.80246913580247, 148.88888888888889, 212.76543209876544, 192.69135802469134, 1408.6913580246915, 177.77777777777777, 103.87654320987654, 366.0987654320988, 3.5555555555555554, 328.09876543209873, 2.617283950617284, 85.4320987654321, 150.66666666666666, 242.66666666666666, 1400.7654320987651, 2.0987654320987654, 284.8888888888889, 366.7654320987655, 318.6666666666667, 171.55555555555554, 2720.8888888888887, 5019.506172839508, 232.22222222222217, 3515.1111111111113, 5.5061728395061715, 207.35802469135803, 248.5432098765432, 3106.246913580247, 153.80246913580245, 9789.283950617284, 0.4444444444444444, 80.0, 0.8395061728395061, 768.0987654320988, 230.98765432098764, 1512.2222222222222, 10630.888888888889, 2.8888888888888893, 382.09876543209873, 2.3209876543209877, 4.888888888888889, 78.0246913580247, 1.5555555555555558, 123.1358024691358, 0.09876543209876544, 150.66666666666666, 143.55555555555554, 102.32098765432099, 402.24691358024694, 196.83950617283952, 2.246913580246914, 44.00000000000001, 424.8888888888889, 691.3333333333334, 153.58024691358023, 54.617283950617285, 995.8024691358023, 130.88888888888889, 0.0, 949.1358024691358, 66.98765432098766, 124.24691358024688, 161.95061728395063, 1.2098765432098766, 198.17283950617286, 10779.111111111111, 85.87654320987653, 0.4444444444444444, 172.00000000000003, 7283.876543209877, 0.3950617283950617, 6.098765432098765, 148.91358024691357, 3031.111111111111, 314.8888888888889, 711.7777777777778, 160.22222222222226, 268.32098765432096, 212.76543209876544, 117.20987654320987, 21.358024691358025, 724.6172839506172, 966.5432098765432, 47.333333333333336, 181.33333333333334, 15.135802469135804, 362.69135802469134, 533.5555555555555, 1.432098765432099, 86.22222222222223, 319.5061728395061, 34.617283950617285, 200.2469135802469, 0.7654320987654321, 2.246913580246914, 228.32098765432102, 2371.3580246913575, 73.55555555555556, 381.8765432098765, 0.09876543209876544, 205.1358024691358, 205.11111111111111, 2.4444444444444446, 28.469135802469136, 2.4444444444444446, 256.0, 0.09876543209876544, 102.02469135802468, 145.4320987654321, 492.7654320987654, 247.95061728395063, 0.0, 9.87654320987654, 72.88888888888889, 2.6666666666666665, 452.9135802469136, 112.02469135802468, 246.54320987654324, 163.33333333333334, 25.80246913580247, 261.4320987654321, 0.2222222222222222, 424.2469135802469, 1.506172839506173, 2.6666666666666665, 344.6172839506173, 230.91358024691357, 541.2098765432099, 101.72839506172839, 237.80246913580245, 4.765432098765432, 157.58024691358023, 140.44444444444446, 302.46913580246917, 430.6666666666667, 115.55555555555556, 571.5555555555557, 100.0, 146.69135802469134, 114.09876543209877, 260.44444444444446, 100.66666666666667, 1.1111111111111112, 661.283950617284, 172.44444444444443, 8821.555555555555, 0.5432098765432098, 154.2469135802469, 117.33333333333333, 396.02469135802465, 33.58024691358024, 7.135802469135801, 1480.0, 245.77777777777777, 118.66666666666667, 319.7777777777777, 1.580246913580247, 212.39506172839504, 1.5061728395061729, 0.0, 0.9135802469135803, 296.22222222222223, 11454.617283950618, 4.8395061728395055, 298.32098765432096, 177.11111111111111, 1150.7654320987654, 190.9135802469136, 1567.9506172839504, 157.1358024691358, 2.3209876543209877, 235.65432098765436, 177.33333333333334, 1331.5555555555557, 190.91358024691354, 0.691358024691358, 75.55555555555556, 206.0987654320988, 191.35802469135803, 808.8395061728395, 0.0, 301.8765432098765, 119.11111111111111, 30.543209876543205, 50.395061728395056, 246.54320987654324, 1230.2222222222222, 344.5432098765432, 1.1358024691358024, 328.0, 2078.3209876543206, 318.76543209876536, 121.4320987654321, 12016.666666666666, 442.9876543209877, 285.2098765432099, 82.22222222222223, 564.320987654321, 365.3333333333333, 2016.4444444444443, 760.0987654320988, 1034.7654320987654, 355.13580246913574, 1.3333333333333333, 98.98765432098766, 876.6666666666666, 0.0, 356.4444444444445, 62.0, 0.617283950617284, 91.50617283950619, 0.32098765432098764, 634.2222222222222, 120.24691358024691, 0.4444444444444444, 1.3333333333333333, 0.9876543209876546, 11.777777777777779, 215.28395061728395, 66.24691358024691, 0.7654320987654321, 312.91358024691357, 112.0, 0.2222222222222222, 199.6543209876543, 220.54320987654322, 187.77777777777777, 11627.20987654321, 231.58024691358025, 314.7654320987654, 141.28395061728395, 0.7654320987654323, 4508.666666666667, 316.6666666666667, 249.43209876543207, 181.95061728395063, 169.20987654320984, 7873.135802469135, 20.39506172839506, 0.09876543209876544, 2437.3580246913584, 56.888888888888886, 0.6666666666666666, 190.0, 0.1728395061728395, 29.555555555555557, 569.7777777777778, 0.09876543209876544, 193.95061728395063, 1.5555555555555556, 0.0, 138.2469135802469, 129.33333333333331, 811.5802469135803, 344.6172839506173, 71.80246913580247, 325.2098765432099, 0.09876543209876544, 40.987654320987666, 236.02469135802474, 51.77777777777778, 441.8765432098765, 0.09876543209876544, 929.1358024691358, 9209.061728395061, 301.8024691358024, 755.3580246913581, 85.43209876543209, 733.5555555555557, 321.6543209876543, 334.8888888888889, 1007.5802469135801, 0.6666666666666666, 113.06172839506172, 0.5432098765432098, 955.1111111111111, 11328.765432098764, 148.0, 424.2469135802469, 154.22222222222223, 1.2098765432098766, 0.7654320987654321, 293.7283950617284, 138.46913580246917, 738.6913580246915, 498.22222222222223, 1.1358024691358026, 92.0, 468.98765432098764, 103.87654320987654, 49.283950617283956, 356.69135802469134, 164.0, 1810.888888888889, 31.77777777777778, 1.2098765432098766, 0.2222222222222222, 246.69135802469134, 120.0, 2004.0246913580247, 97.55555555555556, 159.33333333333334, 366.22222222222223, 120.54320987654322, 5128.3209876543215, 0.2222222222222222, 430.3950617283951, 2.0000000000000004, 198.88888888888889, 295.95061728395063, 0.2222222222222222, 0.32098765432098764, 9.555555555555555, 9964.46913580247, 398.6666666666667, 103.11111111111111, 2.888888888888889, 0.6172839506172839, 4056.0246913580245, 4.987654320987654, 332.6172839506173, 776.6172839506172, 115.77777777777777, 202.61728395061732, 1.432098765432099, 97.7283950617284, 6.5432098765432105, 0.4444444444444444, 5235.358024691358, 0.2222222222222222, 329.72839506172835, 452.44444444444446, 298.1728395061728, 82.44444444444444, 0.5432098765432098, 4056.0246913580245, 116.46913580246913, 150.76543209876544, 575.6543209876543, 328.3950617283951, 228.83950617283952, 156.39506172839506, 161.65432098765433, 45.654320987654316, 210.69135802469134, 156.46913580246914, 187.43209876543207, 253.0617283950617, 1.0617283950617287, 28.88888888888889, 37.4320987654321, 4577.506172839505, 72.0246913580247, 142.02469135802468, 137.33333333333334, 317.358024691358, 4158.469135802468, 0.54320987654321, 0.1728395061728395, 358.5432098765432, 222.22222222222223, 142.0246913580247, 3941.4320987654323, 258.98765432098764, 159.65432098765436, 203.20987654320984, 91.33333333333331, 137.77777777777777, 143.11111111111111, 53.55555555555556, 194.2222222222222, 166.91358024691357, 161.28395061728395, 57.1111111111111, 55.358024691358025, 1048.888888888889, 383.0617283950617, 410.9876543209877, 180.0, 57.432098765432116, 3.9506172839506166, 136.02469135802468, 126.44444444444444, 371.58024691358025, 1.5555555555555558, 139.77777777777777, 1.5555555555555558, 48.69135802469136, 1.432098765432099, 0.7654320987654322, 311.77777777777777, 139.28395061728395, 1640.543209876543, 300.6913580246914, 2.0000000000000004, 304.320987654321, 3.7777777777777777, 0.6172839506172839, 247.35802469135803, 362.4691358024691, 256.24691358024694, 2835.1111111111113, 124.0, 194.66666666666669, 0.46913580246913583, 410.8888888888889, 0.0, 1.580246913580247, 0.9135802469135803, 290.69135802469134, 11138.543209876545, 138.88888888888889, 224.88888888888886, 38.46913580246914, 3658.0246913580245, 276.22222222222223, 525.4320987654321, 437.20987654320993, 1.5802469135802468, 0.0, 2.2222222222222228, 1851.8765432098767, 1.5555555555555558, 4883.333333333333, 101.55555555555554, 175.06172839506175, 213.20987654320984, 2578.765432098766, 786.2222222222222, 2.246913580246914, 3.654320987654321, 318.4444444444445, 1570.172839506173, 0.2222222222222222, 10386.024691358027, 1.506172839506173, 266.4691358024691, 0.17283950617283952, 299.1111111111111, 137.28395061728395, 500.98765432098764, 562.6666666666666, 257.55555555555554, 123.33333333333336, 154.5432098765432, 4229.283950617284, 9585.20987654321, 314.0987654320988, 216.69135802469134, 168.0246913580247, 0.691358024691358, 134.9135802469136, 8713.876543209877, 278.6666666666667, 121.50617283950618, 162.0, 258.17283950617286, 371.58024691358025, 219.7283950617284, 300.0, 2361.432098765432, 1093.5802469135804, 0.0, 0.46913580246913583, 781.8024691358025, 0.32098765432098764, 128.46913580246914, 36.987654320987666, 0.09876543209876544, 42.0, 446.6666666666667, 2.0987654320987654, 0.4444444444444444, 1222.0, 663.1358024691358, 907.4320987654322, 1658.9876543209875, 181.33333333333334, 0.3209876543209877, 233.58024691358025, 176.88888888888889, 997.8024691358025, 2.17283950617284, 231.55555555555554, 1468.6172839506173, 473.55555555555554, 20.024691358024693, 179.65432098765433, 168.22222222222223, 0.5432098765432098, 156.83950617283952, 9789.283950617284, 1.3580246913580247, 291.5802469135803, 239.35802469135803, 187.43209876543207, 358.8395061728395, 511.5802469135803, 0.46913580246913583, 113.77777777777777, 0.6666666666666666, 350.2469135802469, 1.7283950617283952, 90.00000000000001, 399.20987654320993, 174.8395061728395, 109.20987654320987, 190.02469135802468, 1.7777777777777777, 0.2222222222222222, 202.61728395061732, 769.5802469135803, 146.69135802469134, 211.50617283950615, 136.88888888888889, 1.432098765432099, 5292.0, 2145.8765432098767, 0.6666666666666666, 352.4691358024691, 123.28395061728394, 0.32098765432098764, 154.8395061728395, 356.6666666666667, 0.691358024691358, 0.8888888888888888, 0.9135802469135803, 76.39506172839506, 155.28395061728395, 388.0, 122.69135802469135, 7823.876543209875, 422.8888888888889, 343.5061728395061, 1.7777777777777777, 318.46913580246917, 132.17283950617283, 321.6543209876543, 1130.8888888888887, 409.77777777777777, 109.35802469135803, 0.839506172839506, 35.55555555555556, 1056.9876543209875, 54.44444444444444, 1.8765432098765433]
lbp=[[ 14  62  62 ...  62  62  56]
 [143 255 251 ... 247 239 248]
 [139 247   0 ... 255 223 248]
 ...
 [139 131 195 ... 247 239 248]
 [ 13 255 223 ... 255 223  24]
 [  2   0  34 ...  34  34  32]]
hog=[0.05084476 0.01674268 0.0158835  ... 0.00091106 0.00144051 0.        ]
glcm={'contrast': array([[5486.30341717]]), 'dissimilarity': array([[42.71208079]]), 'homogeneity': array([[0.15418713]]), 'energy': array([[0.04338519]]), 'correlation': array([[0.55263245]]), 'asm': array([[0.00188227]])}
corner_kitchen_rosenfeld=[[-160.         -202.           -1.         ...    8.
  -200.         -160.        ]
 [-204.         -326.         -322.         ...   11.
  -312.         -195.        ]
 [  -2.09996796 -164.            0.         ...    0.
   -34.8          -6.85134291]
 ...
 [ 592.49601376 -324.8912826  -174.92914083 ...   57.6
   184.4         158.65530161]
 [ -66.30465444  236.30769231   24.28880866 ...   29.
  -332.84027106 -175.50318471]
 [-258.1902667  -229.51460498  300.61725372 ...   17.
  -253.73023604 -271.34275093]]
daisy=[[[0.00037678 0.00041451 0.00026545 ... 0.0202656  0.0185761  0.00702031]
  [0.00025657 0.00022459 0.00021378 ... 0.01962686 0.01887593 0.00728052]
  [0.00045004 0.00030495 0.00039026 ... 0.01857486 0.01871702 0.00742092]
  ...
  [0.01485571 0.00666153 0.00205882 ... 0.00296519 0.00446992 0.00266081]
  [0.0098042  0.00395668 0.00085882 ... 0.00268569 0.00379976 0.00219755]
  [0.00220311 0.00095804 0.00026026 ... 0.00253077 0.00341155 0.00192832]]

 [[0.00023075 0.00029422 0.00027141 ... 0.01652126 0.01313074 0.00472049]
  [0.00036663 0.0003609  0.0004919  ... 0.015882   0.01361508 0.00513232]
  [0.00118163 0.00133047 0.00117331 ... 0.01501118 0.01380896 0.00547364]
  ...
  [0.00776516 0.00491296 0.00434588 ... 0.00277917 0.00434411 0.002902  ]
  [0.01767922 0.00788412 0.00196927 ... 0.00242111 0.00349491 0.00222112]
  [0.01239627 0.00545626 0.00104218 ... 0.00226005 0.00305048 0.00184096]]

 [[0.00028031 0.00027002 0.00044126 ... 0.01449298 0.00995481 0.00352721]
  [0.00098991 0.00106437 0.00103793 ... 0.01346513 0.01012278 0.00387764]
  [0.0038972  0.00574114 0.00298115 ... 0.01213516 0.00995149 0.00411955]
  ...
  [0.00223702 0.00372558 0.00635667 ... 0.00267098 0.00447564 0.00350335]
  [0.01000595 0.00536369 0.00299947 ... 0.00210467 0.0032368  0.00240157]
  [0.02214518 0.0097642  0.00189567 ... 0.00184843 0.00259191 0.00178248]]

 ...

 [[0.00326903 0.00507284 0.00923519 ... 0.00544859 0.00583491 0.00489362]
  [0.00566662 0.00291272 0.00216714 ... 0.00594252 0.00606808 0.00469823]
  [0.00748636 0.00437913 0.00172292 ... 0.00659977 0.00634567 0.00457395]
  ...
  [0.00424931 0.00444202 0.00396335 ... 0.0066363  0.00393783 0.00358756]
  [0.00308481 0.00463367 0.00348682 ... 0.00527567 0.00331265 0.00389869]
  [0.01191891 0.01719915 0.00748988 ... 0.00446457 0.00313067 0.00435838]]

 [[0.00202928 0.008728   0.01898046 ... 0.00559763 0.00621935 0.00535887]
  [0.00244674 0.00544176 0.01124616 ... 0.00582535 0.00618531 0.00492331]
  [0.00463638 0.00295381 0.003345   ... 0.00632128 0.00626592 0.00465832]
  ...
  [0.00304391 0.00460853 0.00307417 ... 0.00628243 0.00382381 0.00350386]
  [0.00789328 0.0166707  0.00982526 ... 0.00545065 0.00347354 0.00384519]
  [0.01687189 0.03024934 0.01459979 ... 0.00532621 0.00368276 0.00443506]]

 [[0.00171672 0.0060513  0.01153807 ... 0.00584741 0.00660444 0.00586163]
  [0.00192736 0.00901196 0.02051492 ... 0.00600136 0.00660377 0.00545984]
  [0.00207088 0.00753044 0.01643743 ... 0.00626619 0.00647977 0.00504232]
  ...
  [0.0091713  0.02111256 0.01395596 ... 0.0059633  0.00369343 0.00340101]
  [0.01336486 0.0310683  0.01867998 ... 0.00570873 0.00363653 0.00381848]
  [0.00833661 0.01802324 0.00984744 ... 0.00608161 0.00409396 0.0043825 ]]]
draw_multiblock_lbp=[[[0.03921569 0.38421569 0.51921569]
  [0.03921569 0.38421569 0.51921569]
  [0.03921569 0.38421569 0.51921569]
  ...
  [0.07843137 0.07843137 0.07843137]
  [0.07843137 0.07843137 0.07843137]
  [0.07843137 0.07843137 0.07843137]]

 [[0.03921569 0.38421569 0.51921569]
  [0.03921569 0.38421569 0.51921569]
  [0.03921569 0.38421569 0.51921569]
  ...
  [0.07843137 0.07843137 0.07843137]
  [0.07843137 0.07843137 0.07843137]
  [0.07843137 0.07843137 0.07843137]]

 [[0.03921569 0.38421569 0.51921569]
  [0.03921569 0.38421569 0.51921569]
  [0.04117647 0.38617647 0.52117647]
  ...
  [0.0745098  0.0745098  0.0745098 ]
  [0.07843137 0.07843137 0.07843137]
  [0.07843137 0.07843137 0.07843137]]

 ...

 [[0.09803922 0.09803922 0.09803922]
  [0.11372549 0.11372549 0.11372549]
  [0.12156863 0.12156863 0.12156863]
  ...
  [0.07058824 0.07058824 0.07058824]
  [0.07058824 0.07058824 0.07058824]
  [0.07058824 0.07058824 0.07058824]]

 [[0.10588235 0.10588235 0.10588235]
  [0.09411765 0.09411765 0.09411765]
  [0.10196078 0.10196078 0.10196078]
  ...
  [0.06666667 0.06666667 0.06666667]
  [0.07058824 0.07058824 0.07058824]
  [0.07843137 0.07843137 0.07843137]]

 [[0.12941176 0.12941176 0.12941176]
  [0.13333333 0.13333333 0.13333333]
  [0.12941176 0.12941176 0.12941176]
  ...
  [0.13333333 0.13333333 0.13333333]
  [0.13333333 0.13333333 0.13333333]
  [0.13333333 0.13333333 0.13333333]]]
==============================
patch_size=(x, 3)
Feature template={feature_template}

=====> Testing: ['patches_mean']
===> Feature subset testing at step: 1/512
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=1>.		Took <total_dur=0.7639385s> in total.<training_dur=0.6957083s>; <prediction_dur=0.0682302s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=80.0000%> with <n_comps=2>.		Took <total_dur=1.0599322s> in total.<training_dur=0.9725620s>; <prediction_dur=0.0873702s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=80.0000%> with <n_comps=3>.		Took <total_dur=1.0726705s> in total.<training_dur=1.0620059s>; <prediction_dur=0.0106646s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=77.5000%> with <n_comps=4>.		Took <total_dur=1.0744780s> in total.<training_dur=1.0634649s>; <prediction_dur=0.0110131s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=75.0000%> with <n_comps=5>.		Took <total_dur=0.8235759s> in total.<training_dur=0.7494868s>; <prediction_dur=0.0740891s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=75.0000%> with <n_comps=6>.		Took <total_dur=0.8063639s> in total.<training_dur=0.7379239s>; <prediction_dur=0.0684400s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=7>.		Took <total_dur=0.7974227s> in total.<training_dur=0.7266214s>; <prediction_dur=0.0708012s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=8>.		Took <total_dur=0.8403457s> in total.<training_dur=0.7658991s>; <prediction_dur=0.0744466s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=9>.		Took <total_dur=1.0635570s> in total.<training_dur=0.9751957s>; <prediction_dur=0.0883613s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=10>.		Took <total_dur=1.0288282s> in total.<training_dur=0.9564037s>; <prediction_dur=0.0724245s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=11>.		Took <total_dur=1.0144988s> in total.<training_dur=0.9522339s>; <prediction_dur=0.0622649s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=12>.		Took <total_dur=0.8573150s> in total.<training_dur=0.7915242s>; <prediction_dur=0.0657908s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=13>.		Took <total_dur=0.9353868s> in total.<training_dur=0.8384113s>; <prediction_dur=0.0969756s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=14>.		Took <total_dur=0.8717789s> in total.<training_dur=0.7868327s>; <prediction_dur=0.0849462s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=15>.		Took <total_dur=0.8273292s> in total.<training_dur=0.7609162s>; <prediction_dur=0.0664130s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=16>.		Took <total_dur=0.8638613s> in total.<training_dur=0.7947566s>; <prediction_dur=0.0691047s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=17>.		Took <total_dur=0.9023506s> in total.<training_dur=0.8286245s>; <prediction_dur=0.0737261s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=18>.		Took <total_dur=0.9256322s> in total.<training_dur=0.8370014s>; <prediction_dur=0.0886309s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=72.5000%> with <n_comps=19>.		Took <total_dur=0.8588997s> in total.<training_dur=0.7921588s>; <prediction_dur=0.0667409s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=75.0000%> with <n_comps=20>.		Took <total_dur=0.8907264s> in total.<training_dur=0.8265082s>; <prediction_dur=0.0642181s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=21>.		Took <total_dur=0.8537768s> in total.<training_dur=0.7777709s>; <prediction_dur=0.0760059s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=22>.		Took <total_dur=0.8779483s> in total.<training_dur=0.8101994s>; <prediction_dur=0.0677489s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=23>.		Took <total_dur=0.9369498s> in total.<training_dur=0.8485950s>; <prediction_dur=0.0883548s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=24>.		Took <total_dur=0.9784311s> in total.<training_dur=0.8963403s>; <prediction_dur=0.0820908s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=25>.		Took <total_dur=0.9207267s> in total.<training_dur=0.8247446s>; <prediction_dur=0.0959821s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=77.5000%> with <n_comps=26>.		Took <total_dur=1.0081150s> in total.<training_dur=0.9419214s>; <prediction_dur=0.0661936s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=72.5000%> with <n_comps=27>.		Took <total_dur=0.9135794s> in total.<training_dur=0.8312824s>; <prediction_dur=0.0822970s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=75.0000%> with <n_comps=28>.		Took <total_dur=0.9316326s> in total.<training_dur=0.8623789s>; <prediction_dur=0.0692537s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=29>.		Took <total_dur=1.0483265s> in total.<training_dur=0.9678773s>; <prediction_dur=0.0804492s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=30>.		Took <total_dur=0.9454710s> in total.<training_dur=0.8703107s>; <prediction_dur=0.0751603s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=31>.		Took <total_dur=0.9940661s> in total.<training_dur=0.9076264s>; <prediction_dur=0.0864398s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=72.5000%> with <n_comps=32>.		Took <total_dur=0.8950564s> in total.<training_dur=0.8280072s>; <prediction_dur=0.0670492s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=72.5000%> with <n_comps=33>.		Took <total_dur=1.0800979s> in total.<training_dur=1.0696520s>; <prediction_dur=0.0104458s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=34>.		Took <total_dur=0.9921514s> in total.<training_dur=0.9115102s>; <prediction_dur=0.0806412s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=35>.		Took <total_dur=0.9092890s> in total.<training_dur=0.8306238s>; <prediction_dur=0.0786652s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=36>.		Took <total_dur=1.0887482s> in total.<training_dur=1.0784346s>; <prediction_dur=0.0103136s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=75.0000%> with <n_comps=37>.		Took <total_dur=1.0834175s> in total.<training_dur=1.0725338s>; <prediction_dur=0.0108837s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=38>.		Took <total_dur=0.9952110s> in total.<training_dur=0.9328197s>; <prediction_dur=0.0623913s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=72.5000%> with <n_comps=39>.		Took <total_dur=0.9355782s> in total.<training_dur=0.8676167s>; <prediction_dur=0.0679616s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=40>.		Took <total_dur=0.9027646s> in total.<training_dur=0.8388054s>; <prediction_dur=0.0639592s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=41>.		Took <total_dur=1.0382667s> in total.<training_dur=0.9323927s>; <prediction_dur=0.1058740s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=42>.		Took <total_dur=0.9109796s> in total.<training_dur=0.8270660s>; <prediction_dur=0.0839136s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=77.5000%> with <n_comps=43>.		Took <total_dur=1.0828786s> in total.<training_dur=1.0704836s>; <prediction_dur=0.0123949s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=44>.		Took <total_dur=0.9350019s> in total.<training_dur=0.8607652s>; <prediction_dur=0.0742367s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=75.0000%> with <n_comps=45>.		Took <total_dur=1.0863798s> in total.<training_dur=1.0761093s>; <prediction_dur=0.0102705s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=46>.		Took <total_dur=0.8877183s> in total.<training_dur=0.8163489s>; <prediction_dur=0.0713694s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=47>.		Took <total_dur=0.8891288s> in total.<training_dur=0.8224497s>; <prediction_dur=0.0666791s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=48>.		Took <total_dur=0.9061571s> in total.<training_dur=0.8371951s>; <prediction_dur=0.0689620s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=49>.		Took <total_dur=0.9328363s> in total.<training_dur=0.8564047s>; <prediction_dur=0.0764316s>).

=====> Testing: ['patches_std']
===> Feature subset testing at step: 2/512
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=45.0000%> with <n_comps=1>.		Took <total_dur=0.8319629s> in total.<training_dur=0.7400885s>; <prediction_dur=0.0918744s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=42.5000%> with <n_comps=2>.		Took <total_dur=0.8189483s> in total.<training_dur=0.7494644s>; <prediction_dur=0.0694838s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=47.5000%> with <n_comps=3>.		Took <total_dur=0.8048880s> in total.<training_dur=0.7325438s>; <prediction_dur=0.0723442s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=4>.		Took <total_dur=0.8559325s> in total.<training_dur=0.7789032s>; <prediction_dur=0.0770293s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=50.0000%> with <n_comps=5>.		Took <total_dur=0.8452346s> in total.<training_dur=0.7716656s>; <prediction_dur=0.0735690s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=45.0000%> with <n_comps=6>.		Took <total_dur=1.0791970s> in total.<training_dur=1.0661765s>; <prediction_dur=0.0130205s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=7>.		Took <total_dur=1.0733980s> in total.<training_dur=1.0615811s>; <prediction_dur=0.0118170s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=47.5000%> with <n_comps=8>.		Took <total_dur=0.8560837s> in total.<training_dur=0.7904843s>; <prediction_dur=0.0655994s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=9>.		Took <total_dur=0.8613855s> in total.<training_dur=0.7942135s>; <prediction_dur=0.0671720s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=10>.		Took <total_dur=0.8362216s> in total.<training_dur=0.7663723s>; <prediction_dur=0.0698492s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=57.5000%> with <n_comps=11>.		Took <total_dur=0.8353418s> in total.<training_dur=0.7563619s>; <prediction_dur=0.0789800s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=57.5000%> with <n_comps=12>.		Took <total_dur=0.8457135s> in total.<training_dur=0.7772879s>; <prediction_dur=0.0684256s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=13>.		Took <total_dur=0.8465477s> in total.<training_dur=0.7806726s>; <prediction_dur=0.0658750s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=47.5000%> with <n_comps=14>.		Took <total_dur=0.8742825s> in total.<training_dur=0.7875544s>; <prediction_dur=0.0867280s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=15>.		Took <total_dur=0.9261262s> in total.<training_dur=0.8454510s>; <prediction_dur=0.0806752s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=45.0000%> with <n_comps=16>.		Took <total_dur=0.8468219s> in total.<training_dur=0.7801420s>; <prediction_dur=0.0666799s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=47.5000%> with <n_comps=17>.		Took <total_dur=1.0782760s> in total.<training_dur=1.0671824s>; <prediction_dur=0.0110936s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=18>.		Took <total_dur=0.8801516s> in total.<training_dur=0.8095672s>; <prediction_dur=0.0705844s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=19>.		Took <total_dur=1.0664834s> in total.<training_dur=1.0027628s>; <prediction_dur=0.0637206s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=55.0000%> with <n_comps=20>.		Took <total_dur=0.8525369s> in total.<training_dur=0.7806535s>; <prediction_dur=0.0718834s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=21>.		Took <total_dur=0.9149364s> in total.<training_dur=0.8197531s>; <prediction_dur=0.0951834s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=42.5000%> with <n_comps=22>.		Took <total_dur=1.0964401s> in total.<training_dur=1.0855925s>; <prediction_dur=0.0108476s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=23>.		Took <total_dur=0.9207425s> in total.<training_dur=0.8527784s>; <prediction_dur=0.0679641s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=24>.		Took <total_dur=0.8865471s> in total.<training_dur=0.8219949s>; <prediction_dur=0.0645521s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=57.5000%> with <n_comps=25>.		Took <total_dur=1.0837474s> in total.<training_dur=1.0725574s>; <prediction_dur=0.0111900s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=26>.		Took <total_dur=1.0705183s> in total.<training_dur=1.0594912s>; <prediction_dur=0.0110271s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=45.0000%> with <n_comps=27>.		Took <total_dur=0.9314138s> in total.<training_dur=0.8405433s>; <prediction_dur=0.0908705s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=42.5000%> with <n_comps=28>.		Took <total_dur=1.0752681s> in total.<training_dur=1.0638087s>; <prediction_dur=0.0114594s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=29>.		Took <total_dur=0.9098660s> in total.<training_dur=0.8436219s>; <prediction_dur=0.0662441s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=47.5000%> with <n_comps=30>.		Took <total_dur=0.8686033s> in total.<training_dur=0.8047433s>; <prediction_dur=0.0638601s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=31>.		Took <total_dur=0.8557089s> in total.<training_dur=0.7838681s>; <prediction_dur=0.0718408s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=55.0000%> with <n_comps=32>.		Took <total_dur=0.8858291s> in total.<training_dur=0.8205789s>; <prediction_dur=0.0652502s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=57.5000%> with <n_comps=33>.		Took <total_dur=0.9072567s> in total.<training_dur=0.8439221s>; <prediction_dur=0.0633346s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=47.5000%> with <n_comps=34>.		Took <total_dur=0.9262487s> in total.<training_dur=0.8604633s>; <prediction_dur=0.0657854s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=35>.		Took <total_dur=0.8825779s> in total.<training_dur=0.8200571s>; <prediction_dur=0.0625209s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=55.0000%> with <n_comps=36>.		Took <total_dur=0.9712445s> in total.<training_dur=0.8995645s>; <prediction_dur=0.0716800s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=37>.		Took <total_dur=0.9613578s> in total.<training_dur=0.8852490s>; <prediction_dur=0.0761087s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=50.0000%> with <n_comps=38>.		Took <total_dur=0.9249197s> in total.<training_dur=0.8524479s>; <prediction_dur=0.0724719s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=39>.		Took <total_dur=0.9486812s> in total.<training_dur=0.8534894s>; <prediction_dur=0.0951918s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=40>.		Took <total_dur=0.9310782s> in total.<training_dur=0.8684901s>; <prediction_dur=0.0625881s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=42.5000%> with <n_comps=41>.		Took <total_dur=0.9036881s> in total.<training_dur=0.8380105s>; <prediction_dur=0.0656775s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=45.0000%> with <n_comps=42>.		Took <total_dur=1.0751941s> in total.<training_dur=1.0647685s>; <prediction_dur=0.0104256s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=50.0000%> with <n_comps=43>.		Took <total_dur=0.8911535s> in total.<training_dur=0.8154220s>; <prediction_dur=0.0757315s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=44>.		Took <total_dur=0.9265555s> in total.<training_dur=0.8461200s>; <prediction_dur=0.0804355s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=45>.		Took <total_dur=0.9824478s> in total.<training_dur=0.9031206s>; <prediction_dur=0.0793272s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=47.5000%> with <n_comps=46>.		Took <total_dur=0.9461375s> in total.<training_dur=0.8735006s>; <prediction_dur=0.0726369s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=42.5000%> with <n_comps=47>.		Took <total_dur=0.9389045s> in total.<training_dur=0.8713923s>; <prediction_dur=0.0675122s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=48>.		Took <total_dur=0.8846715s> in total.<training_dur=0.8121888s>; <prediction_dur=0.0724828s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=49>.		Took <total_dur=0.9875465s> in total.<training_dur=0.9094720s>; <prediction_dur=0.0780745s>).

=====> Testing: ['patches_mean', 'patches_std']
===> Feature subset testing at step: 3/512
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=1>.		Took <total_dur=0.7734428s> in total.<training_dur=0.6937693s>; <prediction_dur=0.0796735s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=75.0000%> with <n_comps=2>.		Took <total_dur=0.7642756s> in total.<training_dur=0.7005550s>; <prediction_dur=0.0637206s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=85.0000%> with <n_comps=3>.		Took <total_dur=0.7941872s> in total.<training_dur=0.7181681s>; <prediction_dur=0.0760192s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=72.5000%> with <n_comps=4>.		Took <total_dur=0.8247773s> in total.<training_dur=0.7601155s>; <prediction_dur=0.0646618s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=5>.		Took <total_dur=0.8122126s> in total.<training_dur=0.7300980s>; <prediction_dur=0.0821146s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=72.5000%> with <n_comps=6>.		Took <total_dur=0.7960575s> in total.<training_dur=0.7292113s>; <prediction_dur=0.0668462s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=7>.		Took <total_dur=0.7760931s> in total.<training_dur=0.7072941s>; <prediction_dur=0.0687990s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=72.5000%> with <n_comps=8>.		Took <total_dur=0.7919065s> in total.<training_dur=0.7261136s>; <prediction_dur=0.0657929s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=9>.		Took <total_dur=1.0748774s> in total.<training_dur=1.0646045s>; <prediction_dur=0.0102729s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=10>.		Took <total_dur=1.0590802s> in total.<training_dur=0.9953435s>; <prediction_dur=0.0637367s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=11>.		Took <total_dur=0.8043508s> in total.<training_dur=0.7372891s>; <prediction_dur=0.0670616s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=12>.		Took <total_dur=0.8202051s> in total.<training_dur=0.7537473s>; <prediction_dur=0.0664578s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=13>.		Took <total_dur=0.8665047s> in total.<training_dur=0.7941337s>; <prediction_dur=0.0723710s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=14>.		Took <total_dur=0.8491091s> in total.<training_dur=0.7852218s>; <prediction_dur=0.0638872s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=15>.		Took <total_dur=0.8448409s> in total.<training_dur=0.7719898s>; <prediction_dur=0.0728511s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=16>.		Took <total_dur=0.8608859s> in total.<training_dur=0.7713768s>; <prediction_dur=0.0895091s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=72.5000%> with <n_comps=17>.		Took <total_dur=0.8817753s> in total.<training_dur=0.8092013s>; <prediction_dur=0.0725741s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=18>.		Took <total_dur=0.8497155s> in total.<training_dur=0.7618553s>; <prediction_dur=0.0878602s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=75.0000%> with <n_comps=19>.		Took <total_dur=1.0548926s> in total.<training_dur=0.9748893s>; <prediction_dur=0.0800032s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=20>.		Took <total_dur=0.8426416s> in total.<training_dur=0.7796123s>; <prediction_dur=0.0630293s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=55.0000%> with <n_comps=21>.		Took <total_dur=0.8414473s> in total.<training_dur=0.7770725s>; <prediction_dur=0.0643747s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=22>.		Took <total_dur=0.8582066s> in total.<training_dur=0.7916893s>; <prediction_dur=0.0665173s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=23>.		Took <total_dur=0.8485172s> in total.<training_dur=0.7639559s>; <prediction_dur=0.0845613s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=75.0000%> with <n_comps=24>.		Took <total_dur=0.8786391s> in total.<training_dur=0.8149131s>; <prediction_dur=0.0637259s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=25>.		Took <total_dur=0.8564642s> in total.<training_dur=0.7913313s>; <prediction_dur=0.0651329s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=26>.		Took <total_dur=0.8668709s> in total.<training_dur=0.8028530s>; <prediction_dur=0.0640179s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=27>.		Took <total_dur=0.8728710s> in total.<training_dur=0.8083328s>; <prediction_dur=0.0645381s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=28>.		Took <total_dur=0.8789660s> in total.<training_dur=0.8137717s>; <prediction_dur=0.0651943s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=75.0000%> with <n_comps=29>.		Took <total_dur=0.8662868s> in total.<training_dur=0.8034080s>; <prediction_dur=0.0628788s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=30>.		Took <total_dur=0.8518617s> in total.<training_dur=0.7828060s>; <prediction_dur=0.0690558s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=31>.		Took <total_dur=0.8807579s> in total.<training_dur=0.8056937s>; <prediction_dur=0.0750642s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=72.5000%> with <n_comps=32>.		Took <total_dur=0.8931927s> in total.<training_dur=0.8202992s>; <prediction_dur=0.0728935s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=72.5000%> with <n_comps=33>.		Took <total_dur=0.8639532s> in total.<training_dur=0.7961178s>; <prediction_dur=0.0678354s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=34>.		Took <total_dur=0.9071957s> in total.<training_dur=0.8324664s>; <prediction_dur=0.0747293s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=35>.		Took <total_dur=0.9002906s> in total.<training_dur=0.8181549s>; <prediction_dur=0.0821357s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=75.0000%> with <n_comps=36>.		Took <total_dur=1.0853976s> in total.<training_dur=1.0738372s>; <prediction_dur=0.0115603s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=37>.		Took <total_dur=1.0736143s> in total.<training_dur=1.0635930s>; <prediction_dur=0.0100212s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=38>.		Took <total_dur=1.0823504s> in total.<training_dur=1.0720744s>; <prediction_dur=0.0102760s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=72.5000%> with <n_comps=39>.		Took <total_dur=1.0807893s> in total.<training_dur=1.0698554s>; <prediction_dur=0.0109339s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=72.5000%> with <n_comps=40>.		Took <total_dur=1.0587668s> in total.<training_dur=1.0132653s>; <prediction_dur=0.0455015s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=41>.		Took <total_dur=0.8745136s> in total.<training_dur=0.8043478s>; <prediction_dur=0.0701658s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=42>.		Took <total_dur=0.9280380s> in total.<training_dur=0.8654041s>; <prediction_dur=0.0626339s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=43>.		Took <total_dur=0.9183572s> in total.<training_dur=0.8526777s>; <prediction_dur=0.0656794s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=72.5000%> with <n_comps=44>.		Took <total_dur=0.8931713s> in total.<training_dur=0.8222674s>; <prediction_dur=0.0709039s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=80.0000%> with <n_comps=45>.		Took <total_dur=0.8899066s> in total.<training_dur=0.8189120s>; <prediction_dur=0.0709946s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=75.0000%> with <n_comps=46>.		Took <total_dur=0.9491569s> in total.<training_dur=0.8845566s>; <prediction_dur=0.0646004s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=47>.		Took <total_dur=0.9229938s> in total.<training_dur=0.8511092s>; <prediction_dur=0.0718846s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=48>.		Took <total_dur=0.9140602s> in total.<training_dur=0.8438836s>; <prediction_dur=0.0701766s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=49>.		Took <total_dur=0.9175185s> in total.<training_dur=0.8506360s>; <prediction_dur=0.0668826s>).

=====> Testing: ['patches_var']
===> Feature subset testing at step: 4/512
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=55.0000%> with <n_comps=1>.		Took <total_dur=0.8214436s> in total.<training_dur=0.7280759s>; <prediction_dur=0.0933677s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=50.0000%> with <n_comps=2>.		Took <total_dur=0.8436805s> in total.<training_dur=0.7773157s>; <prediction_dur=0.0663648s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=47.5000%> with <n_comps=3>.		Took <total_dur=0.8046975s> in total.<training_dur=0.7313504s>; <prediction_dur=0.0733471s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=4>.		Took <total_dur=0.8102815s> in total.<training_dur=0.7380123s>; <prediction_dur=0.0722691s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=5>.		Took <total_dur=0.8231598s> in total.<training_dur=0.7559366s>; <prediction_dur=0.0672232s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=55.0000%> with <n_comps=6>.		Took <total_dur=0.8058926s> in total.<training_dur=0.7324424s>; <prediction_dur=0.0734503s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=50.0000%> with <n_comps=7>.		Took <total_dur=0.8298862s> in total.<training_dur=0.7653837s>; <prediction_dur=0.0645025s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=8>.		Took <total_dur=0.8272678s> in total.<training_dur=0.7611388s>; <prediction_dur=0.0661290s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=9>.		Took <total_dur=1.0811257s> in total.<training_dur=1.0701081s>; <prediction_dur=0.0110176s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=10>.		Took <total_dur=0.8142339s> in total.<training_dur=0.7484384s>; <prediction_dur=0.0657955s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=11>.		Took <total_dur=0.8517190s> in total.<training_dur=0.7793102s>; <prediction_dur=0.0724088s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=12>.		Took <total_dur=0.8306643s> in total.<training_dur=0.7644274s>; <prediction_dur=0.0662369s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=13>.		Took <total_dur=0.8466014s> in total.<training_dur=0.7720734s>; <prediction_dur=0.0745280s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=14>.		Took <total_dur=0.8406503s> in total.<training_dur=0.7705982s>; <prediction_dur=0.0700521s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=57.5000%> with <n_comps=15>.		Took <total_dur=0.8420978s> in total.<training_dur=0.7590617s>; <prediction_dur=0.0830360s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=55.0000%> with <n_comps=16>.		Took <total_dur=0.8858895s> in total.<training_dur=0.8145129s>; <prediction_dur=0.0713765s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=47.5000%> with <n_comps=17>.		Took <total_dur=0.8812660s> in total.<training_dur=0.8166325s>; <prediction_dur=0.0646335s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=55.0000%> with <n_comps=18>.		Took <total_dur=0.8613002s> in total.<training_dur=0.7937604s>; <prediction_dur=0.0675399s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=19>.		Took <total_dur=0.8730147s> in total.<training_dur=0.7869513s>; <prediction_dur=0.0860634s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=20>.		Took <total_dur=1.0811642s> in total.<training_dur=1.0702446s>; <prediction_dur=0.0109196s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=57.5000%> with <n_comps=21>.		Took <total_dur=0.9006919s> in total.<training_dur=0.8200275s>; <prediction_dur=0.0806644s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=22>.		Took <total_dur=0.9228623s> in total.<training_dur=0.8412158s>; <prediction_dur=0.0816465s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=42.5000%> with <n_comps=23>.		Took <total_dur=0.8914710s> in total.<training_dur=0.8188954s>; <prediction_dur=0.0725756s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=50.0000%> with <n_comps=24>.		Took <total_dur=1.0861005s> in total.<training_dur=1.0754089s>; <prediction_dur=0.0106916s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=25>.		Took <total_dur=1.0875322s> in total.<training_dur=1.0766038s>; <prediction_dur=0.0109284s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=55.0000%> with <n_comps=26>.		Took <total_dur=1.0516547s> in total.<training_dur=0.9616336s>; <prediction_dur=0.0900211s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=27>.		Took <total_dur=1.0722729s> in total.<training_dur=1.0589944s>; <prediction_dur=0.0132785s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=28>.		Took <total_dur=0.9022482s> in total.<training_dur=0.8333177s>; <prediction_dur=0.0689305s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=45.0000%> with <n_comps=29>.		Took <total_dur=0.8715512s> in total.<training_dur=0.7952405s>; <prediction_dur=0.0763107s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=47.5000%> with <n_comps=30>.		Took <total_dur=0.8568646s> in total.<training_dur=0.7928693s>; <prediction_dur=0.0639954s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=45.0000%> with <n_comps=31>.		Took <total_dur=0.9498615s> in total.<training_dur=0.8833057s>; <prediction_dur=0.0665558s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=45.0000%> with <n_comps=32>.		Took <total_dur=0.8943416s> in total.<training_dur=0.8289674s>; <prediction_dur=0.0653742s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=50.0000%> with <n_comps=33>.		Took <total_dur=1.0870147s> in total.<training_dur=1.0759697s>; <prediction_dur=0.0110450s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=42.5000%> with <n_comps=34>.		Took <total_dur=1.0504760s> in total.<training_dur=0.9794210s>; <prediction_dur=0.0710550s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=50.0000%> with <n_comps=35>.		Took <total_dur=0.8807884s> in total.<training_dur=0.8161876s>; <prediction_dur=0.0646008s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=45.0000%> with <n_comps=36>.		Took <total_dur=1.0904605s> in total.<training_dur=1.0796924s>; <prediction_dur=0.0107681s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=37.5000%> with <n_comps=37>.		Took <total_dur=0.9810459s> in total.<training_dur=0.9186546s>; <prediction_dur=0.0623913s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=45.0000%> with <n_comps=38>.		Took <total_dur=0.9040246s> in total.<training_dur=0.8289027s>; <prediction_dur=0.0751219s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=39>.		Took <total_dur=0.9516717s> in total.<training_dur=0.8831641s>; <prediction_dur=0.0685076s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=42.5000%> with <n_comps=40>.		Took <total_dur=0.9015329s> in total.<training_dur=0.8319943s>; <prediction_dur=0.0695386s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=50.0000%> with <n_comps=41>.		Took <total_dur=1.0901737s> in total.<training_dur=1.0791930s>; <prediction_dur=0.0109807s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=57.5000%> with <n_comps=42>.		Took <total_dur=1.0292060s> in total.<training_dur=0.9652973s>; <prediction_dur=0.0639087s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=47.5000%> with <n_comps=43>.		Took <total_dur=1.0906244s> in total.<training_dur=1.0793587s>; <prediction_dur=0.0112658s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=42.5000%> with <n_comps=44>.		Took <total_dur=1.0827855s> in total.<training_dur=1.0714119s>; <prediction_dur=0.0113736s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=50.0000%> with <n_comps=45>.		Took <total_dur=1.0813278s> in total.<training_dur=1.0700692s>; <prediction_dur=0.0112585s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=57.5000%> with <n_comps=46>.		Took <total_dur=1.0799147s> in total.<training_dur=1.0695242s>; <prediction_dur=0.0103905s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=47>.		Took <total_dur=0.9596079s> in total.<training_dur=0.8656194s>; <prediction_dur=0.0939885s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=50.0000%> with <n_comps=48>.		Took <total_dur=1.0858044s> in total.<training_dur=1.0750052s>; <prediction_dur=0.0107992s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=45.0000%> with <n_comps=49>.		Took <total_dur=1.0853317s> in total.<training_dur=1.0747339s>; <prediction_dur=0.0105977s>).

=====> Testing: ['patches_mean', 'patches_var']
===> Feature subset testing at step: 5/512
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=45.0000%> with <n_comps=1>.		Took <total_dur=1.0629194s> in total.<training_dur=1.0342678s>; <prediction_dur=0.0286516s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=2>.		Took <total_dur=1.0590027s> in total.<training_dur=1.0251769s>; <prediction_dur=0.0338259s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=3>.		Took <total_dur=1.0650412s> in total.<training_dur=1.0347392s>; <prediction_dur=0.0303020s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=72.5000%> with <n_comps=4>.		Took <total_dur=1.0683542s> in total.<training_dur=1.0570133s>; <prediction_dur=0.0113409s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=5>.		Took <total_dur=1.0675752s> in total.<training_dur=1.0550972s>; <prediction_dur=0.0124779s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=6>.		Took <total_dur=0.8980998s> in total.<training_dur=0.8082320s>; <prediction_dur=0.0898678s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=72.5000%> with <n_comps=7>.		Took <total_dur=1.0656297s> in total.<training_dur=1.0551461s>; <prediction_dur=0.0104837s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=8>.		Took <total_dur=1.0688754s> in total.<training_dur=1.0575050s>; <prediction_dur=0.0113704s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=75.0000%> with <n_comps=9>.		Took <total_dur=1.0700172s> in total.<training_dur=1.0592911s>; <prediction_dur=0.0107261s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=10>.		Took <total_dur=1.0734089s> in total.<training_dur=1.0616256s>; <prediction_dur=0.0117833s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=11>.		Took <total_dur=1.0692525s> in total.<training_dur=1.0579602s>; <prediction_dur=0.0112923s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=12>.		Took <total_dur=0.8710865s> in total.<training_dur=0.7975714s>; <prediction_dur=0.0735151s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=13>.		Took <total_dur=0.8551444s> in total.<training_dur=0.7715037s>; <prediction_dur=0.0836406s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=14>.		Took <total_dur=0.8514467s> in total.<training_dur=0.7824941s>; <prediction_dur=0.0689526s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=15>.		Took <total_dur=0.8241417s> in total.<training_dur=0.7563458s>; <prediction_dur=0.0677958s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=16>.		Took <total_dur=0.8527266s> in total.<training_dur=0.7815012s>; <prediction_dur=0.0712255s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=17>.		Took <total_dur=0.8473204s> in total.<training_dur=0.7787434s>; <prediction_dur=0.0685770s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=18>.		Took <total_dur=0.8746211s> in total.<training_dur=0.8016516s>; <prediction_dur=0.0729694s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=19>.		Took <total_dur=0.8386476s> in total.<training_dur=0.7748515s>; <prediction_dur=0.0637961s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=20>.		Took <total_dur=1.0794797s> in total.<training_dur=1.0686523s>; <prediction_dur=0.0108274s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=72.5000%> with <n_comps=21>.		Took <total_dur=0.9633031s> in total.<training_dur=0.8982221s>; <prediction_dur=0.0650810s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=22>.		Took <total_dur=0.9648121s> in total.<training_dur=0.8747852s>; <prediction_dur=0.0900269s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=23>.		Took <total_dur=1.0781086s> in total.<training_dur=1.0676540s>; <prediction_dur=0.0104546s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=24>.		Took <total_dur=1.0720698s> in total.<training_dur=1.0617962s>; <prediction_dur=0.0102737s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=25>.		Took <total_dur=0.9244972s> in total.<training_dur=0.8589327s>; <prediction_dur=0.0655645s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=26>.		Took <total_dur=0.9211710s> in total.<training_dur=0.8309926s>; <prediction_dur=0.0901784s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=27>.		Took <total_dur=1.0820524s> in total.<training_dur=1.0717599s>; <prediction_dur=0.0102926s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=28>.		Took <total_dur=1.0779311s> in total.<training_dur=1.0669043s>; <prediction_dur=0.0110268s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=29>.		Took <total_dur=1.0594327s> in total.<training_dur=0.9926930s>; <prediction_dur=0.0667397s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=30>.		Took <total_dur=0.8380805s> in total.<training_dur=0.7739427s>; <prediction_dur=0.0641379s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=31>.		Took <total_dur=0.8656987s> in total.<training_dur=0.7975833s>; <prediction_dur=0.0681153s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=32>.		Took <total_dur=0.8425259s> in total.<training_dur=0.7770861s>; <prediction_dur=0.0654398s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=72.5000%> with <n_comps=33>.		Took <total_dur=0.8692494s> in total.<training_dur=0.8054780s>; <prediction_dur=0.0637714s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=34>.		Took <total_dur=0.8366960s> in total.<training_dur=0.7646540s>; <prediction_dur=0.0720421s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=35>.		Took <total_dur=0.8649751s> in total.<training_dur=0.7877991s>; <prediction_dur=0.0771760s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=36>.		Took <total_dur=0.8977736s> in total.<training_dur=0.8118385s>; <prediction_dur=0.0859351s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=37>.		Took <total_dur=0.8719081s> in total.<training_dur=0.8040944s>; <prediction_dur=0.0678137s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=38>.		Took <total_dur=0.9022170s> in total.<training_dur=0.8115062s>; <prediction_dur=0.0907109s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=39>.		Took <total_dur=1.0845282s> in total.<training_dur=1.0735403s>; <prediction_dur=0.0109880s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=40>.		Took <total_dur=1.0808511s> in total.<training_dur=1.0707308s>; <prediction_dur=0.0101203s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=41>.		Took <total_dur=0.9066976s> in total.<training_dur=0.8357923s>; <prediction_dur=0.0709053s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=42>.		Took <total_dur=1.0719153s> in total.<training_dur=1.0613112s>; <prediction_dur=0.0106041s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=43>.		Took <total_dur=0.8785267s> in total.<training_dur=0.8103363s>; <prediction_dur=0.0681904s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=72.5000%> with <n_comps=44>.		Took <total_dur=0.9024100s> in total.<training_dur=0.8309406s>; <prediction_dur=0.0714694s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=77.5000%> with <n_comps=45>.		Took <total_dur=0.8966991s> in total.<training_dur=0.8315294s>; <prediction_dur=0.0651697s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=46>.		Took <total_dur=0.9042881s> in total.<training_dur=0.8327359s>; <prediction_dur=0.0715523s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=47>.		Took <total_dur=0.9118349s> in total.<training_dur=0.8386600s>; <prediction_dur=0.0731749s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=48>.		Took <total_dur=0.8998335s> in total.<training_dur=0.8363229s>; <prediction_dur=0.0635106s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=49>.		Took <total_dur=0.9402777s> in total.<training_dur=0.8584836s>; <prediction_dur=0.0817942s>).

=====> Testing: ['patches_std', 'patches_var']
===> Feature subset testing at step: 6/512
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=1>.		Took <total_dur=0.8166816s> in total.<training_dur=0.7434167s>; <prediction_dur=0.0732649s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=55.0000%> with <n_comps=2>.		Took <total_dur=0.7625381s> in total.<training_dur=0.6916605s>; <prediction_dur=0.0708775s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=42.5000%> with <n_comps=3>.		Took <total_dur=0.7788318s> in total.<training_dur=0.7160942s>; <prediction_dur=0.0627376s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=50.0000%> with <n_comps=4>.		Took <total_dur=0.7994606s> in total.<training_dur=0.7312876s>; <prediction_dur=0.0681730s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=42.5000%> with <n_comps=5>.		Took <total_dur=0.8228979s> in total.<training_dur=0.7532137s>; <prediction_dur=0.0696842s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=47.5000%> with <n_comps=6>.		Took <total_dur=0.8388876s> in total.<training_dur=0.7723536s>; <prediction_dur=0.0665340s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=47.5000%> with <n_comps=7>.		Took <total_dur=0.8180938s> in total.<training_dur=0.7413837s>; <prediction_dur=0.0767101s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=50.0000%> with <n_comps=8>.		Took <total_dur=0.7996702s> in total.<training_dur=0.7319255s>; <prediction_dur=0.0677447s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=9>.		Took <total_dur=0.8358122s> in total.<training_dur=0.7724568s>; <prediction_dur=0.0633554s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=10>.		Took <total_dur=0.8447055s> in total.<training_dur=0.7686553s>; <prediction_dur=0.0760501s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=50.0000%> with <n_comps=11>.		Took <total_dur=1.0002082s> in total.<training_dur=0.9079135s>; <prediction_dur=0.0922946s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=12>.		Took <total_dur=1.0717555s> in total.<training_dur=1.0610029s>; <prediction_dur=0.0107526s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=55.0000%> with <n_comps=13>.		Took <total_dur=1.0714832s> in total.<training_dur=1.0608748s>; <prediction_dur=0.0106084s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=57.5000%> with <n_comps=14>.		Took <total_dur=1.0643268s> in total.<training_dur=1.0540048s>; <prediction_dur=0.0103219s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=57.5000%> with <n_comps=15>.		Took <total_dur=1.0735254s> in total.<training_dur=1.0629763s>; <prediction_dur=0.0105491s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=57.5000%> with <n_comps=16>.		Took <total_dur=0.8781911s> in total.<training_dur=0.8010115s>; <prediction_dur=0.0771796s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=40.0000%> with <n_comps=17>.		Took <total_dur=1.0372903s> in total.<training_dur=0.9448451s>; <prediction_dur=0.0924452s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=18>.		Took <total_dur=1.0542798s> in total.<training_dur=0.9580525s>; <prediction_dur=0.0962274s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=19>.		Took <total_dur=1.0713213s> in total.<training_dur=1.0609755s>; <prediction_dur=0.0103458s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=20>.		Took <total_dur=1.0573683s> in total.<training_dur=0.9629125s>; <prediction_dur=0.0944559s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=57.5000%> with <n_comps=21>.		Took <total_dur=1.0788798s> in total.<training_dur=1.0686083s>; <prediction_dur=0.0102715s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=42.5000%> with <n_comps=22>.		Took <total_dur=0.8843196s> in total.<training_dur=0.8164152s>; <prediction_dur=0.0679044s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=50.0000%> with <n_comps=23>.		Took <total_dur=0.8720035s> in total.<training_dur=0.8071173s>; <prediction_dur=0.0648862s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=50.0000%> with <n_comps=24>.		Took <total_dur=0.8531925s> in total.<training_dur=0.7889225s>; <prediction_dur=0.0642699s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=25>.		Took <total_dur=0.8394620s> in total.<training_dur=0.7675632s>; <prediction_dur=0.0718988s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=45.0000%> with <n_comps=26>.		Took <total_dur=0.8968824s> in total.<training_dur=0.8224594s>; <prediction_dur=0.0744230s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=50.0000%> with <n_comps=27>.		Took <total_dur=0.8682922s> in total.<training_dur=0.7944785s>; <prediction_dur=0.0738138s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=55.0000%> with <n_comps=28>.		Took <total_dur=0.8722500s> in total.<training_dur=0.8035606s>; <prediction_dur=0.0686894s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=45.0000%> with <n_comps=29>.		Took <total_dur=0.9291138s> in total.<training_dur=0.8324256s>; <prediction_dur=0.0966882s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=42.5000%> with <n_comps=30>.		Took <total_dur=0.8629217s> in total.<training_dur=0.7933719s>; <prediction_dur=0.0695498s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=40.0000%> with <n_comps=31>.		Took <total_dur=0.8904773s> in total.<training_dur=0.8203636s>; <prediction_dur=0.0701137s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=42.5000%> with <n_comps=32>.		Took <total_dur=0.8951986s> in total.<training_dur=0.8210856s>; <prediction_dur=0.0741131s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=33>.		Took <total_dur=0.8810302s> in total.<training_dur=0.8017400s>; <prediction_dur=0.0792902s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=47.5000%> with <n_comps=34>.		Took <total_dur=0.8854305s> in total.<training_dur=0.7945530s>; <prediction_dur=0.0908775s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=50.0000%> with <n_comps=35>.		Took <total_dur=0.8584298s> in total.<training_dur=0.7816852s>; <prediction_dur=0.0767445s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=42.5000%> with <n_comps=36>.		Took <total_dur=0.9206088s> in total.<training_dur=0.8582698s>; <prediction_dur=0.0623390s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=50.0000%> with <n_comps=37>.		Took <total_dur=0.9710625s> in total.<training_dur=0.8846058s>; <prediction_dur=0.0864567s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=38>.		Took <total_dur=0.9062899s> in total.<training_dur=0.8371953s>; <prediction_dur=0.0690946s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=40.0000%> with <n_comps=39>.		Took <total_dur=0.9335508s> in total.<training_dur=0.8704534s>; <prediction_dur=0.0630974s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=50.0000%> with <n_comps=40>.		Took <total_dur=1.0813996s> in total.<training_dur=1.0701826s>; <prediction_dur=0.0112170s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=40.0000%> with <n_comps=41>.		Took <total_dur=0.8672683s> in total.<training_dur=0.7900614s>; <prediction_dur=0.0772069s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=42>.		Took <total_dur=1.0791067s> in total.<training_dur=1.0673238s>; <prediction_dur=0.0117828s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=57.5000%> with <n_comps=43>.		Took <total_dur=0.9666913s> in total.<training_dur=0.9037962s>; <prediction_dur=0.0628951s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=44>.		Took <total_dur=0.8876674s> in total.<training_dur=0.8224957s>; <prediction_dur=0.0651716s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=47.5000%> with <n_comps=45>.		Took <total_dur=0.8877010s> in total.<training_dur=0.8209870s>; <prediction_dur=0.0667139s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=46>.		Took <total_dur=0.9055401s> in total.<training_dur=0.8318892s>; <prediction_dur=0.0736509s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=50.0000%> with <n_comps=47>.		Took <total_dur=0.9053760s> in total.<training_dur=0.8349702s>; <prediction_dur=0.0704058s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=45.0000%> with <n_comps=48>.		Took <total_dur=0.9796832s> in total.<training_dur=0.9094039s>; <prediction_dur=0.0702794s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=45.0000%> with <n_comps=49>.		Took <total_dur=1.0876173s> in total.<training_dur=1.0755806s>; <prediction_dur=0.0120367s>).

=====> Testing: ['patches_mean', 'patches_std', 'patches_var']
===> Feature subset testing at step: 7/512
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=1>.		Took <total_dur=0.8333684s> in total.<training_dur=0.7377200s>; <prediction_dur=0.0956484s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=72.5000%> with <n_comps=2>.		Took <total_dur=1.0526246s> in total.<training_dur=1.0355819s>; <prediction_dur=0.0170427s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=75.0000%> with <n_comps=3>.		Took <total_dur=0.7746680s> in total.<training_dur=0.6964022s>; <prediction_dur=0.0782658s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=4>.		Took <total_dur=0.7968729s> in total.<training_dur=0.7275478s>; <prediction_dur=0.0693251s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=5>.		Took <total_dur=1.0556650s> in total.<training_dur=1.0175090s>; <prediction_dur=0.0381560s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=77.5000%> with <n_comps=6>.		Took <total_dur=1.0496214s> in total.<training_dur=0.9727241s>; <prediction_dur=0.0768973s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=7>.		Took <total_dur=1.0583714s> in total.<training_dur=1.0479190s>; <prediction_dur=0.0104524s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=8>.		Took <total_dur=1.0625567s> in total.<training_dur=1.0511734s>; <prediction_dur=0.0113833s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=9>.		Took <total_dur=0.8362971s> in total.<training_dur=0.7552374s>; <prediction_dur=0.0810597s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=10>.		Took <total_dur=0.8272660s> in total.<training_dur=0.7591418s>; <prediction_dur=0.0681241s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=11>.		Took <total_dur=0.8116393s> in total.<training_dur=0.7476689s>; <prediction_dur=0.0639704s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=12>.		Took <total_dur=0.8136541s> in total.<training_dur=0.7459484s>; <prediction_dur=0.0677057s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=13>.		Took <total_dur=0.8183441s> in total.<training_dur=0.7514125s>; <prediction_dur=0.0669316s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=14>.		Took <total_dur=0.8304213s> in total.<training_dur=0.7647907s>; <prediction_dur=0.0656306s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=57.5000%> with <n_comps=15>.		Took <total_dur=0.8323720s> in total.<training_dur=0.7683266s>; <prediction_dur=0.0640454s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=16>.		Took <total_dur=0.8776886s> in total.<training_dur=0.8132144s>; <prediction_dur=0.0644743s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=17>.		Took <total_dur=1.0245527s> in total.<training_dur=0.9309623s>; <prediction_dur=0.0935904s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=18>.		Took <total_dur=1.0710657s> in total.<training_dur=1.0608125s>; <prediction_dur=0.0102532s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=19>.		Took <total_dur=1.0593379s> in total.<training_dur=0.9853409s>; <prediction_dur=0.0739970s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=20>.		Took <total_dur=0.9694149s> in total.<training_dur=0.9020622s>; <prediction_dur=0.0673527s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=21>.		Took <total_dur=0.8551016s> in total.<training_dur=0.7918557s>; <prediction_dur=0.0632459s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=22>.		Took <total_dur=0.8589827s> in total.<training_dur=0.7928573s>; <prediction_dur=0.0661254s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=23>.		Took <total_dur=0.9139201s> in total.<training_dur=0.8184807s>; <prediction_dur=0.0954394s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=24>.		Took <total_dur=0.9140188s> in total.<training_dur=0.8222708s>; <prediction_dur=0.0917480s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=25>.		Took <total_dur=0.9276585s> in total.<training_dur=0.8557950s>; <prediction_dur=0.0718636s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=72.5000%> with <n_comps=26>.		Took <total_dur=0.8844996s> in total.<training_dur=0.8099819s>; <prediction_dur=0.0745178s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=27>.		Took <total_dur=0.8826672s> in total.<training_dur=0.8164750s>; <prediction_dur=0.0661922s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=28>.		Took <total_dur=0.9663933s> in total.<training_dur=0.8790474s>; <prediction_dur=0.0873459s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=29>.		Took <total_dur=0.9277637s> in total.<training_dur=0.8491864s>; <prediction_dur=0.0785774s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=30>.		Took <total_dur=0.9631305s> in total.<training_dur=0.8928580s>; <prediction_dur=0.0702724s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=31>.		Took <total_dur=0.9560034s> in total.<training_dur=0.8651239s>; <prediction_dur=0.0908796s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=32>.		Took <total_dur=0.9070620s> in total.<training_dur=0.8380414s>; <prediction_dur=0.0690206s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=72.5000%> with <n_comps=33>.		Took <total_dur=0.9473042s> in total.<training_dur=0.8703302s>; <prediction_dur=0.0769740s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=57.5000%> with <n_comps=34>.		Took <total_dur=1.0764537s> in total.<training_dur=1.0654861s>; <prediction_dur=0.0109676s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=35>.		Took <total_dur=1.0659192s> in total.<training_dur=1.0551610s>; <prediction_dur=0.0107582s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=36>.		Took <total_dur=1.0695097s> in total.<training_dur=1.0586598s>; <prediction_dur=0.0108499s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=37>.		Took <total_dur=1.0209503s> in total.<training_dur=0.9486214s>; <prediction_dur=0.0723289s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=38>.		Took <total_dur=1.0501215s> in total.<training_dur=1.0057285s>; <prediction_dur=0.0443930s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=39>.		Took <total_dur=0.9191607s> in total.<training_dur=0.8505680s>; <prediction_dur=0.0685926s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=40>.		Took <total_dur=1.0783142s> in total.<training_dur=1.0669425s>; <prediction_dur=0.0113716s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=41>.		Took <total_dur=1.0716877s> in total.<training_dur=1.0598364s>; <prediction_dur=0.0118513s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=42>.		Took <total_dur=0.9174351s> in total.<training_dur=0.8397331s>; <prediction_dur=0.0777020s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=57.5000%> with <n_comps=43>.		Took <total_dur=0.8850126s> in total.<training_dur=0.8186228s>; <prediction_dur=0.0663898s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=44>.		Took <total_dur=0.9310504s> in total.<training_dur=0.8498165s>; <prediction_dur=0.0812339s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=72.5000%> with <n_comps=45>.		Took <total_dur=0.9306265s> in total.<training_dur=0.8586535s>; <prediction_dur=0.0719730s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=46>.		Took <total_dur=0.8861818s> in total.<training_dur=0.8230403s>; <prediction_dur=0.0631415s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=47>.		Took <total_dur=0.9325781s> in total.<training_dur=0.8592367s>; <prediction_dur=0.0733414s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=48>.		Took <total_dur=0.9029676s> in total.<training_dur=0.8347555s>; <prediction_dur=0.0682120s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=49>.		Took <total_dur=1.0227209s> in total.<training_dur=0.9514179s>; <prediction_dur=0.0713030s>).

=====> Testing: ['lbp']
===> Feature subset testing at step: 8/512
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=1>.		Took <total_dur=0.9881484s> in total.<training_dur=0.9771296s>; <prediction_dur=0.0110189s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=2>.		Took <total_dur=0.9891033s> in total.<training_dur=0.9785952s>; <prediction_dur=0.0105081s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=55.0000%> with <n_comps=3>.		Took <total_dur=0.9853422s> in total.<training_dur=0.9749866s>; <prediction_dur=0.0103556s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=57.5000%> with <n_comps=4>.		Took <total_dur=0.9924638s> in total.<training_dur=0.9811011s>; <prediction_dur=0.0113626s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=5>.		Took <total_dur=0.9826893s> in total.<training_dur=0.9719306s>; <prediction_dur=0.0107587s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=6>.		Took <total_dur=0.9895889s> in total.<training_dur=0.9791621s>; <prediction_dur=0.0104268s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=7>.		Took <total_dur=0.7950274s> in total.<training_dur=0.7283047s>; <prediction_dur=0.0667227s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=8>.		Took <total_dur=0.8129990s> in total.<training_dur=0.7435639s>; <prediction_dur=0.0694351s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=70.0000%> with <n_comps=9>.		Took <total_dur=0.9657546s> in total.<training_dur=0.8829809s>; <prediction_dur=0.0827737s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=57.5000%> with <n_comps=10>.		Took <total_dur=0.9843599s> in total.<training_dur=0.9719394s>; <prediction_dur=0.0124204s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=67.5000%> with <n_comps=11>.		Took <total_dur=0.9764045s> in total.<training_dur=0.9657641s>; <prediction_dur=0.0106404s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=55.0000%> with <n_comps=12>.		Took <total_dur=0.8573846s> in total.<training_dur=0.7803952s>; <prediction_dur=0.0769894s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=55.0000%> with <n_comps=13>.		Took <total_dur=0.8168134s> in total.<training_dur=0.7530547s>; <prediction_dur=0.0637587s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=62.5000%> with <n_comps=14>.		Took <total_dur=0.8294092s> in total.<training_dur=0.7678744s>; <prediction_dur=0.0615349s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=65.0000%> with <n_comps=15>.		Took <total_dur=0.8352392s> in total.<training_dur=0.7680373s>; <prediction_dur=0.0672019s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=52.5000%> with <n_comps=16>.		Took <total_dur=0.8403710s> in total.<training_dur=0.7707546s>; <prediction_dur=0.0696163s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=17>.		Took <total_dur=0.8460688s> in total.<training_dur=0.7803167s>; <prediction_dur=0.0657521s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=55.0000%> with <n_comps=18>.		Took <total_dur=0.8279298s> in total.<training_dur=0.7652295s>; <prediction_dur=0.0627003s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=57.5000%> with <n_comps=19>.		Took <total_dur=0.8730202s> in total.<training_dur=0.7779392s>; <prediction_dur=0.0950810s>).
Testing model.
================= About to train.
================= About to predict.
Got accuracy of <acc=60.0000%> with <n_comps=20>.		Took <total_dur=0.8652996s> in total.<training_dur=0.8014359s>; <prediction_dur=0.0638637s>).
Testing model.
================= About to train.
================= About to predict.
